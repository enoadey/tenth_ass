{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetune your Neural Network\n",
    "\n",
    "**Exercise objectives:**\n",
    "- `Finetune` the model optimizer\n",
    "- `Save` and `Load` a `trained neural network`\n",
    "\n",
    "<hr>\n",
    "\n",
    "Now that you have solid foundations of what Neural Networks, how to design their architecture and how to regularize them, let's take a closer look at the `.compile(loss = ..., metrics = ..., activation = ...)` part.\n",
    "\n",
    "# Data\n",
    "\n",
    "We will use the data from the `Boston Housing dataset`. \n",
    "\n",
    "Our goal is to `predict the values of the houses` (in k USD), and we will measure our models' performances  using the `Mean Absolute Error` metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.datasets import boston_housing\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:ylabel='Count'>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "(X_train, y_train), (X_test, y_test) = boston_housing.load_data()\n",
    "X_train.shape\n",
    "sns.histplot(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 404 entries, 0 to 403\n",
      "Data columns (total 13 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   0       404 non-null    float64\n",
      " 1   1       404 non-null    float64\n",
      " 2   2       404 non-null    float64\n",
      " 3   3       404 non-null    float64\n",
      " 4   4       404 non-null    float64\n",
      " 5   5       404 non-null    float64\n",
      " 6   6       404 non-null    float64\n",
      " 7   7       404 non-null    float64\n",
      " 8   8       404 non-null    float64\n",
      " 9   9       404 non-null    float64\n",
      " 10  10      404 non-null    float64\n",
      " 11  11      404 non-null    float64\n",
      " 12  12      404 non-null    float64\n",
      "dtypes: float64(13)\n",
      "memory usage: 41.2 KB\n"
     ]
    }
   ],
   "source": [
    "pd.DataFrame(X_train).info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 102 entries, 0 to 101\n",
      "Data columns (total 13 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   0       102 non-null    float64\n",
      " 1   1       102 non-null    float64\n",
      " 2   2       102 non-null    float64\n",
      " 3   3       102 non-null    float64\n",
      " 4   4       102 non-null    float64\n",
      " 5   5       102 non-null    float64\n",
      " 6   6       102 non-null    float64\n",
      " 7   7       102 non-null    float64\n",
      " 8   8       102 non-null    float64\n",
      " 9   9       102 non-null    float64\n",
      " 10  10      102 non-null    float64\n",
      " 11  11      102 non-null    float64\n",
      " 12  12      102 non-null    float64\n",
      "dtypes: float64(13)\n",
      "memory usage: 10.5 KB\n"
     ]
    }
   ],
   "source": [
    "pd.DataFrame(X_test).info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>404.000000</td>\n",
       "      <td>404.000000</td>\n",
       "      <td>404.000000</td>\n",
       "      <td>404.000000</td>\n",
       "      <td>404.000000</td>\n",
       "      <td>404.000000</td>\n",
       "      <td>404.000000</td>\n",
       "      <td>404.000000</td>\n",
       "      <td>404.000000</td>\n",
       "      <td>404.000000</td>\n",
       "      <td>404.000000</td>\n",
       "      <td>404.000000</td>\n",
       "      <td>404.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.745111</td>\n",
       "      <td>11.480198</td>\n",
       "      <td>11.104431</td>\n",
       "      <td>0.061881</td>\n",
       "      <td>0.557356</td>\n",
       "      <td>6.267082</td>\n",
       "      <td>69.010644</td>\n",
       "      <td>3.740271</td>\n",
       "      <td>9.440594</td>\n",
       "      <td>405.898515</td>\n",
       "      <td>18.475990</td>\n",
       "      <td>354.783168</td>\n",
       "      <td>12.740817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>9.240734</td>\n",
       "      <td>23.767711</td>\n",
       "      <td>6.811308</td>\n",
       "      <td>0.241238</td>\n",
       "      <td>0.117293</td>\n",
       "      <td>0.709788</td>\n",
       "      <td>27.940665</td>\n",
       "      <td>2.030215</td>\n",
       "      <td>8.698360</td>\n",
       "      <td>166.374543</td>\n",
       "      <td>2.200382</td>\n",
       "      <td>94.111148</td>\n",
       "      <td>7.254545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.006320</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.460000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.385000</td>\n",
       "      <td>3.561000</td>\n",
       "      <td>2.900000</td>\n",
       "      <td>1.129600</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>188.000000</td>\n",
       "      <td>12.600000</td>\n",
       "      <td>0.320000</td>\n",
       "      <td>1.730000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.081437</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.130000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.453000</td>\n",
       "      <td>5.874750</td>\n",
       "      <td>45.475000</td>\n",
       "      <td>2.077100</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>279.000000</td>\n",
       "      <td>17.225000</td>\n",
       "      <td>374.672500</td>\n",
       "      <td>6.890000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.268880</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.690000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.538000</td>\n",
       "      <td>6.198500</td>\n",
       "      <td>78.500000</td>\n",
       "      <td>3.142300</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>330.000000</td>\n",
       "      <td>19.100000</td>\n",
       "      <td>391.250000</td>\n",
       "      <td>11.395000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>3.674808</td>\n",
       "      <td>12.500000</td>\n",
       "      <td>18.100000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.631000</td>\n",
       "      <td>6.609000</td>\n",
       "      <td>94.100000</td>\n",
       "      <td>5.118000</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>666.000000</td>\n",
       "      <td>20.200000</td>\n",
       "      <td>396.157500</td>\n",
       "      <td>17.092500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>88.976200</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>27.740000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.871000</td>\n",
       "      <td>8.725000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>10.710300</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>711.000000</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>396.900000</td>\n",
       "      <td>37.970000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               0           1           2           3           4           5   \\\n",
       "count  404.000000  404.000000  404.000000  404.000000  404.000000  404.000000   \n",
       "mean     3.745111   11.480198   11.104431    0.061881    0.557356    6.267082   \n",
       "std      9.240734   23.767711    6.811308    0.241238    0.117293    0.709788   \n",
       "min      0.006320    0.000000    0.460000    0.000000    0.385000    3.561000   \n",
       "25%      0.081437    0.000000    5.130000    0.000000    0.453000    5.874750   \n",
       "50%      0.268880    0.000000    9.690000    0.000000    0.538000    6.198500   \n",
       "75%      3.674808   12.500000   18.100000    0.000000    0.631000    6.609000   \n",
       "max     88.976200  100.000000   27.740000    1.000000    0.871000    8.725000   \n",
       "\n",
       "               6           7           8           9           10          11  \\\n",
       "count  404.000000  404.000000  404.000000  404.000000  404.000000  404.000000   \n",
       "mean    69.010644    3.740271    9.440594  405.898515   18.475990  354.783168   \n",
       "std     27.940665    2.030215    8.698360  166.374543    2.200382   94.111148   \n",
       "min      2.900000    1.129600    1.000000  188.000000   12.600000    0.320000   \n",
       "25%     45.475000    2.077100    4.000000  279.000000   17.225000  374.672500   \n",
       "50%     78.500000    3.142300    5.000000  330.000000   19.100000  391.250000   \n",
       "75%     94.100000    5.118000   24.000000  666.000000   20.200000  396.157500   \n",
       "max    100.000000   10.710300   24.000000  711.000000   22.000000  396.900000   \n",
       "\n",
       "               12  \n",
       "count  404.000000  \n",
       "mean    12.740817  \n",
       "std      7.254545  \n",
       "min      1.730000  \n",
       "25%      6.890000  \n",
       "50%     11.395000  \n",
       "75%     17.092500  \n",
       "max     37.970000  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(X_train).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>102.000000</td>\n",
       "      <td>102.000000</td>\n",
       "      <td>102.000000</td>\n",
       "      <td>102.000000</td>\n",
       "      <td>102.000000</td>\n",
       "      <td>102.000000</td>\n",
       "      <td>102.000000</td>\n",
       "      <td>102.000000</td>\n",
       "      <td>102.000000</td>\n",
       "      <td>102.000000</td>\n",
       "      <td>102.000000</td>\n",
       "      <td>102.000000</td>\n",
       "      <td>102.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.092336</td>\n",
       "      <td>10.901961</td>\n",
       "      <td>11.264902</td>\n",
       "      <td>0.098039</td>\n",
       "      <td>0.544156</td>\n",
       "      <td>6.354157</td>\n",
       "      <td>66.849020</td>\n",
       "      <td>4.011982</td>\n",
       "      <td>9.980392</td>\n",
       "      <td>417.500000</td>\n",
       "      <td>18.374510</td>\n",
       "      <td>364.163333</td>\n",
       "      <td>12.30549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>5.373088</td>\n",
       "      <td>21.572929</td>\n",
       "      <td>7.084148</td>\n",
       "      <td>0.298836</td>\n",
       "      <td>0.110015</td>\n",
       "      <td>0.672335</td>\n",
       "      <td>29.034993</td>\n",
       "      <td>2.379973</td>\n",
       "      <td>8.772121</td>\n",
       "      <td>177.390477</td>\n",
       "      <td>2.026785</td>\n",
       "      <td>79.138325</td>\n",
       "      <td>6.69540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.013110</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.220000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.392000</td>\n",
       "      <td>4.880000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>1.465500</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>187.000000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>24.650000</td>\n",
       "      <td>1.92000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.084840</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.455000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.445500</td>\n",
       "      <td>5.966000</td>\n",
       "      <td>42.450000</td>\n",
       "      <td>2.117375</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>279.250000</td>\n",
       "      <td>17.400000</td>\n",
       "      <td>377.692500</td>\n",
       "      <td>7.30500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.229015</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.795000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.532000</td>\n",
       "      <td>6.229000</td>\n",
       "      <td>73.750000</td>\n",
       "      <td>3.324850</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>330.000000</td>\n",
       "      <td>18.900000</td>\n",
       "      <td>392.110000</td>\n",
       "      <td>11.06000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>3.779445</td>\n",
       "      <td>16.250000</td>\n",
       "      <td>18.100000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.609000</td>\n",
       "      <td>6.633750</td>\n",
       "      <td>92.975000</td>\n",
       "      <td>5.276650</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>666.000000</td>\n",
       "      <td>20.200000</td>\n",
       "      <td>396.780000</td>\n",
       "      <td>15.91500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>25.046100</td>\n",
       "      <td>90.000000</td>\n",
       "      <td>27.740000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.871000</td>\n",
       "      <td>8.780000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>12.126500</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>711.000000</td>\n",
       "      <td>21.200000</td>\n",
       "      <td>396.900000</td>\n",
       "      <td>31.99000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               0           1           2           3           4           5   \\\n",
       "count  102.000000  102.000000  102.000000  102.000000  102.000000  102.000000   \n",
       "mean     3.092336   10.901961   11.264902    0.098039    0.544156    6.354157   \n",
       "std      5.373088   21.572929    7.084148    0.298836    0.110015    0.672335   \n",
       "min      0.013110    0.000000    1.220000    0.000000    0.392000    4.880000   \n",
       "25%      0.084840    0.000000    5.455000    0.000000    0.445500    5.966000   \n",
       "50%      0.229015    0.000000    9.795000    0.000000    0.532000    6.229000   \n",
       "75%      3.779445   16.250000   18.100000    0.000000    0.609000    6.633750   \n",
       "max     25.046100   90.000000   27.740000    1.000000    0.871000    8.780000   \n",
       "\n",
       "               6           7           8           9           10          11  \\\n",
       "count  102.000000  102.000000  102.000000  102.000000  102.000000  102.000000   \n",
       "mean    66.849020    4.011982    9.980392  417.500000   18.374510  364.163333   \n",
       "std     29.034993    2.379973    8.772121  177.390477    2.026785   79.138325   \n",
       "min      6.000000    1.465500    1.000000  187.000000   13.000000   24.650000   \n",
       "25%     42.450000    2.117375    4.000000  279.250000   17.400000  377.692500   \n",
       "50%     73.750000    3.324850    5.000000  330.000000   18.900000  392.110000   \n",
       "75%     92.975000    5.276650   24.000000  666.000000   20.200000  396.780000   \n",
       "max    100.000000   12.126500   24.000000  711.000000   21.200000  396.900000   \n",
       "\n",
       "              12  \n",
       "count  102.00000  \n",
       "mean    12.30549  \n",
       "std      6.69540  \n",
       "min      1.92000  \n",
       "25%      7.30500  \n",
       "50%     11.06000  \n",
       "75%     15.91500  \n",
       "max     31.99000  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(X_test).describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ **Question** ❓ Standardize `X_train` and `X_test` set without data leakage, and replace them keeping similar variable names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "mn = X_train.mean(axis=0)\n",
    "X_train -= mn\n",
    "std = X_train.std(axis=0)\n",
    "X_train /= std\n",
    "\n",
    "X_test -= mn\n",
    "X_test /= std\n",
    "#scaled price\n",
    "# Note that the quantities used for normalizing the test data are computed using the\n",
    "# training data. You should never use in your workflow any quantity computed on the\n",
    "# test data, even for something as simple as data normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.27224633, -0.48361547, -0.43576161, ...,  1.14850044,\n",
       "         0.44807713,  0.8252202 ],\n",
       "       [-0.40342651,  2.99178419, -1.33391162, ..., -1.71818909,\n",
       "         0.43190599, -1.32920239],\n",
       "       [ 0.1249402 , -0.48361547,  1.0283258 , ...,  0.78447637,\n",
       "         0.22061726, -1.30850006],\n",
       "       ...,\n",
       "       [-0.40202987,  0.99079651, -0.7415148 , ..., -0.71712291,\n",
       "         0.07943894, -0.67776904],\n",
       "       [-0.17292018, -0.48361547,  1.24588095, ..., -1.71818909,\n",
       "        -0.98764362,  0.42083466],\n",
       "       [-0.40422614,  2.04394792, -1.20161456, ..., -1.30866202,\n",
       "         0.23317118, -1.15392266]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler, normalize\n",
    "# Scaling the data so that all the features become comparable\n",
    "#scaler = StandardScaler()\n",
    "#X_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "# Normalizing the data so that the data approximately\n",
    "# follows a Gaussian distribution\n",
    "#X_normalized = normalize(X_scaled)\n",
    "\n",
    "from sklearn import preprocessing\n",
    "X_train_N = preprocessing.scale(X_train)\n",
    "X_train_N \n",
    "\n",
    "##X_train_scaled = pd.DataFrame(scaler.fit_transform(train_X))\n",
    "##X_train_scaled.columns\n",
    "\n",
    "###cols=train_X.columns\n",
    "#X_test_scaled = pd.DataFrame(scaler.fit_transform(test_X))\n",
    "#X_test_scaled.columns=cols\n",
    "#X_test_scaled.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.55369355, -0.48361547,  1.0283258 , ...,  0.78447637,\n",
       "        -3.48459553,  2.25092074],\n",
       "       [-0.39242675, -0.48361547, -0.16087773, ..., -0.30759583,\n",
       "         0.42733126,  0.47880119],\n",
       "       [-0.39982927, -0.48361547, -0.86940196, ...,  0.78447637,\n",
       "         0.44807713, -0.41415936],\n",
       "       ...,\n",
       "       [-0.20709507, -0.48361547,  1.24588095, ..., -1.71818909,\n",
       "         0.37051949, -1.49344089],\n",
       "       [-0.36698601, -0.48361547, -0.72093526, ..., -0.48960787,\n",
       "         0.39275481, -0.41829982],\n",
       "       [-0.0889679 , -0.48361547,  1.24588095, ..., -1.71818909,\n",
       "        -1.21946544, -0.40449827]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler = preprocessing.StandardScaler().fit(X_train)\n",
    "X_test_N =scaler.transform(X_test) \n",
    "X_test_N"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ **Question** ❓ To get a sense of a benchmark score you have to beat, what is the mean absolute error on the test set if your dumb prediction corresponds to the mean value of $y$ computed on the train set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average values : 22.395049504950492\n"
     ]
    }
   ],
   "source": [
    "#calculate the average score of the train dataset\n",
    "mean_values = y_train.mean(axis=0)\n",
    "print(\"Average values :\",mean_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 7.2, 18.8, 19. , 27. , 22.2, 24.5, 31.2, 22.9, 20.5, 23.2, 18.6,\n",
       "       14.5, 17.8, 50. , 20.8, 24.3, 24.2, 19.8, 19.1, 22.7, 12. , 10.2,\n",
       "       20. , 18.5, 20.9, 23. , 27.5, 30.1,  9.5, 22. , 21.2, 14.1, 33.1,\n",
       "       23.4, 20.1,  7.4, 15.4, 23.8, 20.1, 24.5, 33. , 28.4, 14.1, 46.7,\n",
       "       32.5, 29.6, 28.4, 19.8, 20.2, 25. , 35.4, 20.3,  9.7, 14.5, 34.9,\n",
       "       26.6,  7.2, 50. , 32.4, 21.6, 29.8, 13.1, 27.5, 21.2, 23.1, 21.9,\n",
       "       13. , 23.2,  8.1,  5.6, 21.7, 29.6, 19.6,  7. , 26.4, 18.9, 20.9,\n",
       "       28.1, 35.4, 10.2, 24.3, 43.1, 17.6, 15.4, 16.2, 27.1, 21.4, 21.5,\n",
       "       22.4, 25. , 16.6, 18.6, 22. , 42.8, 35.1, 21.5, 36. , 21.9, 24.1,\n",
       "       50. , 26.7, 25. ])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of y_test: (102,)\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of y_test:\",y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE for Test Data: 6.533042127742185\n"
     ]
    }
   ],
   "source": [
    "#Calculate the Mean Absolute Error on the test dataset\n",
    "print(\"MAE for Test Data:\",abs(y_test - mean_values).mean(axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. The model\n",
    "\n",
    "❓ **Question** ❓ Now, write a function `initialize_model` that generates a neural network with 3 layers: \n",
    "- a layer with 10 neurons and the `relu` activation function (choose the appropriate input dimension)\n",
    "- a layer with 7 neurons and the `relu` activation function\n",
    "- an appropriate layer corresponding to the problem at hand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "def initialize_model():\n",
    "  model = models.Sequential()\n",
    "  model.add(layers.Dense(10, activation='relu',input_shape=(X_train.shape[1],)))\n",
    "  model.add(layers.Dense(7, activation='relu'))\n",
    "  model.add(layers.Dense(1))\n",
    "  \n",
    "  return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. The optimizer\n",
    "\n",
    "❓ **Question** ❓ Write a function that :\n",
    "* takes as arguments a model and an optimizer, \n",
    "* `compiles` the model,\n",
    "* and returns the compiled model\n",
    "\n",
    "Please select the `loss function` to be optimized and  the `metrics` on which the model should be evaluated wisely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import Dense\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "def compile_model(model, optimizer_name):\n",
    "    model.compile(loss='mse', optimizer=optimizer_name, metrics=['mae'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ **Question** ❓ Initialize the model, compile it with the `adam` optimizer and fit it on the data. \n",
    "- Evaluate your model using an Early Stopping criterion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building your network\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import Dense\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "def initialize_model():\n",
    "  model = models.Sequential()\n",
    "  model.add(layers.Dense(10, activation='relu',input_shape=(X_train.shape[1],)))\n",
    "  model.add(layers.Dense(7, activation='relu'))\n",
    "  # regression - no activation function in the last layer\n",
    "  model.add(layers.Dense(1))\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = initialize_model();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 10)                140       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 7)                 77        \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 8         \n",
      "=================================================================\n",
      "Total params: 225\n",
      "Trainable params: 225\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "def compile_model(model, optimizer):\n",
    "    '''return a compiled model suited for the cifar tasks'''\n",
    "    model.compile(loss='mse', optimizer=optimizer,metrics=['mae', 'mse'])\n",
    "    \n",
    "    return model\n",
    "    # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    " model = compile_model(model, optimizer);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "9/9 [==============================] - 0s 19ms/step - loss: 113.3274 - mae: 9.1659 - mse: 113.3274 - val_loss: 158.3399 - val_mae: 10.2326 - val_mse: 158.3399\n",
      "Epoch 2/10\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 106.4546 - mae: 8.7803 - mse: 106.4546 - val_loss: 150.1662 - val_mae: 9.9083 - val_mse: 150.1662\n",
      "Epoch 3/10\n",
      "9/9 [==============================] - 0s 14ms/step - loss: 100.0260 - mae: 8.4348 - mse: 100.0260 - val_loss: 142.9898 - val_mae: 9.6007 - val_mse: 142.9898\n",
      "Epoch 4/10\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 94.4155 - mae: 8.1149 - mse: 94.4155 - val_loss: 136.3819 - val_mae: 9.3101 - val_mse: 136.3819\n",
      "Epoch 5/10\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 89.3820 - mae: 7.8144 - mse: 89.3820 - val_loss: 129.9856 - val_mae: 9.0257 - val_mse: 129.9856\n",
      "Epoch 6/10\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 84.6010 - mae: 7.5383 - mse: 84.6010 - val_loss: 123.8642 - val_mae: 8.7424 - val_mse: 123.8642\n",
      "Epoch 7/10\n",
      "9/9 [==============================] - 0s 26ms/step - loss: 80.0164 - mae: 7.2745 - mse: 80.0164 - val_loss: 117.9530 - val_mae: 8.4731 - val_mse: 117.9530\n",
      "Epoch 8/10\n",
      "9/9 [==============================] - 0s 16ms/step - loss: 75.4353 - mae: 7.0226 - mse: 75.4353 - val_loss: 112.2688 - val_mae: 8.2118 - val_mse: 112.2688\n",
      "Epoch 9/10\n",
      "9/9 [==============================] - 0s 21ms/step - loss: 71.1953 - mae: 6.7810 - mse: 71.1953 - val_loss: 106.4939 - val_mae: 7.9400 - val_mse: 106.4939\n",
      "Epoch 10/10\n",
      "9/9 [==============================] - 0s 15ms/step - loss: 66.8891 - mae: 6.5299 - mse: 66.8891 - val_loss: 100.7894 - val_mae: 7.6593 - val_mse: 100.7894\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "earlystop = EarlyStopping(monitor = 'val_loss',min_delta = 0.005,patience = 15, verbose = 1,restore_best_weights = True, mode='auto')\n",
    "history=model.fit(X_train_N, y_train, epochs=10, validation_split=0.3, validation_data=(X_test, y_test), callbacks=[earlystop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>mae</th>\n",
       "      <th>mse</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_mae</th>\n",
       "      <th>val_mse</th>\n",
       "      <th>epoch</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>84.601021</td>\n",
       "      <td>7.538344</td>\n",
       "      <td>84.601021</td>\n",
       "      <td>123.864227</td>\n",
       "      <td>8.742450</td>\n",
       "      <td>123.864227</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>80.016441</td>\n",
       "      <td>7.274481</td>\n",
       "      <td>80.016441</td>\n",
       "      <td>117.953018</td>\n",
       "      <td>8.473105</td>\n",
       "      <td>117.953018</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>75.435341</td>\n",
       "      <td>7.022618</td>\n",
       "      <td>75.435341</td>\n",
       "      <td>112.268784</td>\n",
       "      <td>8.211826</td>\n",
       "      <td>112.268784</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>71.195259</td>\n",
       "      <td>6.781044</td>\n",
       "      <td>71.195259</td>\n",
       "      <td>106.493927</td>\n",
       "      <td>7.939963</td>\n",
       "      <td>106.493927</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>66.889084</td>\n",
       "      <td>6.529932</td>\n",
       "      <td>66.889084</td>\n",
       "      <td>100.789444</td>\n",
       "      <td>7.659319</td>\n",
       "      <td>100.789444</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        loss       mae        mse    val_loss   val_mae     val_mse  epoch\n",
       "5  84.601021  7.538344  84.601021  123.864227  8.742450  123.864227      5\n",
       "6  80.016441  7.274481  80.016441  117.953018  8.473105  117.953018      6\n",
       "7  75.435341  7.022618  75.435341  112.268784  8.211826  112.268784      7\n",
       "8  71.195259  6.781044  71.195259  106.493927  7.939963  106.493927      8\n",
       "9  66.889084  6.529932  66.889084  100.789444  7.659319  100.789444      9"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hist = pd.DataFrame(history.history)\n",
    "hist['epoch'] = history.epoch\n",
    "hist.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ **Question** ❓ Re-run the same model on the same data using different optimizers (in a `for` loop). \n",
    "\n",
    "For each optimizer, plot the history and report the corresponding Mean Absolute Error. (see [here](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers)), as well as the time it took to fit your Neural Net."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss_mae(history):\n",
    "    fig, (ax1, ax2) = plt.subplots(1,2, figsize=(13,4))\n",
    "    ax1.plot(history.history['loss'])\n",
    "    ax1.plot(history.history['val_loss'])\n",
    "    ax1.set_title('Model loss')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylim(ymin=0, ymax=200)\n",
    "    ax1.legend(['Train', 'Validation'], loc='best')\n",
    "    ax1.grid(axis=\"x\",linewidth=0.5)\n",
    "    ax1.grid(axis=\"y\",linewidth=0.5)    \n",
    "    \n",
    "    ax2.plot(history.history['mae'])\n",
    "    ax2.plot(history.history['val_mae'])\n",
    "    ax2.set_title('MAE')\n",
    "    ax2.set_ylabel('MAE')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylim(ymin=0, ymax=20)\n",
    "    ax2.legend(['Train', 'Validation'], loc='best')\n",
    "    ax2.grid(axis=\"x\",linewidth=0.5)\n",
    "    ax2.grid(axis=\"y\",linewidth=0.5)    \n",
    "\n",
    "    plt.show()\n",
    "\n",
    "def plot_loss_mse(history):\n",
    "    fig, (ax1, ax2) = plt.subplots(1,2, figsize=(13,4))\n",
    "    ax1.plot(history.history['loss'])\n",
    "    ax1.plot(history.history['val_loss'])\n",
    "    ax1.set_title('Model loss')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylim(ymin=0, ymax=20)\n",
    "    ax1.legend(['Train', 'Validation'], loc='best')\n",
    "    ax1.grid(axis=\"x\",linewidth=0.5)\n",
    "    ax1.grid(axis=\"y\",linewidth=0.5)    \n",
    "\n",
    "    \n",
    "    ax2.plot(history.history['mse'])\n",
    "    ax2.plot(history.history['val_mse'])\n",
    "    ax2.set_title('MSE')\n",
    "    ax2.set_ylabel('MSE')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylim(ymin=0, ymax=200)\n",
    "    ax2.legend(['Train', 'Validation'], loc='best')\n",
    "    ax2.grid(axis=\"x\",linewidth=0.5)\n",
    "    ax2.grid(axis=\"y\",linewidth=0.5)    \n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 936x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_loss_mae(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "tags": [
     "delete"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s 2ms/step - loss: 21.2817 - mae: 2.9146 - mse: 21.2817\n",
      "MAE with the rmsprop optimizer: 2.9146  reached in 20 s after 241 epochs\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 936x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s 3ms/step - loss: 16.3964 - mae: 2.8439 - mse: 16.3964\n",
      "MAE with the adam optimizer: 2.8439  reached in 29 s after 333 epochs\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAwsAAAEWCAYAAAApcQvWAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/d3fzzAAAACXBIWXMAAAsTAAALEwEAmpwYAABhYElEQVR4nO3dd5zdZZn//9d1+pnek8mkQyAQSoCICoqwoALqYhfWAosua1vdr2VX3e9a1+/u+tNdZXVVXFnEVdEVQVQsgAULLfSSIY30OjNJps+ccv/+uD+TnExmkpnJnDbzfj4e53HOp1/nk8m5z3XuZs45RERERERERgsVOwARERERESlNShZERERERGRMShZERERERGRMShZERERERGRMShZERERERGRMShZERERERGRMShZEJsjMFpuZM7PIBPa9xsz+cLznERERESkmJQsyI5nZJjMbNrOmUesfC76oLy5SaCIiMgNNptwxs08G684dte81ZpYxs95Rj3kFehsiR1CyIDPZc8BVIwtmdjqQLF44IiIywx2z3DEzA94KdAFXj3GO+5xzVaMeO/IZtMjRKFmQmezbwNtylq8Gbs7dwcxqzexmM9trZpvN7P+aWSjYFjazz5tZh5ltBF4xxrHfNLOdZrbdzP7JzMKTDdLM5pnZHWbWZWbrzeyvcrada2arzazbzHab2b8F6xNm9j9m1mlm+83sITObM9lri4jItDpmuQO8GJgHvB+40sxiBYpNZEqULMhMdj9QY2anBF/i3wT8z6h9/gOoBZYCL8F/yP9lsO2vgFcCZwGrgNePOvZbQBo4MdjnZcA7phDn94Bt+MLj9cD/M7OLg21fAr7knKsBTgB+EKy/Ooh7AdAIvBMYmMK1RURk+kyk3Lka+Anw/WD5lQWMT2TSlCzITDfyK89LgXZg+8iGnA/yjzrnepxzm4Av4KuHAd4IfNE5t9U51wX8c86xc4DLgL91zvU55/YA/w5cOZngzGwB8CLg751zg865x4D/yokhBZxoZk3OuV7n3P056xuBE51zGefcw8657slcW0RE8uJo5U4F8Abgu865FPBDjmyK9IKgxnjksaFAcYuMSaOxyEz3beBeYAlHVgU3ATFgc866zUBb8HoesHXUthGLgCiw0zc/BXzynbv/RMwDupxzPaOusyp4/Xbg00C7mT0HfMo599PgfS0AbjGzOvwvV/8QFD4iIlI8Ryt3XoOvkb4zWP4OcLeZNTvn9gbr7nfOvaggkYpMgGoWZEZzzm3Gdzi7HPjRqM0d+F/oF+WsW8ihX4F24r+Q524bsRUYApqcc3XBo8Y5t2KSIe4AGsyseqwYnHPrnHNXAS3AvwI/NLNK51zKOfcp59ypwHn4auy3ISIiRXWMcudqoArYYma7gP/F//B0FSIlSsmCzAZvB/7MOdeXu9I5l8H3AfismVWb2SLgAxxqX/oD4H1mNt/M6oGP5By7E/gV8AUzqzGzkJmdYGYvmUxgzrmtwJ+Afw46LZ8RxPsdADN7S/CLUxbYHxyWMbOLzOz0oClVNz7pyUzm2iIikjdjlTttwMX4H3dWBo8z8T8EjTUqkkhJULIgM55zboNzbvU4m/8G6AM2An8AvgvcGGz7BvBL4HHgEY78heht+GZMzwD78G1PW6cQ4lXAYnwtw23AJ5xzdwXbLgWeNrNefGfnK51zg8Dc4HrdwBrgdxzZiU5ERIpgnHLnxcBjzrlfOed2jTyA64EzzOy0YL8XjjHPwvMK+gZEcphzrtgxiIiIiIhICVLNgoiIiIiIjClvyYKZLTCz35jZGjN72szeH6xvMLO7zGxd8Fyfc8xHg0mpnjWzl+crNhERKZ6plA+jjr80KCfWm9lHxtpHRESmR96aIZlZK9DqnHskGOnlYeDVwDX4oSL/JfiQr3fO/b2ZnYqfnOpc/HCSdwMnBZ1QRURkhphs+TDq2DCwFj+G/TbgIeAq59wzBXwLIiKzRt5qFpxzO51zjwSve/CdMNuAK/Az3xI8vzp4fQVwi3NuyDn3HLAenziIiMgMMoXyIde5wHrn3Ebn3DBwS3CciIjkQUEmZTOzxcBZwAPAnGDYSZxzO82sJditDT9N+ohtHJocK/dc1wHXAVRUVJyzZMmSKcWUyWQIh8PH3G8wnWXrgRRt1VEqYiHCw91Ee7czVLsUF45P6dpTNdGYS005xq2YC0Mx59/TTz/d4ZxrLnYc45lg+ZCrjcMnP9wGPH+cc09rebH1QIoq10trdhdDNUtwkcSUzlcI5fZ3Coq5kMoxbsWcf+OVF3lPFsysCrgV+FvnXHfObLdH7DrGuiPaSDnnbgBuAFi1apVbvXq8ETGPrr29neXLlx9zvzU7u7nsS7/nP99yNpee1gprfwXffQO843swf9Uxj59OE4251JRj3Iq5MBRz/pnZ5mPvVRyTKB8OO2yMdWO2p53u8uKDP3iczrX3c1P67+DK62H55VM6XyGU298pKOZCKse4FXP+jVde5HU0JDOL4guC7zjnRsao3x20Vx1pt7onWL+Nw2fLnY8fd76oklGfEQ6kgq4T8WCi3aHuIkUkIlL+Jlk+5CpaWbG4sYKne6v8Qvf2o+8sIjJD5HM0JAO+Caxxzv1bzqY7ODRT4dXAj3PWX2lmcTNbAiwDHsxXfBOVjPlkoX94dLLQU6SIRETK2xTKh1wPAcvMbImZxYArg+PyblFTJR3U4EIR6C76b1kiIgWRz2ZI5wNvBZ40s8eCdR8D/gX4gZm9HdgCvAHAOfe0mf0APxtuGnhPKYyElBipWVCyICIyXSZVPpjZPOC/nHOXO+fSZvZe/OzqYeBG59zThQh6cWMFjhCDiRaSPTsLcUkRkaLLW7LgnPsDY7ctBbh4nGM+C3w2XzFNRUVQszB4RDMkJQsiU5VKpdi2bRvDw8OsWbOm2OFMSiqVKsmYE4kE8+fPJxqNFjuUY5ps+eCc2wFcnrN8J3BnfqIb3/z6CgC6o80k1QxJpCBUXky/yZYXBRkNqZxFwyEiIcvps1DjnwfVZ0FkqrZt20Z1dTVz5syhoqKi2OFMysDAAMlksthhHMY5R2dnJ9u2bWOqI/7IsdVXRElEQ3SGGpnTvaXY4YjMCiovptdUyou8dnCeKZLR8KE+C+EIRCvUwVnkOAwODtLY2MgER7+RYzAzGhsbGRwcLHYoM5qZMa8uyU7X4Pss5GlSUxE5ROXF9JpKeaFkYQISsfChZkjgmyKpGZLIcdEH//TS/SyMtrokm1J1kOqHwQPFDkdkVtDn2/Sa7P1UsjAByWj4UAdnULIgIjJLtdYm2DAYNEfViEgiMgsoWZiAilj4UJ8FULIgUuY6OztZuXIlK1euZO7cubS1tR1cHh4ePuqxDz/8MO973/sKFKmUmnl1STYOBO2m+8aaBkJEZhKVF+rgPCGJ3D4LoGRBpMw1Njby2GOPAfDJT36SqqoqPvShDx3cnk6niUTG/ng855xzeNGLXlSIMKUEzatLstfV+oXevcUNRkTyTuWFahYmJBkd3WehRh2cRWaYa665hg984ANcdNFF/P3f/z0PPvgg5513HmeddRbnnXcezz77LAD33nsvr3zlKwFfcFx77bVceOGFLF26lOuvv76Yb0EKoC03WVDNgsisNNvKC9UsTEAyFmZ3d+rQiniNahZEpsmnfvI0z+yY3uT71Hk1fOJVKyZ93Nq1a7n77rsJh8N0d3dz7733EolEuPvuu/nYxz7GrbfeesQx7e3t/OY3v6Gnp4eTTz6Zd73rXWUx14FMzby6JN1UkglFCffuLnY4IrOKyoviULIwAZXxCH1D6UMr4tWqWRCZgd7whjcQDvuJGA8cOMDVV1/NunXrMDNSqdSYx7ziFa8gHo8Tj8dpaWlh9+7dzJ8/v5BhSwG11iYAoz/aQLWaIYnMWrOpvFCyMAFV8TC9Q2P0WXAONJyXyHGZyi86+VJZWXnw9T/+4z9y0UUXcdttt7Fp0yYuvPDCMY+Jx+MHX4fDYdLp9Jj7ycyQiIZprIxxIFRPtZohiRSUyoviUJ+FCagaq2bBZf042yIyIx04cIC2tjYAbrrppuIGIyVlXl2SDlcLaoYkIsz88kLJwgRUxiMMpDKkM1m/IhGMsa1+CyIz1t/93d/x0Y9+lPPPP59MJnPsA2TWmFeXYGemRqMhiQgw88sLNUOagKq4v019wxlqkyHfwRlgsBuq5xYxMhE5Xp/85CfHXP/CF76QtWvXHlz+zGc+A8AFF1zAy1/+8jGPfeqpp/ISo5SW1tokW4arcOm9WDYLIf3uJjIbzNbyQp9wE3AwWRhpihSv9s+qWRARmXXa6pLsTFdjLgMDXcUOR0Qkr5QsTEBlkCz0HpEsaEQkEZHZprUuwV5X5xd61clZRGY2JQsTUDVusqCaBRGR2aa5Ku47OIMmZhORGU/JwgRUJdQMSUREvObqOB0EfddUsyAiM5yShQmojAU1C4MjyYJGQxIRma2aq+NqhiQis0beRkMysxuBVwJ7nHOnBeu+D5wc7FIH7HfOrTSzxcAa4Nlg2/3OuXfmK7bJUjMkEZHpNZkyYoxjNwE9QAZIO+dWFSDkg6riEYajVaQtSkTNkERkhstnzcJNwKW5K5xzb3LOrQw+/G8FfpSzecPItlJKFGCMZkjhKESSMHSgiFGJyPG48MIL+eUvf3nYui9+8Yu8+93vHnf/1atXA3D55Zezf//+I/b55Cc/yec///mjXvf222/nmWeeObj88Y9/nLvvvnuS0c8INzG5MmK0i4J9C5ooAJgZzdUJesL1qlkQmeGOp6x49atfPSPKirwlC865e4Exx5QzMwPeCHwvX9efTpXxMJBTswC+dkE1CyJl66qrruKWW245bN0tt9zCVVdddcxj77zzTurq6qZ03dEFwKc//WkuueSSKZ2rnJV7GdFcFafL6pQsiMxwx1NW3H777TOirChWn4UXA7udc+ty1i0xs0fN7Hdm9uIixTWmeCRMNGz0DuXMyqdkQaSsvf71r+enP/0pQ0NDAGzatIkdO3bw3e9+l1WrVrFixQo+8YlPjHns4sWL6ejoAOCzn/0sJ598MpdccgnPPvvswX2+8Y1v8LznPY8zzzyT173udfT39/OnP/2JO+64gw9/+MOsXLmSDRs2cM011/DDH/4QgHvuuYezzjqL008/nWuvvfZgbIsXL+YTn/gEZ599Nqeffjrt7e35vDWlYKwyIpcDfmVmD5vZdQWM66Dm6jh7XK1GQxKZ4Y6nrFi+fPmMKCuKNYPzVRz+i9FOYKFzrtPMzgFuN7MVzrkjJjIICobrANra2qZ8Izo6OiZ1bDJibNu9l5FDFrkomc6dbCtgoT3ZmEtFOcatmPMrlUoxMDBAOp0m/ZMPYXumdyZL13IaqUs+c9R9KioqOOecc/jxj3/Mq171Kr797W/zute9jg996EM0NDSQyWS4/PLLecUrXsHpp59ONptlcHCQdDqNc46BgQH++Mc/8r3vfY8//elPpNNpzjvvPM444wwGBga47LLLeMtb3gL4Kuevfe1rvOtd7+IVr3gFl112Ga95zWsAyGQyDA8Ps2/fPq6++mruvPNOli1bxjve8Q6uv/563vve9+Kco7a2lj/+8Y98/etf51/+5V/46le/OuZ9LZe/gWMYXUaMdr5zboeZtQB3mVl7UFNxmHyWF9H0ADtSVaT3b2R9Cd7zcvo8GKGYC6ec4i52eTHVsmJgYACgJMuKkfs60b+BgicLZhYBXgucM7LOOTcEDAWvHzazDcBJwOrRxzvnbgBuAFi1apVbvnz5lOJob29nMsfWVOwgmqw+dMwDLZBJTeocx2uyMZeKcoxbMefXmjVrSCaTDAwMEIlEIBSe3gtEIkSSyWPu9pa3vIXbbruNN77xjdx6663ceOON/OQnP+GGG24gnU6zc+dONm7cyLnnnksoFCKRSBCJRDAzkskkDz30EK997WtpbGwE4IorriAajZJMJtmwYQNXXXUV+/fvp7e3l5e//OUkk0nC4TCxWIxkEN/I8pYtW1i6dClnnHEGANdeey1f+cpX+PCHP4yZ8aY3vYlkMskLX/hCfvrTnx48Plc0Gi2bv4HxjFVGjOac2xE87zGz24BzgSOShXyWFydtD7NzQw3hof0sP+kkCJXW4ILl9HkwQjEXTjnFXQrlxVTKipHP6FIsK2By5UUxahYuAdqdc9tGVphZM9DlnMuY2VJgGbCxCLGNqyoeGdVnoQb2bSpaPCIzxmX/UrRLv/rVr+YDH/gAjzzyCAMDA9TX1/P5z3+ehx56iPr6eq655hoGBwePeg7fvP5I11xzDbfffjtnnnkmN910E7/97W+Peh7n3FG3x+NxwBcY6XT6qPuWuSPKiFxmVgmEnHM9weuXAZ8uZIDgmyGtczWYy8DgfqhoKHQIIrNPkcqL2V5W5O2nEDP7HnAfcLKZbTOztwebruTI6uULgCfM7HHgh8A7nXNjdnwrlqp45NBoSOD7LAwe0UpKRMpIVVUVF154Iddeey1XXXUV3d3dVFZWUltby+7du/n5z39+1OMvuOACbrvtNgYGBujp6eEnP/nJwW09PT20traSSqX4zne+c3B9dXU1PT1H9ndavnw5mzZtYv369QB8+9vf5iUveck0vdPSM5kywszmmdmdweIc4A9BefEg8DPn3C8KFfeI5qo4nS4YRruvo9CXF5ECmu1lRd5qFpxzY3YTd85dM8a6W/HD5JWsyniEff3Dh1bEq2FIyYJIubvqqqt47Wtfyy233MLy5cs566yzWLFiBUuXLuX8888/6rFnn302b3rTm1i5ciWLFi3ixS8+NDbDZz7zGZ7//OezaNEiTj/99IMf+ldeeSV/9Vd/xfXXX3+wsxpAIpHgv//7v3nDG95AOp3mec97Hu98Z0mNIj2tJllG7AAuD15vBM7Ma3AT0Fwdp5Nav9DfgW85KyIz1WwuK+xY1RmlbNWqVW5kLNvJmmx7vfd85xHW7Orm1x+80K+459Pwhy/CxzthnKql6VZObQxzlWPcijm/1qxZwymnnMLAwMC47SlLVSnHPHJfc5nZw8WYi6DUTHd5sWP/AO/41xu5M/4xeOO34dQ/n44wp005fR6MUMyFU05xq7zIj8mUF6XVI6uEHdkMqQZcBlIDxQtKRESKorEqRqer8Qt9e4sbjIhIHilZmKDKeITewVF9FkBzLYiIzELxSJhsMujU3N9Z3GBERPJIycIEVcXD9A1nyGaDZlvx4BclJQsiU1LOTSBLke5n4dVVV9EfqlQHZ5E80+fb9Jrs/VSyMEFVCd8XvD8VzOJ8sGbhQJEiEilfiUSCzs5OFQDTxDlHZ2cniUSi2KHMKs3VcQ5YbdDBWUTyQeXF9JpKeVGsGZzLTmXc36rewTRV8QgkglEwBpUsiEzW/Pnz2bZtGzt27CAajRY7nElJpVIlGXMikWD+/PnFDmNWaa6O07mzmlb1WRDJG5UX02+y5YWShQmqGkkWRjo5K1kQmbJoNMqSJUvKakSOEeUYs+RHU1WcXZlqVvR1UJgx8URmH5UXxadmSBNUGfPJwsERkZJ1/nlgf1HiERGR4mqujrM3U43rUwdnEZm5lCxM0EifhSNrFvYXJyARESmqpqo4XVRj/R2QzRY7HBGRvFCyMEFHNEOKVkAoqmZIIiKzVFNVjC5Xg7mMfjgSkRlLycIEjXRwPtgMycw3RVIzJBGRWampKk7HyMRsmmtBRGYoJQsTdETNAvimSKpZEBGZlZqr43QxMouzhk8VkZlJycIEjZ0s1KnqWURklmqo9M2QANDwqSIyQylZmKBENETIcpohgWoWRERmsWg4RDrR4Bc0MZuIzFBKFibIzKiKR+gdzEkW1GdBRGRWi1Q3+xcaPlVEZiglC5NQnYjSo5oFEREJ1FZX0WcVqlkQkRlLycIkVCci9AyO0WfBuWKFJCIiRdRUFWcfNeqzICIzlpKFSahJROkeSB1akaiFbBpS/cULSkREiqapKk5HtlqjIYnIjKVkYRJqkhG6R/dZAPVbEBGZpZqqY+zO1pLt2V3sUERE8iJvyYKZ3Whme8zsqZx1nzSz7Wb2WPC4PGfbR81svZk9a2Yvz1dcx6MmEaVncFTNAmj4VBGRSZpsGTHq2EuDsmK9mX2kcFEfqakqzh5Xh+tVsiAiM1M+axZuAi4dY/2/O+dWBo87AczsVOBKYEVwzH+aWTiPsU1JdSIyqhlSnX9WJ2cRkcm6iQmWEbmCsuErwGXAqcBVQRlSFM1BshAe3AfpoWKFISKSN3lLFpxz9wJdE9z9CuAW59yQc+45YD1wbr5im6qapB8NKZsNOjSP1CyoGZKIyKRMsozIdS6w3jm30Tk3DNyCL0OKork6zh7q/YJqF0RkBooU4ZrvNbO3AauBDzrn9gFtwP05+2wL1h3BzK4DrgNoa2ujvb19SkF0dHRM+tjB7v04B48+tYbKWIhoTxcnADuea6ebpVOKYzKmEnMpKMe4FXNhKGYZw1hlRK42YGvO8jbg+WOdqBDlxf6+NLtdHQCbnnqAwabSGPCiHP9OFXPhlGPcirl4Cp0sfBX4DOCC5y8A1wI2xr5jjkfqnLsBuAFg1apVbvny5VMKpL29nckee2LPFljdydyFS2irS0J/C/wM5tUnmTfFOCZjKjGXgnKMWzEXhmKWUcYrI3KVVHmRymT55x/+DoDFDXEokb+Ncvw7VcyFU45xK+biKehoSM653c65jHMuC3yDQ02NtgELcnadD+woZGwTUZ2IAhzqt3Cwg7P6LIiIHK+jlBG5Sqq8iIZDDCaa/IKaIYnIDFTQZMHMWnMWXwOMjIJxB3ClmcXNbAmwDHiwkLFNRE2QLBycmC0UhniN+iyIiEyDo5QRuR4ClpnZEjOL4QfHuKMQ8Y0nXNVMlhD07CpmGCIieZG3Zkhm9j3gQqDJzLYBnwAuNLOV+CrjTcBfAzjnnjazHwDPAGngPc65TL5im6qapL9dR0zMppoFEZFJmUwZYWbzgP9yzl3unEub2XuBXwJh4Ebn3NOFfweHNFQn2d9XR0OvkgURmXnyliw4564aY/U3j7L/Z4HP5iue6XCwGdLgqOFTNc+CiMikTKaMcM7tAC7PWb4TOGJY1WJprk6wZ1cDDd07ix2KiMi00wzOk1CT8LlVT+4szqpZEBGZ1ZqqYmzLNED39mKHIiIy7ZQsTMIRHZwBknXqsyAiMos1VcXZmqnHHdhW7FBERKadkoVJiEVCJKKhUc2QVLMgIjKbNVfF2eEaseFelQciMuMoWZikmkR0VDOkOvVZEBGZxZqqY+x0jX7hgJoiicjMomRhkmqS0SNrFoZ7IZMa/yAREZmxmoKaBUD9FkRkxlGyMEnViQjdAzk1CxUN/nlgX3ECEhGRomqqiufULKjfgojMLEoWJsk3Q8qpRRhJFvo7ixOQiIgUVWNVjD3U+YnZVLMgIjOMkoVJ8s2QcmsWgl+TlCyIiMxK8UiYqmSCA7EW2Lep2OGIiEwrJQuTVJOIHD506sFkoas4AYmISNE1VcXYFW6Dzg3FDkVEZFopWZik6mA0JOecX6GaBRGRWa+pKs4W5kLXBhgpH0REZgAlC5NUk4wwnMkylM76FUn1WRARme2aquOsz8zx8yyopllEZhAlC5NUM3oW52gCYlUqHEREZrHmqjjPDDX7hS41RRKRmUPJwiRVJyIAh3dyTjaoZkFEZBZrqorxzHCLX+hcX9xgRESmkZKFSapJBjULo4dPHVDNgojIbNVcHWera8ZZWJ2cRWRGUbIwSUc0QwLfyVk1CyIis1ZTVZw0EYar5qsZkojMKEoWJqkmaIbUM3quBSULIiKzVlNVHICeyoWqWRCRGUXJwiSNNEM6cETNgpohiYjMVk3VPlnoii+Aro0aPlVEZgwlC5NUO2afhUYY6ob0cJGiEhGRYmqsjAGwMzwPhnuhd3eRIxIRmR55SxbM7EYz22NmT+Ws+//MrN3MnjCz28ysLli/2MwGzOyx4PG1fMV1vBLRMPFIiAP9uclCvX8e2FecoEREysxkyogxjt1kZk8G5cXqggV9FIlomJpEhM02z69QUyQRmSHyWbNwE3DpqHV3Aac5584A1gIfzdm2wTm3Mni8M49xHbe6iij7+0fVLID6LYiITNxNTK6MGO2ioLxYlaf4Jq21Nsma4WCuBQ2fKiIzRN6SBefcvUDXqHW/cs6N9Ay+H5ifr+vnU20yemSfBVCyICIyQTOxjGitS/B0Xw2E40oWRGTGiBTx2tcC389ZXmJmjwLdwP91zv1+rIPM7DrgOoC2tjba29undPGOjo4pHxsjzY7O/QePj+/vZgmwff0T9IzM4JkHxxNzMZVj3Iq5MBSzHMXoMiKXA35lZg74unPuhrF2KnR5kXRDbO4aYLB2PqlNj7K9iH8n5fh3qpgLpxzjVszFU5Rkwcz+AUgD3wlW7QQWOuc6zewc4HYzW+Gc6x59bFAo3ACwatUqt3z58inF0N7ezlSPbX2wl61d/YeO76mDX0BbXQKmeM6JOJ6Yi6kc41bMhaGYZSxjlBGjne+c22FmLcBdZtYe1FQcptDlxanbw/x87Vqiy08lsXdNUf9OyvHvVDEXTjnGrZiLp+CjIZnZ1cArgTc758eWc84NOec6g9cPAxuAkwod20TVJqOHT8qWbPDPmsVZROS4jFVGjOac2xE87wFuA84tXITja61LAtBTuQS6ntMIeSIyIxQ0WTCzS4G/B/7cOdefs77ZzMLB66XAMmBjIWObjLpklP25yUIkBrFqzbUgInIcxisjRu1TaWbVI6+BlwFPjbVvoc2rTQCwN74QXAb2bSpuQCIi0yCfQ6d+D7gPONnMtpnZ24EvA9X4auPcIVIvAJ4ws8eBHwLvdM6V7Dfvuooo/cMZhtPZQysrGtTBWURkgiZTRpjZPDO7Mzh0DvCHoLx4EPiZc+4XRXgLR5gX1CxsCbX5FR1rixiNiMj0yFufBefcVWOs/uY4+94K3JqvWKZbbc4szs3BrJ1+FmclCyIiEzHJMmIHcHnweiNwZh5Dm7K5Qc3C+kwrFwN0ritqPCIi00EzOE9BbYWfqfPAQE57VCULIiKzWiIaprEyxqa+CFTNgQ4Nnyoi5U/JwhTUBTULh03MVtkMvXuLFJGIiJSC1roEOw8MQOMyNUMSkRlBycIU5DZDOqh6LvTugmx2nKNERGSma61NsnP/IDQFycLYAzqJiJQNJQtTUFcxRs1C9VzIpjV8qojILDavNsGOAwM+WRjcr+apIlL2lCxMQV3S91nYP7pmAaBnZxEiEhGRUtBal6RnME1/zVK/okOdnEWkvClZmILqRASz0c2QWv1zz+7iBCUiIkXXGoyItDu2wK9QvwURKXNKFqYgFDJqElEO9OeMhlQ1xz+rZkFEZNZqC+Za2JxuhHBcw6eKSNlTsjBFdRXRcZoh7SpOQCIiUnQjE7Pt6E5B4wkaPlVEyp6ShSmqTUYPb4YUiUOywY+IJCIis9KcmgTRsLF1Xz80nqhmSCJS9pQsTFFtMnr4aEjgaxdUsyAiMmuFQ8b8+gq2dPVD00mwbxOkh495nIhIqVKyMEV1FbHDaxYgSBbUZ0FEZDabX59ka1e/Hz7VZWDfc8UOSURkypQsTFFtMjJGstCq0ZBERGa5hQ0jNQvL/AoNnyoiZUzJwhTVJWPs7x8mm82ZnbNqjmZxFhGZ5RY2VLC/P0V31RK/Qv0WRKSMTShZMLNKMwsFr08ysz83s2h+QyttdRVRsg56h9OHVla3+lmcNWOniMwCZlZzlG0LCxlLKVnYUAHA1r4wVM2FTo2IJCLla6I1C/cCCTNrA+4B/hK4KV9BlYOapM+VDvSPMXyqRkQSkdnhtyMvzOyeUdtuL2gkJWTBSLIw0hRJNQsiUsYmmiyYc64feC3wH8651wCn5i+s0lcXJAv7x0oWNCKSiMwOlvO64SjbZpWRZGFL18jwqevAuWMcJSJSmiacLJjZC4E3Az8L1kXyE1J5qK+MAbAvdxZnJQsiMru4cV6PtTxr1Caj1Cajh4ZPHdyv5qkiUrYm+oX/b4GPArc55542s6XAb/IWVRloGCtZqJrjn5UsiMjs0GJmH8DXIoy8JlhuLl5YxedHRBqAFSMjIq2FyqbiBiUiMgUTqllwzv3OOffnzrl/DTo6dzjn3ne0Y8zsRjPbY2ZP5axrMLO7zGxd8Fyfs+2jZrbezJ41s5dP+R0VSEOFTxY6e3OShZFZnDXXgojMDt8AqoGqnNcjy/91tAMnW0aMOvbSoKxYb2YfmbZ3M40WNlSwTcOnisgMMNHRkL5rZjVmVgk8AzxrZh8+xmE3AZeOWvcR4B7n3DJ8R+mPBOc/FbgSWBEc859mFp7wuyiC2mSUkI2qWQCoaYPu7cUJSkSkgJxznxrvAdx5jMNvYoJlRK6gbPgKcBm+79xVQRlSUuY3JNm2b4BM9XwIx9XJWUTK1kT7LJzqnOsGXo0vABYCbz3aAc65e4GuUauvAL4VvP5WcL6R9bc454acc88B64FzJxhbUYRCRn1FjM6+UclC7Xw4sK04QYmIFJGZnWpmnzazdcBXj7bvJMuIXOcC651zG51zw8AtwXElZWFDBcOZLLt7U9B4goZPFZGyNdE+C9FgXoVXA192zqXMbCqd1+Y453YCOOd2mllLsL4NuD9nv23BuiOY2XXAdQBtbW20t7dPIQzo6OiY8rEjKqOOLbs6DzvPHFdFTdcm1h3nuccyHTEXQznGrZgLQzGXPzNbBFwVPNLAImCVc27TFE43XhmRqw3YmrO8DXj+OLEVrbywvn4A/vBYO5fG55LY8RQbC/h3U45/p4q5cMoxbsVcPBNNFr4ObAIeB+4NCofuaYxjrCH2xkxGnHM3ADcArFq1yi1fvnxKF2xvb2eqx46YW7+PYTj8PJ1nwLr/ZfniVkjUHtf5R5uOmIuhHONWzIWhmMubmf0JqMX/uv9659w6M3tuionChC87xrqSKy+SzX3wq51Q1UTNknNg2+9YfuIS37etAMrx71QxF045xq2Yi2eiHZyvd861Oecud95m4KIpXG+3mbUCBM97gvXbgAU5+80Hdkzh/AXVWBlj3+hmSHXB29i/9cgDRERmlr34Ds1zODT60fEMmTpeGZGrLMqLeXVJQjYyMdtJ4DLQ9VyxwxIRmbSJdnCuNbN/M7PVweMLQOUUrncHcHXw+mrgxznrrzSzuJktAZYBD07h/AVVXxmj64g+C0EZdkDJgojMbM65K4DTgUeAT5nZc0C9mU21z9l4ZUSuh4BlZrbEzGL4wTHumOL18iYaDtFamzw0izOok7OIlKWJdnC+EegB3hg8uoH/PtoBZvY94D7gZDPbZmZvB/4FeGnQ+e2lwTLOuaeBH+BHWvoF8B7nXGbyb6ewGitj7OsfJpvN+SGtVjULIjJ7OOcOOOdudM69FHgB8Angi2Z21A/ByZQRZjbPzO4MrpcG3gv8ElgD/CAoQ0qOn2uhHxqVLIhI+Zpon4UTnHOvy1n+lJk9drQDnHNXjbPp4nH2/yzw2QnGUxLqK2JkHRwYSB2c0ZnKZj9M3v7NxQ1ORKTAnHO7geuB64O+bUfbd8JlhHNuB3B5zvKdHHto1qJb2FDBPe17IF7lh9XWXAsiUoYmmiwMmNmLnHN/ADCz84GB/IVVHhqrfILQ1T98KFkIhaBhKXRtLGJkIiL5Z2bHav7z5wUJpEQtbKygo3eI/uE0FU3LVLMgImVposnCO4GbzWxkeJ99HGpXOmvVB7M4d/UNc0JzzobGE/QLkojMBi/ED2P6PeABxh6paNZa2FABwObOfk5pOgke+x44B6bbJCLlY6KjIT3unDsTOAM4wzl3FvBneY2sDDRUHkoWDtN4oq9ZyJZ8twsRkeMxF/gYcBrwJXw/gw7n3O+cc78ramQlYHGjHwdkc2efHxFpuAd6dhU5KhGRyZloB2cAnHPdwUzOAB/IQzxl5WAzpLGShWwK9m8pQlQiIoXhnMs4537hnLsa37l5PfBbM/ubIodWEhY1+ZqFTZ3B8KmgpkgiUnYmlSyMMuvrUXObIR2m8QT/3LmhwBGJiBRWMOT1a4H/Ad6D7+D8o+JGVRpqElEaK2Ns6uhTsiAiZWuifRbGcjwT78wIiWiYylh47JoFgM51sOySwgcmIlIAZvYtfBOknwOfcs49VeSQSs7ipko2dfZB9ekQq1Z/NhEpO0dNFsysh7GTAgOSeYmozNSPNYtzZTMkG2DPM8UJSkSkMN4K9AEnAe+zQx13DXDOuZpiBVYqFjVW8Kf1nb5Tc9My2Nte7JBERCblqMmCc666UIGUq8bKGJ2jkwUzmLMCditZEJGZyzl3PE1ZZ4XFjZX86JHtDAxnSM5ZAWt/UeyQREQmRR/0x6k+mMX5CHNWwJ41kM0WPigRESkJi5uCEZG6+ny50LcXevcUOSoRkYlTsnCcGipjdPaOkSy0nAqpPti/qeAxiYhIaVjcGIyI1NHvkwWA3eraISLlQ8nCcWqoGK9m4TT/rKZIIiKz1qJgroVNnX3QMpIsPF3EiEREJkfJwnFqqIrRP5yhfzh9+IaW5YCpk7OIyCxWm4zSUBnzE7NVNkJ1q5IFESkrShaOU3NVHICOnlG1C7FKqF+sQkFEZJZb3FjhmyFBMPiFmiGJSPlQsnCcWmoSAOzpGTxy45wVShZERGa5xY3BXAvg+7PtfRYy6aMfJCJSIpQsHKeRmoW9PUNHbpyzAro2QGqgwFGJiEipWNxUyc4DgwymMr4/W2YYOtcXOywRkQlRsnCcmquDZKF3jGSh5VRwWU3CIyIyiy0KRkTa3KkRkUSk/ChZOE4NlTHCIWNP9xjJwryV/nn7wwWNSURESsfi3BGRmk6CUBR2PVnkqEREJkbJwnEKh4zGytjYzZDqFkFlC2x9qPCBiYhISTiYLHT0QSTmaxd2PFrkqEREJqbgyYKZnWxmj+U8us3sb83sk2a2PWf95YWObapaauJjd3A2g/nPg20PFj4oEZEyNF4ZMWqfC83sQM4+Hy9SuBNSWxGlviLKps5gRKS2s2HHY5DNFjUuEZGJiBT6gs65Z4GVAGYWBrYDtwF/Cfy7c+7zhY7peDVXxcfuswCw4Hnw7M+grwMqmwobmIhImTlKGTHa751zryxgaMdlcVOlr1kAmHc2rL4RujZC04nFDUxE5BiK3QzpYmCDc25zkeM4Ls3V8bH7LAAsPM8/b/pD4QISEZkZZkQZAbC0qYr1e3v9QtvZ/ln92USkDBS8ZmGUK4Hv5Sy/18zeBqwGPuic2zf6ADO7DrgOoK2tjfb2qY001NHRMeVjR4um+tjbM8STT68hGrbDN2YrWRapoPvRH7M7vPy4rjOdMRdSOcatmAtDMcsxjC4jcr3QzB4HdgAfcs4dMalNKZUXdaF+9vYM8dDjT1MdhWWxGnoe+wm74mce13nHU45/p4q5cMoxbsVcPEVLFswsBvw58NFg1VeBzwAueP4CcO3o45xzNwA3AKxatcotXz61L+Dt7e1M9djRzuzZynce30dd6yIWNFQcucOjL6K+6wnqj/N60xlzIZVj3Iq5MBSzjGeMMiLXI8Ai51xv0L/tdmDZ6J1Kqbx4EXv45uouqG1l+eIGePJC6rY/Qt3JJ/v+bdOsHP9OFXPhlGPcirl4itkM6TLgEefcbgDn3G7nXMY5lwW+AZxbxNgmZV5dEoBt+8aZfG3JS/wEPAe2FzAqEZGydlgZkcs51+2c6w1e3wlEzaykO4Utm1MFwNrdPX7F0guhext0biheUCIiE1DMZOEqcqqXzaw1Z9trgLKZsaat3icLO/aPkywsfYl/fu53BYpIRKTsHVZG5DKzuWb+53gzOxdflnUWMLZJa6tLUhkLs2530G/hhIv888bfFC8oEZEJKEqyYGYVwEuBH+Ws/pyZPWlmTwAXAf+nGLFNRWttAoDt4yULLSugohE2KlkQETmWscoIM3unmb0zWHw98FTQZ+F64ErnnCt8pBNnZpzYUnWoZqF+CdQthI2/LWpcIiLHUpQ+C865fqBx1Lq3FiOW6ZCIhmmqio9fsxAKwZILfM2Cc3lpnyoiMlOMU0Z8Lef1l4EvFzqu47VsTjW/fXavXzDzTZGe/jFk0hAu9ngjIiJjK/bQqTNGW11i/JoFgJMuhZ6dsG114YISEZGScdKcKjp6h9jXN+xXLL0Qhg5o4k4RKWlKFqZJW33y6MnCyZdBOAZPjzW3kIiIzHTL5lQDOZ2cl70MIkl44gdFjEpE5OiULEyTebVJduwfYNxms4laOPESePpHvspZRERmlZNGkoU9QSfneDWc8kr/I1J6nIk9RUSKTMnCNGmrTzKYytI1Ur08lpV/4Zsirb+rcIGJiEhJmFeboCoeYe2unkMrz7gSBvfDul8VLS4RkaNRsjBNRuZaOGa/hcoWePhbBYpKRERKhZlx6rwantx+4NDKpRf6cuHxW4oWl4jI0ShZmCZtdceYawEgHIWz3gzrfgndOwoUmYiIlIoz59fyzM5uhtNZvyIcgdNfD2t/CX0dxQ1ORGQMShamSduxZnEecfbbwGXh0e8UICoRESklZy6oYzid5dncpkhnvw2yKXj0f4oXmIjIOJQsTJO6iigVsTA79g8efceGpX7OhUdvhmy2MMGJiEhJOHN+HQCPb9t/aGXLKbDoRbD6mxoAQ0RKjpKFaWJmzKtLsn1//7F3Pvtq2L8FNv4m/4GJiEjJmF+fpL4iyhO5yQLAC97lywUNry0iJUbJwjRa2FDB5s4JJAunvAoqmuCPX/IzOouIyKxgZpy5oI7Htx44fMPJl0PzKXDv/6faBREpKUoWptGSpko2dfaRzR4jAYjE4YIPwXO/gw33FCY4EREpCWfMr2Pdnh76h3OSglAI/uwfoONZePTbxQtORGQUJQvTaElTJYOpLLu6j9FvAWDV26F+Mdz1Cchm8h6biIiUhpULask6eGp79+Eblr8SFr4QfvP/YKhn7INFRApMycI0WtpcCcBzHX3H3jkSg4s/Drufgie+n+fIRESkVJwRdHJ+bOu+wzeYwcv+Cfr2wG//pfCBiYiMQcnCNFraVAXAxokkCwCnvgbmnQ2//idIHWPIVRERmRGaquIsaqzg4c37jtw4fxWsuhbu+zI8+cPCByciMoqShWk0pyZOMhpm497eiR0QCsFLPw3d2+GBr+c3OBERKRnnLKrn4c37cGMNcnHpv8LC8+DH74HtDxc+OBGRHEoWppGZcWJLFet2TzBZAFjyYjjpUvj9v0Hv3vwFJyIiJeOcRfV09A6zpWuMEfQiMXjTt6GqBb73F9C9o/ABiogElCxMs+Vzq2nf1X3sHXO99NOQHoCf/q2GUhURmQWet7gBgAc2do29Q2UTXHULDPfCd98IgwfG3k9EJM+ULEyzU1pr6OgdZk/PBEZEGtF8MvzZP0L7T9XZWURkFljWUkVzdZw/rO8Yf6c5K+ANN8GeNfDfr4Ce3QWLT0RkRFGSBTPbZGZPmtljZrY6WNdgZneZ2brgub4YsR2vU1prAFizc5LD3r3wPb6N6p0fhs4NeYhMRKQ8jFVGjNpuZna9ma03syfM7OxixHk8zIwXndjEH9d3HH1unmUvhb/4PnRthBtfpvJBRAqumDULFznnVjrnVgXLHwHucc4tA+4JlsvOKa3VAKzZOcmmSKEwvPbrYCH436s1OpKIzHajy4hclwHLgsd1wFcLGtk0Of/EJjr7hnl6xzHKixMvgat/AoPdcOPLYcdjBYlPRARKqxnSFcC3gtffAl5dvFCmrq4iRltdkie3T6F9ad1CeO0NsOtJ+PnfTX9wIiIzwxXAzc67H6gzs9ZiBzVZFy9vIRIyfvrkBDowzz8Hrv0lRBLw35fDU7eqj5uIFESkSNd1wK/MzAFfd87dAMxxzu0EcM7tNLOWsQ40s+vwvyTR1tZGe3v7lALo6OiY8rHHsrQuxEMb9k7x/ItoOvUamh65iZ2RRRxY+sqDW/IZcz6VY9yKuTAUs4xjrDIiVxuwNWd5W7BuZ+5O5VBenD0vyY9Wb+GKxRAyO+b+4Qu/xvw//D3JH15Lz303sfucD5GumHPEfuX4d6qYC6cc41bMxVOsZOF859yOICG4y8wmfCeDQuMGgFWrVrnly5dPKYD29nameuyxXLAnxu/vXENj2xKaq+OTP8GyL0D/Blof+TytZ18Kc08D8htzPpVj3Iq5MBSzjOOIMsI5d2/O9rG+VR/xM3s5lBdvHqzm/bc8RmekiRcva57YQWf8Fu7/T6p/889U/+LNcPHH4Xlv981ZCxBzvijmwinHuBVz8RSlGZJzbkfwvAe4DTgX2D1SjRw87ylGbNPhzAV1ADy+df/UThCOwOtvhEQtfOcNsG/TdIUmIlLyxikjcm0DFuQszwfKcjKCS0+bS0NljG/ft3niB4WjcP774d33+Rmff/5h35dh99P5C1REZq2CJwtmVmlm1SOvgZcBTwF3AFcHu10N/LjQsU2X09tqiYaN1Zv3Tf0kVS3w1h9Bqh9uvgK6dx77GBGRMneUMiLXHcDbglGRXgAcGGnGWm7ikTBXPm8Bd6/ZzaaOvskd3LAE3nobvPYbfrSkr70YbnkzbPpjfoIVkVmpGDULc4A/mNnjwIPAz5xzvwD+BXipma0DXhosl6VkLMyZ8+u4b2Pn8Z1ozgp4y63Q1wHffjWhIU3KIyIz3phlhJm908zeGexzJ7ARWA98A3h3cUKdHtecv5hoOMR//nb95A82gzPeCO95CM77G9j8J7jpcuY+8GnY+Ft1ghaR41bwPgvOuY3AmWOs7wQuLnQ8+XLeCY18+Tfr6R5MUZOITv1E81f5WTz/53Us+N3/gZN+BsmynIJCROSYjlJGfC3ntQPeU8i48qmlOsFfPH8hN9+3mWtftITlc2smf5LKRnjpp+DCj8Bdn6Dm4Zvg5p/B/OfBBX/n52uYQAdqEZHRSmno1BnlBSc0knXwwMau4z/ZkhfD628ksa8dvnExdJdl01wRERnH+y9eRk0iwsdvfxp3PLUB0SRc/jnWvfYueOW/Q88u+O4b4HNL4af/B3Y+Pn1Bi8isoGQhT1YtaqAmEeHnT01TM9pTXsmWi74CvXt8H4YD26fnvCIiUnR1FTH+7tLlPLipi9sfO/7PdxeOw6pr4W8e8X0aTrwEHvsufP0CuOFCePhbMDzJPhIiMispWciTWCTEpafN5VdP72YwlZmWcw60nAV/8X3f2fmbL4M95T92r4iIeG9atYCVC+r4+O1Ps35P7/ScNBLzfRpe9w34YDtc9jlIDcJP3gf/usR3iN715PRcS0RmJCULefTKM+bRO5Tmd2v3Tt9JF58Pf3knZFNww0vgj9erA5uIyAwQChlfefPZxKMh3vGth9jfPzy9F0jWw/P/2g+5eu0v/dwMm34PX3sR/PBaeORmX3stIpJDyUIenXdCIw2VMX7y+DT3MWg9A677ra9Wvusf4c4PQ3Z6ai9ERKR42uqSfP2t57Bj/yDv/s4jpDLZ6b+IGSx8AVz6z/D+x+HFH4RnfwF3/A1cfxbc9i545g4N2S0igJKFvIqEQ1x22lzuWbOHvqH09J68Zh688dt+qLyHvuEn5Nl83/ReQ0RECu6cRQ38v9eezp82dPL3tz5BNpvH2uNkvZ8B+sPr4Z1/gFNeBc/+DH7wVvj3U+HmV8NdH4enb4eOdarJFpmFCj506mzzunPm850HtvC/q7dyzflLpvfkoRC87J+gZQXc8yn470v9L0R/9o8aIk9EpIy9/pz5bN83wL/fvZYD/Sm+eOVKqo9nGO5jiVXA3NPhNV+DTAp2PArtP4P198B9/+mbvgLUL4YFL4DFL/LNYqvnQTSRv7hEpOiULOTZ2Qvred7ier7x++d48wsWEQ3noTJn5VVw6hXwi4/A77/gh1Z91fW+Y5uIiJSl9118InUVUT7z02d48389wFf+4mwWNFTk/8LhKCw41z9e+ikY6oGu52Dbgz552PBreOIWv28k4cufxmW+aVPTMojX+ORDRGYEJQsF8NcXnMA7bl7NnU/u5IqVbfm5SKwCXvUlqF0Av/kn6NroO7KteK1qGUREypCZcfV5i5lfn+T9tzzGy794Lx+9/BTefO5CQqECfq7Hq31fudYz4Hnv8E2R9qyB7athy/2w7i544vuHH7P4xbDg+X5SuEUvhERt4eIVkWmlZKEA/mx5C8taqvjqbzfwqjPm5e9D3gxe8mGoWwB3f8qPbvHkrfDn/+Fn9xQRkbJz8Slz+OX/uYCP3PoE/3j7U/z08R18/FWnsmJekb6Am8GcU/3j7Lf5dQP7YOtDcGCrnwjuyf+FLfdBNuivV9EE81f55GHB86HtbIhVFid+EZkUJQsFEAoZ7/2zE3n/LY9x+2Pbee3Z8/N7wTOvhNPfCA98Fe7+JHz1PLj0/8Epf+6rl0VEpKy01SW5+dpzueWhrXzuF+288j/+wKtXtvHOl5zAyXOrix2e7yh90ssOLf/ZP/j5HLY95GsgOtb7Zkxrf+G3WxgaT4Cmk6D5ZGg6mdhgNbiTVRsuUmKULBTIq86Yxzf/8Byf+8WzXHZaK8lYOL8XDIXghe+BJRfArX/laxmq58HzroWzr4Gq5vxeX0REppWZcdW5C7n8tFa+8tv1fPu+zdz26HYuOaWFq89bzPknNBW2edKxRBOw5MX+MaK/C7at9onDnjWw91l49ufgMiwF+GMbNC+HU//c10I0LlP/O5EiU7JQIKGQ8X9fcSpv/Pp9fOP3G3nfxcsKc+G5p8O7/gjrfgUPfB1+/U/wu8/Baa+Dc6/zVcEiIlI2aiuifOzyU3jXS07gW/dt4lt/2sTda/bQVpfkjasW8IZVea69Ph4VDb4GIrcWIj0MXRvZ9cAPmZva4msjfvJ+vy1eA/WLfALReia0rvR9J9QHQqRglCwU0LlLGnjF6a18+dfrufS0uZw0p0BVx6EwnHyZf+xdCw/eAI9/zz/aVvmO0Ke+Wr/eiIiUkfrKGH97yUm868IT+NXTu/n+Q1v597vX8sV71nJWa5I391VxyalzqE2WePPTSAxalrN/2euZu3y570C9+2nY2w6b/wj7t8LmP/l+ECNq5kNNK1TPhYalPpmoaPLJyNzTIRIv3vsRmWGULBTYp65Ywf0bO3n3dx7htnefl99xs8fSfBK84vN+Ep7HvusThx/9Ffzsg3DCRXDmVbDs5b4Zk4iIlLx4JMyrzpzHq86cx5bOfn74yDa+/8BzfPB/HycaNs5eWM/zlzby/CUNnL2wPv/NYI+XGcw9zT9Of/2h9b17YOcTsPNR3weiZ2fQjOkXh+aBAF8b0XomDHX7ztTJBqhohKoWf+6WU6HhBBjc7ztgRysgHNMPZiLjULJQYE1Vcf7jqrN4640P8rHbnuI/rjqrOIEkauAF7/RNkTb+2s/Oue5X8MyP/a8zC871TZTOvtp/wIqISMlb2FjBB156EpfOzzBUOZdfPLWLP23o5Mu/Xsf1DqJh44z5dbxgaQMvOamFsxfWEcnH/D/5UNUCyy7xj1yZNOzf7Edk6t7uJ5Pbs8YnCI/cDOkhYNTM0xYGlzm0HK+Bqjk+wUgPwSmv9AnEzsdh3lm+5qJ7Owz3QTTpO3S3nUPlzg3g1vtaDgxCEZ/E1C/x+1c2B3NOWNBx26CySSNBSVlRslAE553YxN9evIwv3LWWi5e38Oqz8jT3wkSEQnDiJf6RSUH7T/2vNDse9Z3Ofvc5aDvHf1CueDUsvbB4sYqIyISEzDhrYT1nLawHoHswxcOb9nH/c508sLGLr/1uI1/5zQaq4hHOWVTP85c28PwljayYV0MiWuI1D6OFI35kJQBW+UniRgz1+uf+Tv9FP5vytROd631yYAapfti/BQa7/ZwSqQF44n8B5xOFx2+B4V4IRSFe5c+TGQZgwVTijSR886lwDDBID/jyN1EXxNwD81ZCsg56dvvaj+q5MLDf99fIpKC2zZ9nqNfXiITjPompnQ+bfu9rU6pb/VC2c0/38Vrw79q8nPDgPt/ca2TkqZG5MyoaoXrOVN6VzGBKForknReewO/Xd/Ch/32ciliYl62YW+yQ/LCqK17jHwCdG2D1jbD9YXjqVnj4v33V7uIXw+IX+ZGW9OuIiEjJq0lEuWh5Cxct9zXFBwZS/HF9B3/a0MEDG7v43C+eBSAcMpY2VXJKa03wqObU1hqaq+NYOQ5pGq86/Bl8OXYsr/k6uKxPRDJpGOjynapH+kL0d8H2h9m8ex+LFi2B3l3+S302A3ULfY1E7QKfZGRSgPNfyHGw6yno2xt8gTffDCoU8TUj2bQvV7c/4pOW6rm+3+Hedp8crP35kbUiU7AM4GeV/vouCxik+nzSEauAWPWhJGb+Kp9YhcI+ru4dPhGJVfjYe/f4mpaaVp+8dDzrk455Z8GBbb4mZbgf0oP+uJG/o55dvuZl20O+38nc0/364T7o2uCTp4oG/5zqp3FfD3Sf6M8z1Ovv7VCPT6rql/hED3yMjUuhYx3UL/bfbSIJ3/Ssb4+PqeUUqGzx/w49u/xxjSf6f5P+Dr9t91O+lUW0wl8HDt0vR3DfXLDsjlyuW0iiaw1018KeZ/x7r1/srznc6/+dLeRrtTrX++Sxps2/ZzM/9DDOJ4G5Mqkjh8HPTfzyoODJgpktAG4G5gJZ4Abn3JfM7JPAXwF7g10/5py7s9DxFUo0HOKbV6/iLd98kPd+91G+cfUqXnJSiQ1n2ngCvPyz/nV6GFZ/E9b8BB78Btz3Zf+fb85pcMqrfIJRt1DjY4vIcRmvjBi1z4XAj4HnglU/cs59uoBhlr3aZJTLT2/l8tNbAejsHeKhTft4ZscBntnZw8Ob93HH4zsO7t9YGTuYPIwkEkubKwmblU8zpskIhYDgfYUjRzbHrWiAZS9lINMOC5YfefyCc/MT13AfRJI+OUkP+S+amWHIDMHgAV87sPQi/6V4/1Y/h0XXc8GXS+cTn91PsbtzH3Piw/7LKvgvuA3BF+xsKmhuVeFf73gc9rT7RKZpmX9vB7YH1+z2xw0e8D8sWti3RujZ5ftFVrf69fEaH8PmP0I2669Xvwg2/Nq3XNi2Gtbc4WMZmYNjqMcnZZkhwGjGwZM59yJa6ZPAgX0Ha3qCE/j3Go4HxxbPYoBfTfKgcNy/r/5On0TGqnwiseTFPlHbcr9PkJL1PnGoavH/PlXNPrFa9ZdwzjXT+j6KUbOQBj7onHvEzKqBh83srmDbvzvnPl+EmIqiOhHl5r88l6u+cT/X3byaf3/TyoMf3CUnEoMXvMs/UoOw9QHfx2HLfXD3J/wjVg3zz4Hlr/QfVk0nFjtqESk/Y5YRzrlnRu33e+fcK4sQ34zUWBXn0tPmculph2q5D/SnWLOrmzU7Rx49fOu+zQynswf3SURDXLpiLqe01jC3NsEprTUsaqwgHimzpkzlYqQ2v2be2NtHak2q5xz6pf5gE63AskvY197OnOVjJDmFkM34X8LDkcN/EU8HNS2ZVNDPA789NQCROM+ueYqTF871v7THKn1NB/ikqa/D/1qfGvA1BF0bYc4Kn2ykB/zzSI1B1RzY/aSvnahs9rU3mZRvihaO+CZc+57zSdDOJ/wPoyM1U875BMtCPtaRfigHl0N+GWDPM+zo6mVeMu3/LYZ7/TWqWnwtyHCfP19/B9Qt8olB93b/GOrxI36l+vzr4T5Y81OfYJ3/ft+3pr8LcP75rDfDvk0+CcvDsMIFTxacczuBncHrHjNbAxSx0X5x1VZE+fbbz+UdN6/m3d95hPdfvIz3X7ystCbWGS2agKUv8Q/wzZU2/Bo61sLaX8KdHwr2q4BlL6WmeiU0R/1/PNU8iMhRHKWMGJ0sSJ7VVkR5wdJGXrC08eC6dCbLcx19PLOzm82d/Wzb1889a/Zw+2OHaiHMoLkqzrI5VSyfW8MZ82tZ2lTFkuZKquJq/TzrhXISydzvBCOjUeU2sTE7mDi4cHzs/hSRuO/Dkav1DP88MgFt3cLDt491npac5GneyuA8E2iyNp4lL6a7vZ1505WUvWZ6TjMVRf1fa2aLgbOAB4Dzgfea2duA1fhflvYVMbyCaayK872/egH/cNtTfOmedTyzs5t/fd0ZNFSWyTBujScc+uXiss/5zLn9p74N3lO3Mm/wx/DAp/x/1hMvgRNf6qsxKxqVPIjIuEaVEaO90MweB3YAH3LOPV3I2GarSDjEsjnVLMuZJ8g5R/9whq37+llzMIkYYN3uHr7zwGa++YdDNRH1FVHm11fwwhMaOaW1mkWNlSxurKS+osTnghCZxcw5d+y98nFhsyrgd8BnnXM/MrM5QAe+28hngFbn3LVjHHcdcB1AW1vbOXffffeUrt/R0UFTU9NUw88L5xy3PXOAGx/upCoW5r0vaOL8RZUHO5WVYszHlM3Qv+Ux5qU3U7nzPip2ryac7gdguLKVofqT6W9eyWD9yQw2noYLl0aCVI73WjEXRrnFfMoppzzsnFtV7Dgma3QZMWpbDZB1zvWa2eXAl5xzy8Y4x4wtL46lVGJOZx1bDwyzvTvFlv0puvrTbN4/TPveQVKHcggqoiFaKmBBfZLW6ijzqqP+uSZKU4X/JboUO1iXyn2erHKMWzHn33jlRVGSBTOLAj8Ffumc+7cxti8GfuqcO+1o51m1apVbvXr1lGJob29nebHa6x3Dmp3dfPAHj/PMzm7OO6GRT7xqBSfPrS7pmI/msLjTw7D1fj8axHO/802Xujb6bbFq3xmr8QQ/ikTdAl8F2HRSwUddKsd7rZgLo9xiNrOySxaOVUaMsf8mYJVzrmO8fWZqeTGeUo95KJ1ha1c/mzv72dTZz5bOPp7esofOoRBbu/pJZw99N6mKRxhKZzitrZbnLW5gTk2C5uo4TVUxWqrjNFXFqU1Gi5JMlPp9Hk85xq2Y82+88qIYoyEZ8E1gTW4hYGatQVtV8C2znip0bKXilNYa7njv+Xz3wS184VdruexL9/K6s+dz2aIQ5fMnN45IzA+5uuQCeOG7/bp9m2H307DhHp88bP6T7+Djcn52ql0Ii17oR1aYezrMf55GXxKZgcYrI0btMxfY7ZxzZnYuftiazgKGKccpHglzYks1J7Ycas408sUqncmy88Agmzv7ea6jl3V7eomGQzy6ZR///cfnSGWO/JEzFg7RWBULkog4zVVxkrEwNckoDRVRYpEwDZVRmqvjNFclaKmJl998EiJFUow+C+cDbwWeNLPHgnUfA64ys5X4ZkibgL8uQmwlIxIO8bYXLuZVZ8zj+l+v47sPbOHWR7K8elOGa89fwmlt09/bvWjqF/nH8ssPrcukfcKw5X7o3gY7HoPn7vVDiY0MkZao87UPtSOP+b6TU7TSd3iqbPLjKo+MqiAi5WC8MmIhgHPua8DrgXeZWRoYAK50xWpTK9MuEg6xoKGCBQ0VvGjZ4U04slnHgYEUHb1D7O0ZYm/w3NE7HDwPsevAIE9tP8DAcIa+4TTZcf4yqhMRmqvjREN++ND6yigNlTFqkzEiIaO1LsHgcIb9AylOm1dLZTxC/3CahQ0VVCeipLNZtnYOcUImS9Y5MllHRUwduGXmKcZoSH/g4LhSh5mxcyocj/rKGJ941Qre9ZIT+OfbV/PzJ3fxo0e2s3JBHX/x/IVcfnrrzBxdIhw5lETkyqR8LcT21bD7GT+5yr5N8NzvYbhn7HNFkj55mHeWn8gm2QAtp0LDEj9sWazCD72WrPPjQyfr8/3uRGQcRykjcvf5MvDlwkQkpSQUMuorY9RXxg7rZD2e/uE0A8MZhjNZOnuH6egdYk9PkGj0DLGnZ5B0xmEGXX3DPLurhwMDKYbTWboH04QMktEwN9+3edxrRH62HYCsczRUxomFjdqKGN0DKQBOmlNFxvl+ifNqk4RCkMo46pJR0lnHwHCGMxfUkYyFSGUckZDRUBkj6xzOQU3Sd/5OpbOYGUuaKsk6RywSIhEN0zOYojYZpSIWYTCVIREN45wjlXFEw1aSfT2kvMzAb5kzU0tNgr8+t4lPvuH5/OiRbXz7/s383Q+f4OM/foqXnTqX15zVxouWNRGdiZPj5ApH/ZBmI8OajXDOTwrTvcOPqZwahN7dfljXoQN+jOLN9/lhX/dvhWd+jK/EGkO0gmUO32RqZOxk5/wkPM3Lg1k4OTTGcigCFU1+7Oaq5mCMYzt8zOVQ2K9PNvjzhCL+EY4Fz/qvKCIy3SpikYO/9rfWJo+x9+H6h9OEzIiFQzzX2cdQKktlPMyGvb0MpbJEwyHWb9pCd6iakBlm0NE7TCqTZX//MKfMrSaddazd3UM8GgbnWLOz2/9GFQmxvz9FNGyEQsb3V2897vfaUBmjq2+Ypqo4qUyWAwMpahIRljRVHmyyGzboHkwTI82i1X1UxiMcGEiRyTqqExGS0TAt1XF2dQ+yqLGS5mp/rmQ07B+xMPFImFjEiIZDxCIh/xwOEY+GiEfCxCN+uaSHgJdJ0TeUMlObjPKX5y/hmvMW88iW/dz26DZ++sRO7nh8Bw2VMS4LJtV5wdLGmZ845DILZjSsm9j+w32+VsJCByd8oWeXr7EY2MeBjr001NUcmsLdzCcZXRtzZrx0+Bkxh/2EMIP7px5/rMrHEI75hCgcg1D00OtwLKhtWeyTosoWP+V9JOFjq26lbn8/9LT6+EaSkGzKP1e1+Gv07vbbauf7/h8u699/1v8CRijqJ7xJD0Gixk8cA75ZGO7IKeZFRGao3CZFJzRXHXy9qPHQgBvzQ/uOuwOrc45Nnf0454iGQ6Szjs7eIUIhI2RG90CKkBnRsDGUzrKlq59o2OgfzjCczlKdiNLVN8S2fQO0VMfZ3T1ENGLMqU6wff8AOw8M+usAmWyWluoEu7oOsHZ3D31DGeoqooRDxvo9afqHM3T0DlGbjHIgqBmZqmjYDiYPfcNpapNREtEwqXSW+soYlbEIGzt6aaqK01gVI2RGVTwSvKcINckonb2+2XFrbYL+nv20bFtLJOQTrMHhDOCHn+/qG6YiFqalJs6+vhTJWDi4Xgjn/Henjt5hzKCtLklztT9mMJVhX/8w5y5pJBIkN7FwiN09gzRVxWfX96ijULJQpsyMcxbVc86iej7+yhX89tk9/OSJnfzoke1854Et1CQiXHzKHF6+Yg4XnNSsdpSjxSr9yEu5mk8+ONHcnvZ2GiZbAKSHoG+vn21xJJFwWf86m/Jf8vu7gqnpU35dNu1fDx7wx2eGg3XDwSMVPIZhuB+eucM3k+rvPPSl3jkYOsDcYwY4BbFqH2d60Cch9Yt9AuGCxMFlfTOveJDspAZ8/OlhX5tSvxjiNUeeNxwFHC0de6DjZF8TlOr358umgwRnjr/2SDOySNzHkag7VKNTt8jfn1S/nwQwmvDXTg/6c9UtCBKqsI9nqMefp2qOOseLSEkYaVqUa/TydDvaKD1D6QyxcIjeoXRQ+xFiKJ1hIJWhfzjDUCpLKpNlOB08Z7IMpf1jOJ1lKO33Gc5kGUxlGEpnqYiG6RlMM5DKEA2H2Nc/TO9Qmhcva6azb5j+oTTprGPH/gHikTBr96ToGUxTnYgQDYX4dfseBlMZHIem3zLz7RXH65MyVSHz56yOR8g6RzQSIhn1SU8kHDrYvMwnE0bW+WMqg/1TGce+/mGWNFbSdaCHOQ/1MZTOHExgapJRktEw+/qGGUhlqE745UM1NUY84pNG56AiFiYcsoPJlt/P1+yMPGLhENFgXVU8Mu2d9/UNcgaIRUK8bMVcXrZiLgPDGX6/bi+/fHo397Tv5rZHt5OIhnjxsmZevKyJsxfWs3xuNRFly9MvEve/2BfDUC/r1jzBshNO8P0yMqlDX7ozKV+jkOr3E+Fl075WpWen3x5J+NoGs0NJSSTmE5ieXX5brMp/ce9Y589jIX8e8AlL7x7/BT1acaiJVWbYz+idGjgy3rT/pavOwvDsIGA+jlDYf7HPDB3cJy8iCX/N9GBQCxMOalNqfZIRjvqEKBQO3nvabw9FWBSugj/W+qQvEvcJS6ofBrt9Ipce9AlVetAnQRUNfn282o/8lRmGuacdar6WqPEd+HFQ2ezvV+18/28WivrjI3E46y35ux8iIoF4xH/RrE5EqU6UTm1ye3s7J510MpmgM3nIjHQ2S/9whvqKGH3DafZ0D1FfEWU4aIY1GEzmcWDA9+sIm7F9fz97e4aor4wRNiMRC7NmZ/fB6/QMpmmqirN+Ty+JaIhs1jGQyjAYJEmV8QhhMzp6h0hnHeGQkc46egZT/rdB5zippZpNnX1k01nW7ekhEQ0zlPYxjfSJqY5HqIj7JGowlZm2pOcDLz2J9118xLQzx0XJwgyTjIUPJg7pTJYHn+viV8/s5ldP7+KuZ3YDPktduaCOc5c0cO6SBs6YXzczO0nPJvEqMomGsaewB2g68fDlBefmP6ajcQ6cY+3atSxfuuBQ86sR2azvsB5J+CZj6SGfQITjQXOv4Iv+ga2+5iGas18k4ZMd5/z2TCqoscgc6sy+b5O/TjQZ1OZkfAyDB3wilB70X+ZdBoZ6/bZIHNJDZPZuPlRrks34fjGxSl+70rvbJ0z7t/jjY1W+6Vp/Fwx1+1G7QmFYf5d/DyP9ZmJVfv3I9XOHDQbfV0bJgojMcqGQEcIY+eE8Ruhgy4maRJSanORmvD4qp88/cjTJi05umf5gGb8GJ5t1R/TpyGQdw0HtzHAmy8jmoXSWdMaPAjac8TU1qYwjlVOzk8o4Uhm/fOb8uml/H/qGOINFwiHOO7GJ805s4hOvOpXt+wd4ePM+Htm8j4c27eNL96zzP54aLGup5rS2Wk6aU8VJc6tZtai+pH5RkBlmpHM4jD3hXigUdBTHf0nPVdN66PXoju4FsG06JtlxLugHs8XX5DQtO1S7YWE/bHBNm09aBg9AdV4amYmISBGM1fk7HDKSMd+JvNQoWZglzIz59RXMr6/gipVtABzoT/HIln08unU/j23dz73r9nLrI9sA/0e7uLGCZS3VLJtTxYktVSxrqWZpc6UmshE5XiOJUt3Cw9ePJEYjQwaHq3yNhYiISJEoWZjFaiuiXLS8hYuWH6p+298/zDM7url/YyfP7u5h7Z4e7lqzm0zQmC5ksLChghNbqjlpThXL5lSxpKmK1toELdVxjecsIiIiMoMoWZDD1FXEDjZdGjGUzvBcRx/r9/Sybnevf97Tw+/W7iGVOdQjp6kqxslzq1ncWOkfTZUsbqxgOJ0d61IiIiIiUuKULMgxxSNhls+tYfncw4fATGWybO7sY1NHP9v3D/Dk9gOs29PLz57cyf7+Q+MzG9Bau5MFDRUsaqxgYUMFCxoqaKiMsbixkra6pCZvERERESlBShZkyqLhECe2VHNiS/UR2/b3D7O5s59NnX2sbt9Mn1Wwpauf3z67lz09Q4ftGwkZTVVxmqvjtFT758NfJw6+Vn8JERERkcJRsiB5UVcRo64ixpkL6jg50XPY6DEDwxm27utnX98w6/b0svPAAHu6h9jbO8TOA4M8sf0Anb1DY445XJ2I+ASiKk5LTSJ49svN1f51U1WcqniEeCSkPhQiIiIix0HJghRcMhbmpDm+NuL5SxvH3CeTdXT2DbG3Z4g9Pf4597GnZ5Ant+1nb88QfcGU76OFDCpjftKTyliEZCw8xnKYiniEyliYZCxycHnfnj66oh1UxCIko2GS0TCJWIhE8FpTwIuIiMhsoGRBSlI4ZLRUJ2ipTrDiGPv2DaV9EtE7xJ7uITp6h+gbTtM/lKFvOM3AcIa+4Qz9Q2n6htN09Q2zbd9AsJyhL5hm/gj37DpqfMlomEQ0RCQUYjiTpToRIZN1VMYiJKIhwiEjEvZTtx82JfvIciRENOSniq9J+hkhs84nUyO1ImGDaMQfF4sE5wwZITMiYf88sm1nxxC2q4dI2AibEQ4ZoZB/HQ0b8ZHp6kOmGhcRERGZECULUvYq4xEq4xEWN40xudcEDaezQVKRpn84zdNrN9DcOp+hVJaBVIaB4czB58FUhsG0n/p9IJUhlc4SjYToHUwTDhl9Q2mG0lkyWT+j4lAqS+9gmuGcGRZT6SzDGcdwOkMoZHQPpMgG83S545ryfdsx9zCDaChEJOwTj1jEJzyRsE8wnINo2E8Ok4iEDyYckbCRiIapiB2qWTE4eFwsHCYeDZF1jnTGURmPUB2PHExyomEjHPLJykjSs2tnHzvZc3B5JME69Nqvj4RCB5fDISMaChEO5xwTbBcREZHppWRBBIhF/K/ztRV+1up0V4LlJzQd46jp45zDzHDOMZzJMpjyw82OJBwj079nsu6wRzpn+4ZNW2ie23rY9qxzZLJ+5KqhIMFJBVPDpzNZ0ll/vXTGTyefcQ4DUlnHYJAgjZxrMJ1hb88QA6kM6YzDOYcLYkznTFPvcETDIfrHaR52pPFrcCbDjCOSh2j48OW64N8361xQMxT2NTDBvQcIWW6i4hMXF/wbhUMhensO0LgmdVjSkwjOFTLDzDeBM3zNzqEYfDIzMJyhuTpxMA7wcUfCIUZat5kZNYkoIfO1WBWxCCe2aHI2EREpPCULIiVgpFmQmRGPhIlHJj/qU0u2k+XL5013aFOWzTr6htMMB7Us6ZwEZyRRWb/xORYsXEQm6xOYQ/sduZzOBMdmHZng+EPnC/YJ1qVGLY9cs6s/5WtDQsZAKkPvUPpgUmX4fwOfYB0epwVf/jNZx9DwMOwcPnj+dMYnUsdXI3R0y+dW84u/vSB/FxARERmHkgURyYtQyKhORI+6jx2Is3xBXWECmibt7e2Hje4FHKwRcs4nGyPP2SykR5KKrCObdSSiYTr7hjCMcMg3O8tNasDX1vQMpnDOv66Ia8hgEREpDiULIiLHaaRGaKKaq+N5jEZERGT6lNz4j2Z2qZk9a2brzewjxY5HREQK61jlgHnXB9ufMLOzixGniMhsUFLJgpmFga8AlwGnAleZ2anFjUpERAplguXAZcCy4HEd8NWCBikiMouUVLIAnAusd85tdM4NA7cAVxQ5JhERKZyJlANXADc7736gzsxaCx2oiMhsUGp9FtqArTnL24Dn5+5gZtfhf0kC6DWzZ6d4rSagY4rHFks5xgzlGbdiLgzFnH+Lih3AJB2zHBhnnzZgZ+5OKi8UcwGUY8xQnnEr5vwbs7wotWRhrFmVDhuQ0Dl3A3DDcV/IbLVzbtXxnqeQyjFmKM+4FXNhKGYZwzHLgQnuo/JCMeddOcYM5Rm3Yi6eUmuGtA1YkLM8H9hRpFhERKTwJlIOqKwQESmQUksWHgKWmdkSM4sBVwJ3FDkmEREpnImUA3cAbwtGRXoBcMA5t3P0iURE5PiVVDMk51zazN4L/BIIAzc6557O0+WOu2q6CMoxZijPuBVzYShmOcx45YCZvTPY/jXgTuByYD3QD/xlnsMqx39zxVwY5RgzlGfcirlIzLkjmnmKiIiIiIiUXDMkEREREREpEUoWRERERERkTLMyWTCzS83sWTNbb2YfKXY84zGzTWb2pJk9Zmarg3UNZnaXma0LnuuLHOONZrbHzJ7KWTdujGb20eC+P2tmLy+hmD9pZtuDe/2YmV1eYjEvMLPfmNkaM3vazN4frC/Ze32UmEv2XptZwsweNLPHg5g/Fawv2fss+VMuZQWovChwzCX7GRbEoPKiMDHPnvLCOTerHvgOcxuApUAMeBw4tdhxjRPrJqBp1LrPAR8JXn8E+Ncix3gBcDbw1LFiBE4N7nccWBL8O4RLJOZPAh8aY99SibkVODt4XQ2sDWIr2Xt9lJhL9l7jx++vCl5HgQeAF5TyfdYjb38LZVNWBPGqvChczCX7GRbEofKiMDHPmvJiNtYsnAusd85tdM4NA7cAVxQ5psm4AvhW8PpbwKuLFwo45+4FukatHi/GK4BbnHNDzrnn8COZnFuIOHONE/N4SiXmnc65R4LXPcAa/Iy1JXuvjxLzeEohZuec6w0Wo8HDUcL3WfKm3MsKUHlx3FReFD3m8ZRCzLOmvJiNyUIbsDVneRtH/4MsJgf8ysweNrPrgnVzXDCeePDcUrToxjdejKV+799rZk8E1c4j1YYlF7OZLQbOwv+KURb3elTMUML32szCZvYYsAe4yzlXNvdZplW5/duqvCiskv0My6XyIr9mS3kxG5MFG2NdqY4fe75z7mzgMuA9ZnZBsQM6TqV8778KnACsBHYCXwjWl1TMZlYF3Ar8rXOu+2i7jrGuKHGPEXNJ32vnXMY5txI/K/C5ZnbaUXYviZglL8rt31blReGU9GfYCJUX+TdbyovZmCxsAxbkLM8HdhQplqNyzu0InvcAt+Grq3abWStA8LyneBGOa7wYS/beO+d2B//ps8A3OFQ1WDIxm1kU/yH6Hefcj4LVJX2vx4q5HO41gHNuP/Bb4FJK/D5LXpTVv63Ki8Iph88wlReFNdPLi9mYLDwELDOzJWYWA64E7ihyTEcws0ozqx55DbwMeAof69XBblcDPy5OhEc1Xox3AFeaWdzMlgDLgAeLEN8RRv5jB16Dv9dQIjGbmQHfBNY45/4tZ1PJ3uvxYi7le21mzWZWF7xOApcA7ZTwfZa8KYuyAlReFFopf4YF8am8KIBZVV7kuwd1KT6Ay/E97TcA/1DseMaJcSm+1/zjwNMjcQKNwD3AuuC5ochxfg9fNZjCZ81vP1qMwD8E9/1Z4LISivnbwJPAE/j/0K0lFvOL8NWVTwCPBY/LS/leHyXmkr3XwBnAo0FsTwEfD9aX7H3WI69/DyVfVgRxqrwobMwl+xkWxKDyojAxz5rywoLgRUREREREDjMbmyGJiIiIiMgEKFkQEREREZExKVkQEREREZExKVkQEREREZExKVkQEREREZExKVkQGYOZZczssZzHR6bx3IvN7Klj7ykiIqVO5YXMdJFiByBSogacn8JdRETkaFReyIymmgWRSTCzTWb2r2b2YPA4MVi/yMzuMbMngueFwfo5ZnabmT0ePM4LThU2s2+Y2dNm9qtg9kcREZkhVF7ITKFkQWRsyVHVym/K2dbtnDsX+DLwxWDdl4GbnXNnAN8Brg/WXw/8zjl3JnA2fnZV8NO8f8U5twLYD7wur+9GRETyReWFzGiawVlkDGbW65yrGmP9JuDPnHMbzSwK7HLONZpZB34a+lSwfqdzrsnM9gLznXNDOedYDNzlnFsWLP89EHXO/VMB3pqIiEwjlRcy06lmQWTy3Divx9tnLEM5rzOo/5CIyEyk8kLKnpIFkcl7U87zfcHrPwFXBq/fDPwheH0P8C4AMwubWU2hghQRkaJTeSFlT9mpyNiSZvZYzvIvnHMjw+HFzewBfLJ9VbDufcCNZvZhYC/wl8H69wM3mNnb8b8IvQvYme/gRUSkYFReyIymPgsikxC0QV3lnOsodiwiIlK6VF7ITKFmSCIiIiIiMibVLIiIiIiIyJhUsyAiIiIiImNSsiAiIiIiImNSsiAiIiIiImNSsiAiIiIiImNSsiAiIiIiImP6/wHSvsqMwbyuHAAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 936x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s 2ms/step - loss: 405.3222 - mae: 18.2964 - mse: 405.3222\n",
      "MAE with the adagrad optimizer: 18.2964  reached in 32 s after 500 epochs\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 936x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "for optimizer in ['rmsprop', 'adam', 'adagrad']:\n",
    "    start_time = time.time()\n",
    "    model = initialize_model()\n",
    "    model = compile_model(model, optimizer)\n",
    "\n",
    "    es = EarlyStopping(patience=10, restore_best_weights=True)\n",
    "\n",
    "    history = model.fit(X_train, y_train, \n",
    "                      batch_size=16, \n",
    "                      epochs=500, \n",
    "                      validation_split=0.3,\n",
    "                      callbacks=[es], verbose=0)\n",
    "\n",
    "    res = model.evaluate(X_test, y_test)[1]\n",
    "    end_time = time.time()\n",
    "\n",
    "    print(f'MAE with the {optimizer} optimizer: {res:.4f}  reached in {(end_time - start_time):.0f} s after {len(history.epoch)} epochs')\n",
    "    plot_loss_mae(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13/13 - 0s - loss: 28.4673 - mae: 4.7933 - mse: 28.4673\n",
      "Testing set Mean Abs Error:  4.79\n"
     ]
    }
   ],
   "source": [
    "loss, mae, mse = model.evaluate(X_train_N, X_train, verbose=2)\n",
    "\n",
    "print(\"Testing set Mean Abs Error: {:5.2f}\".format(mae))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_predictions = model.predict(X_test_N).flatten()\n",
    "train_predictions = model.predict(X_train_N).flatten()\n",
    "\n",
    "plt.scatter(y_test, test_predictions)\n",
    "plt.xlabel('True Values')\n",
    "plt.ylabel('Predictions')\n",
    "plt.axis('equal')\n",
    "plt.axis('square')\n",
    "plt.xlim([0,plt.xlim()[1]])\n",
    "plt.ylim([0,plt.ylim()[1]])\n",
    "_ = plt.plot([-100, 100], [-100, 100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ **Question** ❓ Are your predictions better than the benchmark model you've evaluated at the beginning of the notebook?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "YES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❗ **Remark** ❗ \n",
    "- Here, the optimizer is may not be central as the data are in low dimensions and and there are not many samples. However, in practice, you are advised to start with the `adam` optimizer by default which often works best. \n",
    "\n",
    "- Internally, when you call any optimizer with a string, the neural network initializes the hyperparameters the optimizer relies on. Among this hyperparameters, there is quite an important one, the **`learning rate`**. This learning rate corresponds to the intensity of change of the weights at each optimization of the neural network. Different learning rates have different consequences, as shown here : \n",
    "\n",
    "<img src=\"learning_rate.png\" alt=\"Learning rate\" style=\"height:350px;\"/>\n",
    "\n",
    "\n",
    "As the learning rate is initialized with default values when you compile the model optimizer with a string, let's see how to do it differently.\n",
    "\n",
    "\n",
    "❓ **Question** ❓ Instead of initializing the optimizer with a string, we will initialize a real optimizer directly. Look at the documentation of [adam](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Adam) and instantiate it with a learning rate of $0.1$ - keep the other values to their default values. Use this optimizer in the `compile_model` function, fit the data and plot the history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# compile the model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "opt = Adam(learning_rate=0.1)\n",
    "model.compile(optimizer=opt, loss='mae', metrics=['accuracy'] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "9/9 [==============================] - 1s 49ms/step - loss: 10.8791 - accuracy: 0.0000e+00 - val_loss: 5.8587 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/10\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 4.5021 - accuracy: 0.0000e+00 - val_loss: 5.5244 - val_accuracy: 0.0000e+00\n",
      "Epoch 3/10\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 4.7036 - accuracy: 0.0000e+00 - val_loss: 4.6410 - val_accuracy: 0.0000e+00\n",
      "Epoch 4/10\n",
      "9/9 [==============================] - 0s 16ms/step - loss: 3.7067 - accuracy: 0.0000e+00 - val_loss: 4.1906 - val_accuracy: 0.0000e+00\n",
      "Epoch 5/10\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 2.9607 - accuracy: 0.0000e+00 - val_loss: 3.2306 - val_accuracy: 0.0000e+00\n",
      "Epoch 6/10\n",
      "9/9 [==============================] - 0s 17ms/step - loss: 2.8741 - accuracy: 0.0000e+00 - val_loss: 3.5344 - val_accuracy: 0.0000e+00\n",
      "Epoch 7/10\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 2.4851 - accuracy: 0.0000e+00 - val_loss: 3.5542 - val_accuracy: 0.0000e+00\n",
      "Epoch 8/10\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 2.5091 - accuracy: 0.0000e+00 - val_loss: 3.1840 - val_accuracy: 0.0000e+00\n",
      "Epoch 9/10\n",
      "9/9 [==============================] - 0s 15ms/step - loss: 2.3477 - accuracy: 0.0000e+00 - val_loss: 3.1652 - val_accuracy: 0.0000e+00\n",
      "Epoch 10/10\n",
      "9/9 [==============================] - 0s 35ms/step - loss: 2.1350 - accuracy: 0.0000e+00 - val_loss: 2.9256 - val_accuracy: 0.0000e+00\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# fit the model\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "earlystop = EarlyStopping(monitor = 'val_loss',min_delta = 0.005,patience = 15, verbose = 1,restore_best_weights = True, mode='auto')\n",
    "history=model.fit(X_train_N, y_train, epochs=10, validation_split=0.3, validation_data=(X_test, y_test), callbacks=[earlystop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_accuracy</th>\n",
       "      <th>epoch</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2.745371</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.534382</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2.487074</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.554152</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2.601061</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.183999</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2.411526</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.165181</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2.325716</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.925626</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       loss  accuracy  val_loss  val_accuracy  epoch\n",
       "5  2.745371       0.0  3.534382           0.0      5\n",
       "6  2.487074       0.0  3.554152           0.0      6\n",
       "7  2.601061       0.0  3.183999           0.0      7\n",
       "8  2.411526       0.0  3.165181           0.0      8\n",
       "9  2.325716       0.0  2.925626           0.0      9"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hist = pd.DataFrame(history.history)\n",
    "hist['epoch'] = history.epoch\n",
    "hist.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s 2ms/step - loss: 18.1440 - mae: 2.8390 - mse: 18.1440\n",
      "MAE with the rmsprop optimizer: 2.8390  reached in 18 s after 275 epochs\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 936x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s 2ms/step - loss: 20.3454 - mae: 3.0313 - mse: 20.3454\n",
      "MAE with the adam optimizer: 3.0313  reached in 29 s after 457 epochs\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAwsAAAEWCAYAAAApcQvWAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/d3fzzAAAACXBIWXMAAAsTAAALEwEAmpwYAABaFUlEQVR4nO3deZxcZZn3/891Tq29ptPZOysQElkDRBBwAVdAFHfJMyqII+qMo46jo87zOOqov3GecVx4dFRUBlQUVwSVUYFREVHZZAvpQIAAIWtn673W+/fHfaq70unudHe6qzrV3/frVa86deqcOlfdvdx11b2Zcw4REREREZGhgmoHICIiIiIi05OSBRERERERGZaSBRERERERGZaSBRERERERGZaSBRERERERGZaSBRERERERGZaSBZExMrPlZubMLDaGYy81s9sP93VEREREqknJgtQkM9tsZlkzmzNk/33RB/XlVQpNRERq0HjqHTP7eLTv9CHHXmpmBTPrHnJbVKG3IXIQJQtSy54A1pUemNmJQLp64YiISI07ZL1jZga8GdgDXDLMa/zROdcw5LZ1KoMWGY2SBall3wbeUvb4EuBb5QeYWbOZfcvMdpnZk2b2f8wsiJ4LzeyzZtZhZo8DLx/m3G+a2TYze8bMPmVm4XiDNLNFZnajme0xs01m9vay5043s7vNrNPMdpjZ56L9KTP7jpntNrN9ZnaXmc0f77VFRGRSHbLeAZ4HLALeC1xsZokKxSYyIUoWpJb9CWgys2dFH+LfCHxnyDH/D2gGjgJegP8n/9boubcDFwKnAGuB1w059xogDxwTHfNS4K8nEOf3gC34yuN1wP9nZi+Knvsi8EXnXBNwNPCDaP8lUdxLgFbgnUDfBK4tIiKTZyz1ziXAz4DvR48vrGB8IuOmZEFqXelbnpcA7cAzpSfK/pF/xDnX5ZzbDPwHvnkY4A3AF5xzTzvn9gD/WnbufOB84H3OuR7n3E7g88DF4wnOzJYAzwU+5Jzrd87dB3yjLIYccIyZzXHOdTvn/lS2vxU4xjlXcM7d45zrHM+1RURkSoxW79QBrwe+65zLAT/i4K5Iz4lajEu3xyoUt8iwNBuL1LpvA7cBKzi4KXgOkACeLNv3JNAWbS8Cnh7yXMkyIA5s891PAZ98lx8/FouAPc65riHXWRttvw34F6DdzJ4APuGc+3n0vpYA15nZLPw3V/87qnxERKR6Rqt3Xo1vkb4penwtcIuZzXXO7Yr2/ck599yKRCoyBmpZkJrmnHsSP+DsAuAnQ57uwH9Dv6xs31IGvwXahv9AXv5cydNABpjjnJsV3Zqcc8ePM8StwGwzaxwuBufco865dcA84N+AH5lZvXMu55z7hHPuOOAsfDP2WxARkao6RL1zCdAAPGVm24Ef4r94WofINKVkQWaCtwEvdM71lO90zhXwYwA+bWaNZrYMeD+D/Ut/ALzHzBabWQvw4bJztwG/Bv7DzJrMLDCzo83sBeMJzDn3NHAH8K/RoOWTonivBTCzN0XfOBWBfdFpBTM718xOjLpSdeKTnsJ4ri0iIlNmuHqnDXgR/sudNdHtZPwXQcPNiiQyLShZkJrnnHvMOXf3CE//HdADPA7cDnwXuCp67uvAr4D7gXs5+Buit+C7MT0M7MX3PV04gRDXAcvxrQzXAx9zzt0cPXcesN7MuvGDnS92zvUDC6LrdQIbgN9x8CA6ERGpghHqnecB9znnfu2c2166AVcAJ5nZCdFxZw6zzsKzK/oGRMqYc67aMYiIiIiIyDSklgURERERERnWlCULZrbEzH5jZhvMbL2ZvTfaP9vMbjazR6P7lrJzPhItSrXRzF42VbGJiEj1TKR+GHL+eVE9scnMPjzcMSIiMjmmrBuSmS0EFjrn7o1merkHeBVwKX6qyM9E/+RbnHMfMrPj8ItTnY6fTvIW4NhoEKqIiNSI8dYPQ84NgUfwc9hvAe4C1jnnHq7gWxARmTGmrGXBObfNOXdvtN2FH4TZBlyEX/mW6P5V0fZFwHXOuYxz7glgEz5xEBGRGjKB+qHc6cAm59zjzrkscF10noiITIGKLMpmZsuBU4A/A/OjaSdxzm0zs3nRYW34ZdJLtjC4OFb5a10OXA5QV1d32ooVKyYUU6FQIAzDEZ/f21dgVt+TJOJxco1LRjzuSHeocpgpVA6eyqH2ymD9+vUdzrm51Y5jJGOsH8q1ceDih1uAM0Z47SmpL/b1F6jv3UJ96Mg2L5/Qax6Jau1vYyJUBp7Kwau1chipvpjyZMHMGoAfA+9zznWWrXZ70KHD7Duoj5Rz7krgSoC1a9e6u+8eaUbM0bW3t7N69eoRn//G7x/n1Jtfz4lHLSF+6U8ndI0jwaHKYaZQOXgqh9orAzN78tBHVcc46ocDThtm37D9aaeqvrjhvmcIf3wZL23dSeJ9f5nQax6Jau1vYyJUBp7Kwau1chipvpjS2ZDMLI6vCK51zpXmqN8R9Vct9VvdGe3fwoGr5S7GzztfFWFg5AmhmK9WCCIiNWuc9UO5qtcVcxqSdLo09HdW8rIiIlUxlbMhGfBNYINz7nNlT93I4EqFlwA3lO2/2MySZrYCWAncOVXxHUoYGAWnZEFEZLJNoH4odxew0sxWmFkCuDg6r2JaGxJ0UUeY667kZUVEqmIquyGdDbwZeNDM7ov2/RPwGeAHZvY24Cng9QDOufVm9gP8arh54G+rORNSYEaeAKdkQURkso2rfjCzRcA3nHMXOOfyZvZu/OrqIXCVc259JYNvrU/S7dKEhX4o5CCMV/LyIiIVNWXJgnPudobvWwrwohHO+TTw6amKaTzCwCioG5LIlMjlcmzZsoX+/v5qh3KQXC7Hhg0bqh3GuKVSKRYvXkw8Pv0/uI63fnDObQUuKHt8E3DT1ER3aLPrE3RbnX/Q3wn1rdUKRaTmqb6YfOOtLyoyG9KRKIxaFihqmQeRybZlyxYaGxtZvnw5YxzUWjF9fX2k0+lqhzEuzjl2797Nli1bmOiMPzJ2YWAU401+WHVmv5IFkSmk+mJyTaS+mNIBzkcyM9SyIDJF+vv7aW1tnXb/+I9UZkZra+u0/OatVrlUk9/IdFU3EJEap/pick2kvlCyMAI/G1KgZEFkiugf/+RSeVaWpRr9hmZEEply+v82ucZbnkoWRqAxCyIiMpIg1ew3MkoWRKS2KVkYgZ8NKdSYBZEatHv3btasWcOaNWtYsGABbW1tA4+z2eyo595999285z3vqVCkMl3F01HLQra3uoGIyJRSfaEBziMKA6PfBZhaFkRqTmtrK/fddx8AH//4x2loaOADH/gA4Aes5fN5YrHh/z2uXbuWtWvXVipUmaaS6Qa/kdVaCyK1TPWFWhZGNNiyoGRBZCa49NJLef/73895553Hhz70Ie68807OOussTjnlFM466yw2btwIwG9/+1suvPBCwFccl112Geeccw5HHXUUV1xxRTXfglRQqt4PcC5me6ociYhU2kyrL9SyMILAoECAVW9dOJEZ4RM/W8/DWye33/dxi5r42CuOH/d5jzzyCL/4xS9oaGigs7OT2267jVgsxi233MI//dM/8eMf//igc9rb2/nNb35DV1cXq1at4l3vetcRsdaBHJ5Uve+GlO3tJlXlWERmCtUX1aFkYQRhYOSIQWH0/mgiUjte//rXE4YhAPv37+eSSy7h0UcfxczI5XLDnvPyl7+cZDJJMplk3rx57Nixg8WLF1cybKmCpvo6si4k09ulZEFkBppJ9YWShREEUbKgMQsiU2si3+hMlfr6+oHtj370o5x77rlcf/31bN68mXPOOWfYc5LJ5MB2GIbk8/qfMRM0p+P0kiLfrzELIpWi+qI6NGZhBKEZOUKsOHx2KCK1bf/+/bS1tQFw9dVXVzcYmXaa0nF6SVLo16JsIjNdrdcXShZG4BdlixEUc+BctcMRkQr7x3/8Rz7ykY9w9tlnUyho7JIcqCkVp9elKGY0wFlkpqv1+kLdkEYQmJF1UfEUchBLVDcgEZkSH//4xw943NfXB8CZZ57JI488MrD/k5/8JADnnHPOQBPz0HMfeuihKYtTppfGVIy9JKnPKVkQmSlman2hloURBAY5/MAV1BVJRETK1Cdj9JHEtCibiNQ4JQsjiIXRbEigGZFEROQA9cmQHpciyCtZEJHapmRhBGEQlCULalkQEZFByVhIv6UIlSyISI1TsjCC0IysWhZERGQE2SBNrNBX7TBERKaUkoURhIGRc0oWRERkeIWYkgURqX1TNhuSmV0FXAjsdM6dEO37PrAqOmQWsM85t8bMlgMbgI3Rc39yzr1zqmIbi4EVnEHdkEREJtl46ohhzt0MdAEFIO+cW1uBkA+SD9Mk8v3VuLSISMVMZcvC1cB55Tucc290zq2J/vn/GPhJ2dOPlZ6rdqIAQ5MFtSyI1JpzzjmHX/3qVwfs+8IXvsB73/veEY+/++67AbjgggvYt2/fQcd8/OMf57Of/eyo1/3pT3/Kww8/PPD4n//5n7nlllvGGX1NuJrx1RFDnRsdW5VEAYB4mjg5KNbevOoi4o1UV/zN3/zNiMfXWl0xZcmCc+42YM9wz5mZAW8AvjdV1z9csUBjFkRq2bp167juuusO2Hfdddfxhje84ZDn3nTTTcyaNWtC1x1aAfzLv/wLL37xiyf0WkeyI72OACCW8vdqXRCpWSPVFevWrTvkubVSV1RrzMLzgB3OuUfL9q0ws7+Y2e/M7HlVimuAuiGJ1LbXve51/PznPyeTyQCwefNmtm7dyve//33Wrl3L8ccfz8c+9rFhz12+fDkdHR0AfPrTn2bVqlW8+MUvZuPGjQPHfP3rX+fZz342J598Mq997Wvp7e3ljjvu4MYbb+SDH/wga9as4bHHHuPSSy/lRz/6EQC33norp5xyCieeeCKXXXbZQGzLly/nYx/7GKeeeionnngi7e3tU1k008FwdUQ5B/zazO4xs8srGNcBLJ72GzmNWxCpVSPVFd/97nc5++yzZ0RdUa0VnNdx4DdG24ClzrndZnYa8FMzO9451zn0xKhiuBygra1twgXR0dEx6rm7evIDA5yfeuIxentnTeg6092hymGmUDl4lSqHXC43sPJl/JaPYjsndyVLN+8Eci/+5KjH1NXVcdppp3HDDTfwile8gm9/+9u89rWv5X3vex/z5s2jUChwwQUX8PKXv5wTTzyRYrFIf38/fX19OOfo6+vjD3/4A9/73ve44447yOfznHXWWZx00kn09fVx/vnn86Y3vQnwTc5f/epXede73sXLX/5yzj//fF796lcDUCgUyGaz7N27l0suuYSbbrqJlStX8td//ddcccUVvPvd78Y5R3NzM3/4wx/42te+xmc+8xm+8pWvDFuuNfJ7PLSOGOps59xWM5sH3Gxm7VFLxQGmur7I5B0Am9ofIl+/YEKvfSTR/0mVQUkly6Ha9cVIdcUHPvABmpqaMLMjrq4oletYf4YVTxbMLAa8BjittM85lwEy0fY9ZvYYcCxw99DznXNXAlcCrF271q1evXpCcbS3tzPauS2d/QMrOC9tWwDHTOw6092hymGmUDl4lSqHDRs2kE5H38rGYhCEk3uBWIxY6fVH8aY3vYnrr7+eN7zhDfz4xz/mqquu4oYbbuDqq68mn8+zbds2Hn/8cU4//XSCICCVSpFOpzEz0uk0d911F695zWtobW0F4KKLLiIej5NOp3nsscdYt24d+/bto7u7m5e97GWk02nCMCSRSAy8/9Ljp556iqOOOoqTTjoJgMsuu4wvf/nLfPCDH8TMeOMb30g6nebMM8/k5z//+WD5lYnH40f87/FwdcRQzrmt0f1OM7seOB04KFmY6vpiw5/nwF44ZlkbzFk5odc+kuj/pMqgpJLlMB3qi+Hqip/97Gd89atfpVgsHnF1BYyvvqhGy8KLgXbn3JbSDjObC+xxzhXM7ChgJfB4FWIbEB4wZkHdkESmzPmfqdqlX/WqV/H+97+fe++9l76+PlpaWvjiF7/I3XffTUtLC5deein9/aP3R/fd6w926aWX8tOf/pSTTz6Zq6++mt/+9rejvo5zbtTnk8kk4CuMfD4/6rFHuIPqiHJmVg8EzrmuaPulwL9UMsCSMOErYZfrZfjfAhGZVFWqL4arKz772c9y2223sWjRopqvK6ZszIKZfQ/4I7DKzLaY2duipy7m4Obl5wMPmNn9wI+Adzrnhh34VimxwMhrgLNITWtoaOCcc87hsssuY926dXR2dlJfX09zczM7duzgv//7v0c9//nPfz7XX389fX19dHV18bOf/Wzgua6uLhYuXEgul+Paa68d2N/Y2EhXV9dBr7V69Wo2b97Mpk2bAPj2t7/NC17wgkl6p9PPeOoIM1tkZjdFD+cDt0f1xZ3AL5xzv6xU3OWCKFnI9msVZ5FaNtPriilrWXDODTtM3Dl36TD7foyfJm/aCAIb6IakZEGkdq1bt47XvOY1XHfddaxevZqTTz6Z448/nqOOOoqzzz571HNPPfVU3vjGN7JmzRqWLVvG8543ODfDJz/5Sc444wyWLVvGiSeeOPBP/+KLL+btb387V1xxxcBgNYBUKsV//dd/8frXv558Ps+zn/1s3vnOqs8iPWXGWUdsBS6Ith8HTp7S4MYoTNQBkO3rJVnlWERkag2tK0455RROO+00jj766JqvK+xQzRnT2dq1a11pLtvxOlR/u95snpd97Fv8Pvn38KqvwppDT5F1JFL/S0/l4FVyzMKznvWsKb/ORPT19Y3Yx3O6G65czeyeqq5FME1MRX1x882/4CV/+F90XPRt5pzyysMNcdrT/0mVQUmlxyyovph846kvqjV16rQXmA3MhqSWBRERGSqWrAcgp25IIlLDlCyMIKYVnEVEZBTxpO+GlFeyICI1TMnCCLQom8jUOpK7QE5HKs/KS6ajZCGrZEFkKun/2+Qab3kqWRiBmZE3tSyITIVUKsXu3btVAUwS5xy7d+8mlUpVO5QZJZ7y3ZDyGSULIlNF9cXkmkh9Ua0VnI8ILkj4jaJaFkQm0+LFi9myZQu7du2qdigHyeVyxOPxaocxbqlUisWLF1c7jBklmfItC8VsX5UjEaldqi8m33jrCyULo3BBgMMwdUMSmVTxeJwVK1ZUO4xhabYTGat02rcsFNUNSWTKqL6oPnVDGkUsCClYXN2QRETkIOlkjH4Xx+XUsiAitUvJwijCwChYTAOcRUTkIOlESD8JXK6/2qGIiEwZJQujCAMjb3HIZ6odioiITDPpuE8WUMuCiNQwjVkYRRgYeRKQ17dGIiJyoHgYkCGhL5REpKYpWRhFaEZOyYKIiIwgZ3GsoGRBRGqXuiGNIgyMnKmJWUREhpe3hJIFEalpShZGEQuNrCXVsiAiIsPKW5xAM+aJSA1TsjCK0IysqT+qiIgML28JgqKSBRGpXUoWRhEGUcuCuiGJiMgwCkGCUMmCiNQwJQujCAMjqwHOIiIyAiULIlLrlCyMIgyMjAY4i4jICApBgphTsiAitUvJwihialkQEZFRFIMEMZerdhgiIlNmypIFM7vKzHaa2UNl+z5uZs+Y2X3R7YKy5z5iZpvMbKOZvWyq4hqPIIgGOKtlQURkUo23jhhy7nlRXbHJzD5cuagPVgyTxNWyICI1bCpbFq4Gzhtm/+edc2ui200AZnYccDFwfHTOf5pZOIWxjUksMPrVsiAiMhWuZox1RLmobvgycD5wHLAuqkOqwoUJ4mpZEJEaNmXJgnPuNmDPGA+/CLjOOZdxzj0BbAJOn6rYxiowI0PUsuBctcMREakZ46wjyp0ObHLOPe6cywLX4euQqnBhkjhKFkSkdsWqcM13m9lbgLuBf3DO7QXagD+VHbMl2ncQM7scuBygra2N9vb2CQXR0dFxyHMz/X105QxwtD/8EITxCV1rOhtLOcwEKgdP5aAymAaGqyPKtQFPlz3eApwx3AtVor7oyxVJuBztGzaA2YRe/0ihvw2VQYnKwZsp5VDpZOErwCcBF93/B3AZMNx/2GG/ynfOXQlcCbB27Vq3evXqCQXS3t7Ooc5tuqOTQrYeMrD66KWQap7QtaazsZTDTKBy8FQOKoMqG6mOKDet6ovtt88i2O9YvfJoiCUm9PpHCv1tqAxKVA7eTCmHis6G5Jzb4ZwrOOeKwNcZ7Gq0BVhSduhiYGslYxtOaPgxCwA5jVsQEZlKo9QR5aZVfWFhEgCnsW0iUqMqmiyY2cKyh68GSrNg3AhcbGZJM1sBrATurGRswwmDgIyLuh7lNSOSiMhUGqWOKHcXsNLMVphZAj85xo2ViG9YMZ8s5DJKFkSkNk1ZNyQz+x5wDjDHzLYAHwPOMbM1+CbjzcA7AJxz683sB8DDQB74W+dcYapiG6swgH6nlgURkck2njrCzBYB33DOXeCcy5vZu4FfASFwlXNufeXfgWexFADZTB+13QlJRGaqKUsWnHPrhtn9zVGO/zTw6amKZyJiQUBvqYjUsiAiMmnGU0c457YCF5Q9vgk4aFrVaggSvmUhm+mtciQiIlNDKziPIgyMfucrArUsiIjIUEGpG1JWdYSI1CYlC6MIA6PXqWVBRESGFybSgMYsiEjtUrIwigNaFvKZ6gYjIiLTThj3dUQ+oy+URKQ2KVkYRWhGbzH0D3KqCERE5EBh3Lcs5NUNSURqlJKFUYSh0VeaDUlzaIuIyBBhws+GlM/qCyURqU1KFkYRC4yeYrTOgloWRERkiFjSJwuFrLqqikhtUrIwilgQDCYLalkQEZEh4tEA54K+UBKRGqVkYRTxmNFViGZDUkUgIiJDlFoWijm1LIhIbVKyMIp4ENBbCABTy4KIiBwkkagDoKgBziJSo5QsjCIeBhSd4WIptSyIiMhB4qmoZUHTa4tIjVKyMIpYaH4jntI6CyIicpBE0o9ZcPpCSURqlJKFUSRCXzwultIKziIicpBkyndDcvlslSMREZkaShZGUWpZcGEKcuqPKiIiB0omEuRdoHFtIlKzlCyMIn5Ay4IqAhEROVAQGFniUFBXVRGpTUoWRhGPWhaKoQY4i4jI8LLEQd2QRKRGKVkYRSzwxVNUy4KIiIwgZ3FMLQsiUqOULIwiHvPFUwiSalkQEZFh+WRBLQsiUpuULIwiHpS6ISXVsiAiIsPKkSAoqmVBRGrTlCULZnaVme00s4fK9v27mbWb2QNmdr2ZzYr2LzezPjO7L7p9dariGo/SAOdCqG5IIiKTaTx1xDDnbjazB6P64u6KBT2CvMUJ1LIgIjVqKlsWrgbOG7LvZuAE59xJwCPAR8qee8w5tya6vXMK4xqz0tSpeXVDEhGZbFczvjpiqHOj+mLtFMU3ZvkgQaiWBRGpUVOWLDjnbgP2DNn3a+dcPnr4J2DxVF1/MpQWZcuHdZDtrXI0IiK1oxbqiJJCkCAsqmVBRGpTrIrXvgz4ftnjFWb2F6AT+D/Oud8Pd5KZXQ5cDtDW1kZ7e/uELt7R0XHIc7fu8K0Ju3tyzM92s3HDBjCb0PWmq7GUw0ygcvBUDiqDaWRoHVHOAb82Mwd8zTl35XAHVaq+yBRDUsWemv+90d+GyqBE5eDNlHKoSrJgZv8byAPXRru2AUudc7vN7DTgp2Z2vHOuc+i5UaVwJcDatWvd6tWrJxRDe3s7hzq3r24vsJVUywLs6QKrj1kB8dSErjddjaUcZgKVg6dyUBlMB8PUEUOd7ZzbambzgJvNrD1qqThApeqL+xJpEplOjq7x3xv9bagMSlQO3kwph4rPhmRmlwAXAn/lnHMAzrmMc253tH0P8BhwbKVjG6o0wDkXpv2ObE8VoxERqX3D1RFDOee2Rvc7geuB0ysX4cGKQYKYUzckEalNFU0WzOw84EPAK51zvWX755pZGG0fBawEHq9kbMMpJQvZoJQsdFcxGhGR2jZSHTHkmHozayxtAy8FHhru2EophkliLlfNEEREpsxUTp36PeCPwCoz22JmbwO+BDTim43Lp0h9PvCAmd0P/Ah4p3Nuz7AvXEGl2ZD6gzq/Qy0LIiKTYjx1hJktMrObolPnA7dH9cWdwC+cc7+swlsY4MIkCbUsiEiNmrIxC865dcPs/uYIx/4Y+PFUxTJR8cDnUplA3ZBERCbTOOuIrcAF0fbjwMlTGNq4uTBBAiULIlKbtILzKOIx37KQsVKy0FXFaEREZDoqxtIk1bIgIjVKycIoYlHLQr9aFkREZAQuVkfKcrhC/tAHi4gcYZQsjKK0KFsf0XSpShZERGSIYsKPa8v0qfVZRGqPkoVRDAxwtlKyoNmQRERkiHg9ANk+1REiUnuULIyiNHVqn6kbkoiIjCDhk4WcWhZEpAYpWRhFPGpZ6HUJwJQsiIjIweK+G1JOLQsiUoOULIzCzIgFRq6I/+ZIyYKIiAwRJhsAyPerjhCR2qNk4RBioZEvuChZ0LdGIiJyIEv6bkj5fnVDEpHao2ThEOJhQG4gWdC3RiIicqAw5VsWCmpZEJEapGThEBJhQLZQULIgIiLDKiULRbU+i0gNUrJwCMlYQDZfhEQDZNTELCIiB4pF3ZCKGSULIlJ7lCwcQiIWkMkX1bIgIiLDiqWbAHCZ3ipHIiIy+ZQsHEIyFpLJKVkQEZHhJdK+ZUGTYIhILRpTsmBm9WYWRNvHmtkrzSw+taFND8l4QCZfgESjkgURkTJm1jTKc0srGUs1JeMx+lwCcmpZEJHaM9aWhduAlJm1AbcCbwWunqqgppPkAd2Q9K2RiEiZ35Y2zOzWIc/9tKKRVFEyFtJDCsvpCyURqT1jTRbMOdcLvAb4f865VwPHTV1Y00cyFmrMgojI8Kxse/Yoz9W0ZDygy6WJZTurHYqIyKQbc7JgZmcCfwX8ItoXm5qQphffshBNnVrMQT5b7ZBERKYLN8L2cI9rVioWsp96YlnNmCcitWesH/jfB3wEuN45t97MjgJ+M2VRTSPJeBANcPbzaJPthtjQL9BERGakeWb2fnwrQmmb6PHc6oVVWfHQ6KSe+Tm1LIhI7RlTy4Jz7nfOuVc65/4tGujc4Zx7z2jnmNlVZrbTzB4q2zfbzG42s0ej+5ay5z5iZpvMbKOZvWzC72iSHdANCdQVSURk0NeBRqChbLv0+BujnTjeOmLIuedFdcUmM/vwpL2bCTIzumggkVeyICK1Z6yzIX3XzJrMrB54GNhoZh88xGlXA+cN2fdh4Fbn3Er8QOkPR69/HHAxcHx0zn+aWTjmdzGFDuiGBBrkLCIScc59YqQbcNMhTr+aMdYR5aK64cvA+fixc+uiOqSqeoIGUkoWRKQGjXXMwnHOuU7gVfgKYCnw5tFOcM7dBuwZsvsi4Jpo+5ro9Ur7r3POZZxzTwCbgNPHGNuUGpgNKdnod2iFThGRYZnZcWb2L2b2KPCV0Y4dZx1R7nRgk3PucedcFrguOq+q+oIGUvkucDNmqIaIzBBjHbMQj9ZVeBXwJedczswm8h9xvnNuG4BzbpuZzYv2twF/KjtuS7TvIGZ2OXA5QFtbG+3t7RMIAzo6OsZ0bnfnPvqyeZ7c3sUy4KlN6+ntaZzQNaejsZZDrVM5eCoHlcF4mdkyYF10ywPLgLXOuc0TeLmR6ohybcDTZY+3AGeMEFvF6otuqyekwMaH/oKL103oOtOd/jZUBiUqB2+mlMNYk4WvAZuB+4HbosphMttbh5tib9hkxDl3JXAlwNq1a93q1asndMH29nbGcu7CJzeSe3g/y449EW6FpfOaYYLXnI7GWg61TuXgqRxUBuNhZncAzfhv91/nnHvUzJ6YYKIw5ssOs6/q9cXNqVnQA6uWzoPmxRO6znSnvw2VQYnKwZsp5TDWAc5XOOfanHMXOO9J4NwJXG+HmS0EiO53Rvu3AEvKjlsMbJ3A60+6ZCyg6CAfj2ZD6lefVBGRyC78gOb5DM5+dDj9cEaqI8pNy/oiF2/2G337qhqHiMhkG+sA52Yz+5yZ3R3d/gOon8D1bgQuibYvAW4o23+xmSXNbAWwErhzAq8/6ZJxX0SZWJQsZJQsiIgAOOcuAk4E7gU+YWZPAC1mNtExZyPVEeXuAlaa2QozS+Anx7hxgtebNIVklCz076tqHCIik22sA5yvArqAN0S3TuC/RjvBzL4H/BFYZWZbzOxtwGeAl0SD314SPcY5tx74AX6mpV8Cf+ucK4z/7Uy+ZMxPypQJ6gBTy4KISBnn3H7n3FXOuZcAzwE+BnzBzJ4e7bzx1BFmtsjMboqulwfeDfwK2AD8IKpDqiqXnuM3enZVNxARkUk21jELRzvnXlv2+BNmdt9oJzjn1o3w1ItGOP7TwKfHGE/FJGNRy0LBQbJJLQsiIiNwzu0ArgCuiMa2jXbsmOsI59xW4IKyxzdx6KlZK6pQSha6h+s5JSJy5BprstBnZs91zt0OYGZnA31TF9b0MdANKVeEVJNaFkREImZ2qO4/r6xIINNBXSt5FxDr3lHtSEREJtVYk4V3At8ys6hTJnsZ7Fda0wa6IeWLalkQETnQmfhpTL8H/JnhZyqaEeqTcTpoZn7XjplbCCJSk8aULDjn7gdONrOm6HGnmb0PeGAKY5sWBroh5QtRy8L+KkckIjJtLMCPLVgH/C/gF8D3psMYgkqrS8bY5ZqZ27WDsNrBiIhMorEOcAZ8khCt5Azw/imIZ9optSz059SyICJSzjlXcM790jl3CX5w8ybgt2b2d1UOreLqkzE6XDNFjVkQkRoz1m5Iw5kRLa3phE8WerJ537LQsbHKEYmITB9mlgRejm9dWI4f4PyTasZUDfWJkF1uFtb9SLVDERGZVIeTLBzOwjtHjIakL6LeTAHqWqF3T5UjEhGZHszsGuAE4L+BTzjnHqpySFVTn4yxiWbC3l1QLEIwroZ7EZFpa9Rkwcy6GD4pMCA9JRFNM3WlloVMHurm+G5I+QzEklWOTESk6t4M9ADHAu8xG2hwNsA555qqFVil1SdivmXB5aFvL9S3VjskEZFJMWqy4JxrrFQg01WpZaEnmx/859+7G5oWVTEqEZHqc87p6/NIQ8onCwB071CyICI1Q//oD6Eu6VsWerMFqJ/rd2qFThERKdOU8gOcAejRIGcRqR1KFg4hGQuJh0Z3qRsSQE9HdYMSEZFppTEVZxdRsqAZkUSkhihZGIO6RIzeTB7qlSyIiMjBGod2QxIRqRFKFsagIRmjO1MYTBZ6lSyIiMigVDwkE6snZ0no2l7tcEREJo2ShTGoS4T0ZvOQmgVBTC0LIiJykKZUnP3xudC5tdqhiIhMGiULY1CfjPkxC2Z+rQUNcBYRkSEaU3F2h3Nh/9PVDkVEZNIoWRiD+mToZ0MCPyNS7+7qBiQiItNOUyrGjmAe7N9S7VBERCaNkoUxqE/E/KJsELUsqBuSiIgcqDEVZ5tr9WMW8tlqhyMiMimULIxBfTLmF2UDP8hZ3ZBERGSIxlSMp4pzAAedz1Q7HBGRSaFkYQzqkyE9GXVDEhGRkTWl4mzOtfgH6ookIjWi4smCma0ys/vKbp1m9j4z+7iZPVO2/4JKxzaSA7shzYFMJ+Qz1Q1KRKQGjVRHDDnmHDPbX3bMP1cp3APMqo+zsV/JgojUllilL+ic2wisATCzEHgGuB54K/B559xnKx3TodQnY2TyRfKFIrH6Vr+zpwOa26obmIhIjRmljhjq9865CysY2iHNqU/ydGEWxNGMSCJSM6rdDelFwGPOuSerHMeo6hIhAD3Zgu+GBNCzs4oRiYjMCEdEHVHS2pAgQ4J83TwlCyJSMyresjDExcD3yh6/28zeAtwN/INzbu/QE8zscuBygLa2Ntrb2yd04Y6OjjGf27W3E4AHHt7Ikv4My4GnN9xDT2d6QteeTsZTDrVM5eCpHFQG08zQOqLcmWZ2P7AV+IBzbv3QAypdX/Tu7QWgM9ZKYutGttTY75H+NlQGJSoHb6aUQ9WSBTNLAK8EPhLt+grwScBF9/8BXDb0POfclcCVAGvXrnWrV6+e0PXb29sZ67mPZLbCHbtYuGQZy5Oz4GZY0hzCBK89nYynHGqZysFTOagMpoth6ohy9wLLnHPd0fi2nwIrhx5U6foi37gfbt5Gtmkps/ser7nfI/1tqAxKVA7eTCmHanZDOh+41zm3A8A5t8M5V3DOFYGvA6dXMbYDNCSjbkiZAjTM9zu7tlcxIhGRmndAHVHOOdfpnOuOtm8C4mY2p9IBDtXakABgT3wR7HsSioUqRyQicviqmSyso6x52cwWlj33auChikc0grqEb4DpyeQhjPtxC13bqhyViEhNO6COKGdmC8zMou3T8XVZ1ee0nl3vk4VtsTYoZDVuQURqQlW6IZlZHfAS4B1lu/+vma3Bd0PaPOS5qmpIRslCNvqWqHGBWhZERKbIcHWEmb0TwDn3VeB1wLvMLA/0ARc751w1Yi2XjIV+YTai7752PwYty6sak4jI4apKsuCc6wVah+x7czViGYuB2ZBKay00LoKurVWMSESkdo1QR3y1bPtLwJcqHddYzGlIsqm4wD/Y8zh+QicRkSNXtadOPSIMtixEyULLctjzBFT/iywREZlGWusTPNFXD8km2FX7s6SISO1TsjAGdcmyMQsAc4+FbDd0PlPFqEREZLqZXZ9gd08OFpwI2x6odjgiIodNycIY1MXLZkMCmBtNk7VrY5UiEhGR6ai1IcnungwsPBm2PwiFfLVDEhE5LEoWxiAIjLpESHepZWHOKn+vZEFERMrMaUiwpydLccHJkO+D3Y9WOyQRkcOiZGGMmlJxOvty/kH9HEjPhg4lCyIiMqi1PkHRQWfL8X7H1vuqGo+IyOFSsjBGzek4nf1RsmAGc1epZUFERA7Q2pAEYFdiCcTrYNt91Q1IROQwKVkYo+Z0nP2llgWIkoV2zYgkIiID5kTJws6evB/krJYFETnCKVkYo6Z0nP19ZQPV5qyCvr3Qs6t6QYmIyLSysDkFwLb9/bDoVNh2PxRyhzhLRGT6UrIwRk3p2OCYBYB5pRmRNI+2iIh4C0rJwr4+WPJsP8h5x0NVjkpEZOKULIzRQd2Q5kWD13Y8XJ2ARERk2knFQ2bXJ9jW2Q9LzvA7n76zukGJiBwGJQtj1JyO053Jky8U/Y6GeVDXCjvXVzcwERGZVhY2p3zLQvNimH00bPzvaockIjJhShbGqDkdB6CrPxq3YAbzjoPtal4WEZFBC5vTfswCwHEXwRO3Qc/u6gYlIjJBShbGqCnlk4UDuiItOR22PwCZ7ipFJSIi083C5tSByYIrwMZfVDcoEZEJUrIwRrPqfLKwrzxZWP48KObhqT9VKSoREZluFs5Ksb8vR282DwtPhpbl8NBPqh2WiMiEKFkYo9n1CQD29mQHdy45A4I4PPG7KkUlIiLTzQHTp5rBSW+Ex38LezdXNS4RkYlQsjBGpYV2OrozgzsTdb4r0ubfVykqERGZbhY2pwHYti/qinTqJWAB3H1VFaMSEZkYJQtj1NrgWxZ2l7csgO+KtO1+6N1ThahERGS6WRQlC1v39/kdzW2w+gK499uQ66tiZCIi46dkYYzqEjHS8ZDd5S0LAKtfDq4ID/6wOoGJiMi0Mr/Zt0QPtCwAnP4O6NsD911bpahERCamKsmCmW02swfN7D4zuzvaN9vMbjazR6P7lmrENprWhgS7u4e0LCw8CRaugXuuAeeqEpeISC0Zro4Y8ryZ2RVmtsnMHjCzU6sR50iSsZD5TUme3ts7uHP5c2Hxs+F3/1fTqIrIEaWaLQvnOufWOOfWRo8/DNzqnFsJ3Bo9nlZaG5J0DO2GBHDaJX5xtmfurXxQIiK1aWgdUe58YGV0uxz4SkUjG4MVc+p5oqNncIcZXPh532X15+/Tl0sicsSYTt2QLgKuibavAV5VvVCG11qfOLgbEsAJr4N4Hdx7dcVjEhGZgS4CvuW8PwGzzGxhtYMqt2JOw4HJAsCCE+GF/wc23Aj3f686gYmIjFOsStd1wK/NzAFfc85dCcx3zm0DcM5tM7N5w51oZpfjv0mira2N9vb2CQXQ0dEx7nNj+T527Osd9rwFi19E0wM/ZNOKSyjGGyYUUzVMpBxqkcrBUzmoDKaJ4eqIcm3A02WPt0T7tpUfVM36or7Yw56eLHfdv57GZDj4xOyXsHTuDaR+9vc82ddApuXYCcVUDfrbUBmUqBy8mVIO1UoWznbObY0SgpvNbMwlHVUaVwKsXbvWrV69ekIBtLe3M95zj3oCfvPE46xatQozO/DJpn+AK3/GsZ1/gLPfO6GYqmEi5VCLVA6eykFlME0cVEc4524re96GOeegfj3VrC+eU9zBN+7eTTBrEauXDRmCt/i78I0XseJ374b/9UNYesaE4qo0/W2oDEpUDt5MKYeqdENyzm2N7ncC1wOnAztKzcjR/c5qxDaaOQ0JcgVHZ3/+4CcXnQJHnQu3f16D10REDsMIdUS5LcCSsseLga2ViW5sVi1oBGDj9q6Dn2xaCJf9EurmwLcuggc0m56ITF8VTxbMrN7MGkvbwEuBh4AbgUuiwy4Bbqh0bIcysNbCcOMWAM77V8h0ww8vgWKhgpGJiNSGUeqIcjcCb4lmRXoOsL/UjXW6WNySpiEZo3175/AHzFoKl/0KFp4MP/lr+NFl0Le3skGKiIxBNVoW5gO3m9n9wJ3AL5xzvwQ+A7zEzB4FXhI9nlZa6/3c2QctzFYy71l+tovNv4e7vlnByEREasawdYSZvdPM3hkdcxPwOLAJ+DrwN9UJdWRmxuoFjWzYNkKyANAwF956E7zwo/DwDfDl5/hpuAvDtF6LiFRJxccsOOceB04eZv9u4EWVjmc8DtmyAHDKm2D9T+CXH/JNzc96RYWiExE58o1SR3y1bNsBf1vJuCbihLZmrrvrKTL5AslYOPxBQQjP/wAc82K46YPws/fAn/4TXvQxWHW+n3JVRKSKptPUqdPenAbfstAxdGG2cmbwxu/4MQzXvwuevqtC0YmIyHRy1tGt9OeK3PfUvkMfvGgNvO3Xvv4oFuC6dfDNl8D910G295Cni4hMFSUL4zC73rcsdIzWsgCQqIc3fBvq58B33wA7Hq5AdCIiMp2ccVQrgcEfHhvjpBdmvjX6b/4ULeC2G65/B3xuNfz3h2DrfVrMTUQqTsnCOMTDgDkNSbbt6z/0wc1t8KYf+ybmb74Edj829QGKiMi00ZyOc2JbM3ds6hjfiWEM1l4Gf3cvXPJzOOYlfhzclS+ALz0bbvt36NoxNUGLiAyhZGGclsxO8/TeMTYJtx4Nb/8fsBC+fi5sunVqgxMRkWnlzKPncN/T++jOTGDQshmseB687pvwgUfgFV+EhvnwP5+Czz0Lvv0a+Mt3oG/fpMctIlKiZGGclrTUjT1ZAD893lt/AU2LfZekW/8Fcn1TF6CIiEwb566aS77ouOnBw5zZtW42nHapr0/+7l446+9g96Nww9/CZ1fCdy+GB34AmWHWdRAROQxKFsZpyew0W/f1ky8Ux37SghPh0p/DiW+A3/8H/Odz1MogIjIDnL5iNsfOb+CaOzbjJmu8QevR8JJPwHsfgL/+H3j222Hb/fCTt8O/HwPXvBLab9L4BhGZFEoWxmnp7DoKRce2/WMYt1Cubja8+itwyc8giMF3XgM3/aO+BRIRqWFmxlvOXM76rZ3c+9QkL7pmBotPg/P+P/j79fDWX8Kpb4G9m/1sSp+a52fl2/PE5F5XRGYUJQvjdNTcBgA27eye2AuseD686w44/R1w59fg/53m+5wWx9FSISIiR4xXn9JGczrO529+dPJaF4YKAlh2Jlzw7/Duu/z4hhNeBw/9CK5YA19c4xOHne1Tc30RqVlKFsZp9YJGAB4ebVXOQ4kl4YL/65uPZy3zfU6/fi48+cdJilJERKaL+mSM9714Jbdv6uC6u56e+gvGkn58w6u/4sc3vPRTMP94ePin8J9nwLWvh1s+AbsemfpYROSIp2RhnBpTcZa11vHw1sNIFkoWn+YX4XnNN6BnF/zXefDDS2Hvk4f/2iIiMm285czlPG/lHD52w3ru2ryncheetcQPhr74WnjfQ/CCD8GO9fCHL8LXng/fehXc/DF4/Lca4yAiw1KyMAHHLWw6vJaFcmZw0ut9s/ELPgwbfwlfWusX4OmtYIUiIiJTJgyML158Cotmpfirb/yZH92zZeq6JI2kvhXO/Sd4/8N+jMPJF0PfHvjjl+FbF8HnjoPr3wlP/QmyPZWNTUSmrVi1AzgSHbewiV+u3053Jk9DcpKKMFEP537ED0773b/BnV+H+74Lp7wZzngHtCybnOuIiEhVzK5PcP3fnM27rr2HD/zwfv77wW28/6XHcvyi5soH07QQXvEFv53rgwd/CI/9Bjb8HO7/HsRSsOp8WHQqLDsLFp1S+RhFZFpQsjABxy1qwjnYuL2T05bNntwXb26DV14Bp18Of/iCHwR959d8EvG8D/jnRUTkiNRSn+Dav34OV93+BF+45RFefsXtnLJ0Fu954Uqeu3IO8bAKDf7xtK9jTn0L9O+HR2+Gp/4I638K66/3xyQaWDLrWHjqbN9daeVLYMULfOu4iNQ0JQsTcNyiJgDWb52CZKFkwQnw2m/Aiz8Bt38O7rnGz5q04gV+fu35x0/NdUVEZEqFgfH25x/FG9Yu4Qd3P803b3+Ct159F3Makrzi5IW87PgFrF3WQqwaiUOqGU58nb+d/+/Q2+HXBXrmHoLHboc7vgQ4+OOXINEAs4/yLRDNi6F1pZ8mPNnkWy5EpCYoWZiABU0pFjSl+PPje3jLmcun9mLNbfDy/4Cz3gN//io88H342gvglL+CM/8O5hwztdcXEZEp0VwX5+3PP4o3n7mM2x7ZxQ/v2cK1f36K//rDZlrq4rzoWfN56XHzed7KuaQTYeUDDAJomAdr1sGadTzZ3s7qFYshCH2ysHMDPHMv/O7/AkPGX9TN8YvH1c2Buaug7TR/zNzVEK/zSUU8Xfn3JCLjpmRhAsyM562cw68f3kGh6AiDCjTDtiyD8/7Vd0X6zafgL9f61obVL4ez3wuLn63mYBGRI1AqHvLS4xfw0uMX0J3Jc9sju/j1+u38av12fnTPFhKxgLXLWjj7mDmceXQrJ7U1V6fVASDp1xri+R8c3FfIwb6n/OJvvR2wexN074BtD8DuR+GRX4IrDHkh84lIXSssXgu5ft9K0Xq0H8PXuhJSTX7h0lnLIJao2FsUkQMpWZigF6yayw/v2cKdT+zhzKNbK3fh+la48PNwzkfgz1+Du74B7T+H+Sf47kkrzoFQP1YRkSNRQzLGBScu5IITF5LNF/nzE7v57cZd/GFTB//+q40ANCZjnHHUbM46eg7PXTmHlfMasGp+WRTG/Yf81qOHf75zG3Rtg2LBJw+53mjfdujcAu03+QHVD/6Qg1ooSmYfDfVzIIhD4wK/lkS6xScYYRxiaf98/RywEMKE7wqVqJ+yty0yU+hT5QS9aPV8GpIxfnzvlsomCyUN8+BFH4Xn/j08+AP43b/Dd14LjYvgtEvguFfBvNWVj0tERCZFIhbwvJVzed7KuQB0dGf40+O7+cOm3dzxWAe3bNgJwKLmFOesnsfpy2dz2rIWlsyuq2bYB2taODiGYcmzRz4u1wf7t0CmEzoehf5O36LQtR22P+j35/pg618g3w+9u/39aJLNkJ7lz43XgQU+oUg1Q3o27H0CUrOiFo0Gn2SECZ+MxNP+Vj/XJzNhnFTHM/DUfv+aQcwnLMlGn6AEmo1ealPFkwUzWwJ8C1gAFIErnXNfNLOPA28HdkWH/pNz7qZKxzdW6UTIK05eyE/ufYaPnL+a1oZkdQJJNsDay+DkdfDor+Huq+C3n4Hf/ivMPxFOfTM865UabCYiR4SR6oghx5wD3AA8Ee36iXPuXyoYZlXMaUhy4UmLuPCkRQBs2dvL7Y928D/tO7nxvq18989PAXDMvAbOXTWXExfP4rRlLbTNOkLGBsTTMGel32477dDHF/LQsxNcEbK9vgtUzy4/W1O+Hzq3+haNvr2+haGnw3/o79/vb3s3+0Sgfx88+COfiBQyo15y+bB7zXcDTjX7mNItPqZUMzTO90lKwzyfhCTqIdPt9wPUz/PjNzAoZKFhvo+nfp4fG1LI+derm+2TkmLBXyuI+edFKqAaLQt54B+cc/eaWSNwj5ndHD33eefcZ6sQ04S87blH8b07n+Y/f/sYH73wuOoGE0/DcRf5W9d2ePgGuO9a+O9/9LdjXgzHnjc4a4WIyPQ0bB3hnHt4yHG/d85dWIX4po3FLXVcfPpSLj59KYWiY+P2Lv74+G5+076Ta+54kmzB51Jts9KctqyFU5fO4rRls1m9sLE6U7ROtjAGTYvKdhx7+K/pHBTzPtnI9fsuU/u3+DEXxQJPP7WZJYsXQ98+/1z3Tsh2+w/vfXt9l6jePf65fL9PXnJb4Mk7/Af9bJdPGg7VIjIWQcx3v4qn/H0s6b9AzPaUJSULfJIRxn0iE0/5VpD6OYMtKL17/GQqzvnXtABwPnHJZ/y5FvgEaP4JUMyR3LMZtmUHj03P9u852eSvbYFP1Brm+XKsm+2Tm/79/hjwr5frGxwHI9NWxZMF59w2YFu03WVmG4AjcvGAY+Y1sO70JVx9x2ZefUobJ7RVYWGd4TQu8Au5nfEO2LHez5P94A/hpg/4xGH+8b7Voe1U4iyD4rFqPhWRaWGUOmJosiBlwsA4blETxy1q4m3PXUE2X+SRHV3c+cQe7nlyL39+Yjc33r8VgHQ85LhFTSxvrefEtibWLp/N8jn11CfC6o59mA7M/IfjMO4/ZMMBi6L25Nph5WF08S2t2p3r8wlI906fUFjok5+u7f4De67PHxdL+SSkb4//8B8EUUJT8AlHvt8fW7rPdMKspb6lpW6Of72+ff75MB61nuT8h/ZCxrdmxFLjTl5WjPd9W+i7eWX2++sVsj5ZAD/IHfMxxNM+xqY2/7PI90OY9PuKUatNqtknaBb4zzv9nT4JSs/2x5n5gfHpFn+dREM0xmWWP6eQh+7t0HqML+tEgy/jXK+frauQ8a+T6/OtQrneKFly/rVjSd91rW8Pc59+FPad4M/JdEHzEsD59xZL+Z9tqtmP0WmY51u7EvWDrUJdO3ziBlGrWNH/bFLN0c/EfPKXboF8X5Sclf5Gnb+Gc741Koj534vjXw2rzhvvT2hUVR2zYGbLgVOAPwNnA+82s7cAd+O/WdpbxfDG5MPnPYubH97J33//Pn7wjjNpqZ9mMzbMP97fzv3fsPsxeOhHsOUueOxWuP+7HA1waxMsPBmOegEsXAMLThpsIhURqZIhdcRQZ5rZ/cBW4APOufWVjG26S8QCTmhr5oS2Zi577gqcc2zd38+9T+7lnif38vDWTm57dBc/vnfLwDlts9Kc2NbMMfMaOHpePcfMbeSoufXUJzW8cdKUkrFENK4k2TjywPBKKCUv/fujD9JRa4GZ/8xQ6lKF+Q/NHY9AvI4tW55mcdsi/wG+WIhaDBr9fak7V7IZcj0Qr/ezY2U6/TG5vmjgefSaXdt8y0jpS8t4HXQ+41832RglEVFrRd9e/8G4YT4Uc74rWV2rv+6eJ/y52W5/v3+Lfx99+/wH6f79/pxiwfeweOjHZQVh/gN8MT+u4puNQfsIg/KHU2qhKQkTvszBJy2Yb2np2+dnA3NF/3ymy3dNKyUipZgLGSgWoa7FJ0FBCIvWjOs9jCls58bxJifzwmYNwO+ATzvnfmJm84EO/FQInwQWOucuG+a8y4HLAdra2k675ZZbJnT9jo4O5syZM9HwD3Dftl4+evN2lrXE+dSLFzIrfWT8Y03sf5zC5j8yJ7eFup33kuzcPPBcPtVKf8ux9LesIjPrWIrxerINbeQa2qLMtrZM5u/DkUzlUHtl8KxnPese59zaascxXkPriCHPNQFF51y3mV0AfNE5t3KY15h29cV04pxjV0+e9Tv72d6V59HdGZ7cl2VbV45i2UeDefUxjmlN0hTmaKyvY2FjnAWNMeY3xJhbHycRzpzWiFr9XRivI7YcnPMtOkEMy/fjghhBrodivB4r5on1dVCMpXGxJMUwRZjtohhLE+T7sEIGzCjG6gmznRSSTeza08mCenBhnGKsjiDb7RMUM4JsN4VkM2G2i0KigSDXR65hERa1qpjLU4w3Yvk+gmKWQnLW8DEXC1ghg4tP/cQFI9UXVUkWzCwO/Bz4lXPuc8M8vxz4uXPuhNFeZ+3ate7uu++eUAzt7e2sXj15swX9ZuNO3vWde5jXmOLL/+tUTlw8TbokHcIB5dC/3884se0B2P6Av9/VfuD82IlGmHsszFnl79Mtfg7s2St8s2EYr84bOUyT/ftwpFI51F4ZmNkRlywcqo4Y5vjNwFrnXMdIx0yn+mK6y+aLPLm7h8d2dbNpZzcbd3Sz/pn9bNvXS64I+bJMwgzmN6ZYMjvN4pY60omQpbPrSIQBR82tpzEVY3Z9ktn1CZKxgELRHdEtFTPtd2EkKgev1sphpPqiGrMhGfBNYEN5JWBmC6O+qgCvBh6qdGyH49xV87ju8jO5/Ft388ov386605fy/pccy5xqzZI0EalmWP5cfyvJ9cOuDb7P3O7HfDLRsREe/w3c/90Dz7cAmhb7/pItywbnwJ5/vJ9xon5uNMXczPkWSkTGZ6Q6YsgxC4AdzjlnZqcDAbC7gmHWtEQsYOX8RlbObzxgf3t7O8ceu4pn9vWxZW8fW/b2Rvd9PL23lzuf2ENfrsCenuyIr20Gx8xtoKUuQVM6RlM6jnN+Ybq5DQla6hPs7c1x3MJG5jamqE+G1CdidPbnaEzGaamP05CMaWyFSAVVI70/G3gz8KCZ3Rft+ydgnZmtwXdD2gy8owqxHZY1S2Zxyz+8gC/c/CjX/HEzP/3LM7z17OVc/ryjaa47Mr9xJ56CRaf47fIkAvygov59vp/gvif9Cp6l26ZbfN/CwpBKI5by/e4a5vo+g81LfN/Dutl+cFL5fWqWFpgTmXlGqiOWAjjnvgq8DniXmeWBPuBiV60+tTNMEBhLZtdFazkMv8bQ/t4cvbk8z+ztozdbYEdnP/t6c/TnCuQKRdq3d9HZn+OZff1s2NYFQCZfYHdPlrH8FOsTIfOaUqTjIelEyMLmFPWJGGFoxAJjXmOS+U0p4mFAKh4yqy5OoeiY05BkxZx6ErHa60orMpWqMRvS7QwO5S43bddUGI+mVJx/fsVx/NVzlvKFWx7ly795jGvueJLXntrGujOWsnpBU7VDnDypJn+btXT4553zg4/2PObnt+7e6efE7unwMzTseBge+bUf4T+SWUt9N6dYcnBKtgUn+VaLeDqat7rOJyBhzA+CSkVlHD9C5hYXkQGj1BHlx3wJ+FJlIpLxaq6L00ychc3j+x+cLxTp7M+TiAVs3N5Jd6ZAbyZPd8YPOu3sz5MrFNm+v5+O7gz9uQK92QIPPrOfTK5IvujIFYrs78uNeI0wMBpTMVKxkN5snsZUnOVz6kjFQprr4syu85OU5ApF6pIxGpIx0vGQukRIYyrOtm3dPFnYTnPaJyClWzIWsLS1jiBq8cgXHAtnpWpjilqZ8fS17RQ5em4D/2/dKbzrBUfz9d8/zvfufJpr/vgka5bM4uJnL+HCkxfRcAT32xwTMz+WYfYok6w552cu6N3t53ru2wO90TRxvXt8t6e+vf75vZv9vr98e2zXb1rsE4e6Vj8zQqLOJxOJOp94zFoSxRnQtHMPFNr9jAmpZj93d2muaAv8diylLlQiIlMkFgbMjmYUPG3Z7Am/Tk8mz56eLLlCkd5sgc4oedjVneHRHd109vtWDoA9PVn29ebY15ujfXsXe3uzFIqOVDykJ5M/YHzGoB1jez+B0dqQoDdboFh0LJyVpikVI1dw9OcKLJldR65QZFZdgsCgpS5BGBiJWEBDMsbu7ixHza3HDJa01FF0jnzBkYwH1CV8IlOXCKlPxkjFAxJhQGxIctKfK9CdyR9ZXaJl2qnxT6vVd9yiJj7/xjV89MLjuP4vz/D9u57iwz95kE/87GFeuHoe552wgBeunndED/g6LGZ+HEOyEVqWH/p456Ip2Lp9klHM++nYevf4uaNzPYNzSu953C8o07vHz6mc7fXTtGV7/DRkZQO3FwH86RDXDqKWiyCM5muO7hMN0ewHoe+2VUpI4qVb2icb8bpoAZzdfjB4og6CqHtaLOm3U00+WUk1D846Fcb91GhaC0NE5JDqk7HDqlOdc5gZzjmyhSJ92QI92QJd/TkefewJ2pYspT9bIAh8t6cwMDr782zf3+cn2wFCMzbv7mFXV4b6ZAwz2Lqvj67+PHUJozkdZ+s+36q+ZW8fhaJjb092oHUkX3TEQyNXGF/vukQYkE74lpCuft8qk4gFrF3WQhgYTak4Dp8M1SVC6hKDLSfxMMDhu3m1NiQJbPD7McOY25gkFQ/IFRxP7s1i27tY1lpHJl8kGQtIxoKBsSSlMpTaMEM/oVbe7PoEb3vuCi47ezl/eXofP7pnC79ev51fPLiNRCzg+SvncsGJC3jRs+bTnD5CxzdUgplfhKXx0IeOKp/1XaIwcEUee3QDRy9t84lE3z6fXBTzfj7jYmEwwcj1Rgva7Ivmlt7n97miPz7X7xOWbG+0qEsvvuo4DMlmv+pnw/zBRWHqWv31mhb5a4Yx3zWLaMGYZKNPboKYbxGJpSCW8PdB6M+pnzP4GtFCL3U7tkCqY3DhlyCElhX+3ix6zbjvOhav968pIlJDSh9yzYxkLCQZC5lVB5CGfUlWL22Z0usXi27gA/gzUULxzL4+4qERDwMy+SI9mTy92QI9mTw9mTyZfJFM3rek9GXz9GQLBAbdmTxd/Xn6sgXyRceWvX0EBv25In25Ar3ZPP254iEiGsnTB+2Jh0bREY0RSZAIAxpTccLAyBeLJGIBs9K+3ig6N5DA9OUKpOMhQWAUo0QpnYgNJCH+fEcxmk2rMRWjJ1OgpT4+kKQEZqTjIbu6+lnQnGJWXYLebJ66RIw5DQmfiOV9fdyQihGaUZ8MCaOErz4RozdXIJv3cSZjAbHAMDO6+nMkY+GMHe+iZKHCzIxTl7Zw6tIWPnnRCdy1eQ+/fGg7v3xoO7ds2EEsMM44ajbnrprHOavmcfTcemXnUyGW8AOsI7nGHpg/BdOfOTe4EmSux39ITzX7RWgKWX8rFqKkAp+Q9O+PFo+JFofp3eNbF/r2+daFYtGv9BjG/cI1Fvhju9oB51tVsj1+XyHvH5dPfzuKEUafjCyW8i0qQejjCGL+urGUn/2qdN14erCVJV7nj+3f5xOPIOaTnSBaNbWU5ITxaF9s8LhSohSvi8ar1PvXLLXCBOHg+Qfdhj4X+sQv2eBf3xUGF8wpLW4z3N9esTB4Pf1tisgkCwIjnfAr/PqB5IP3U6FYdPTlCuSjVoyuTI69PTmK0Whzh/9gv21fP0XnP8g/+fQztM5bwI7OfpIx39rQFw1gD80wg11dGfJFR1d/jkLREQsCMvkC+/tymBkG9OUKPLKjizkNSfpyBYi+D8sVHL1ZnwRl876lJRb4hKAvN7b6bLzMOGiAvZlvrcnkfULVUhenKR0fSNSy+QLzm7aSjLqB1SVCHH7QVTIWkowHdPXn2b6/n6Wz63hqTy9zG5MsbE6RToTEAiMWBgPvLVco0piKUygWcc6Psdnfl6MpHSebL5KKhyRjAZ39uahVKxho3YoFRhgaZ6yYfVjd+IajZKGKwsB4zlGtPOeoVv75wuO4b8s+frV+O7du2MmnfrGBT/1iA4tb0pyzai7PPWYupy1rYW6j+h0eUcx8d6PEkJlDKr1CdiHnu2SVEpRY0g84d86PCcGBczz15GaWLl3iPzA758/b/ejgh+NCzicDYcInQNku/+G5WPAftot5f2y+3yc5QehfJ9/vW1u6d/rEqJj3s14V81FSk4tW1owSnGJu8Fql+zEmPIfrgJQxlvJlFUv5RM8V/YqZpfG3qSa/3b/Pd0eLRX+fzvmENNfnkynwZRBL+eQmCP1r5fp8ecXT/nUyXb5VyBX9706Y9O+79Ri4+NqKvH8RmVmCwA7ottVcF2fxcI0nZd8mtcf2s3r14mEOmnqFovPdq8KArv6cb3FwfqB5X65AQzJGR3d2oFWhuz/Pnt4s8eiDedE5uvvzOKC7PzfQEtLVnyMZdcnKRklKtuBbbEoD2nd29dPZlx+Y0nfX7j1kAj+Qvy9XoC9bIBELMIvGi3TnSSdCjl/UxLb9/Rw7v4G+XIGN27vI5IsUio580ZEv+u1SMhQLooHyRUdzOs7+vhyJMBiIqT4RDsSdLxYPWETxH15yrJKFWhUEgy0OHzn/WTy9p5ffPbKL327cxU/ufYbv/OkpwsA4sa2ZZy9vYe3y2ZyydBbzGlPVDl2OBGH84AXz0gfXBr39rbBiaAvLS6curvEoFgHnZ9IqJRPZbv+B2zn/XDFKWAbu8yM8jm5BzLfiFHK+BcPBrl07mDt3nt9XyPgkK9/vE6RSiwT4pKZ/vz8v1eyToWJusKtYPuNbQFwUdxD3LT6lZCMIfTKAG+zelp7ljyklI4WsP26kGcdERGaYMLCB7tqlFpihFrdM/WrHUPlF2VyUFA0dyF4sOgrRc8EUtHgrWZimlsyu403PWcabnrOMTL7AQ8/s5zftu/jzE7u55o4n+frvnwBgcUuaU5a2cMqSWZy8pJlFs9Ljnq5O5IhQGuDd3Dall9nd3s7cGlqRU0REaoOZEQsPTgaCwAgw4sPnTodNycIRIBkLOW3ZYB+0/lyB9Vv385en9vGXp/Zx9+Y9/Oz+rQPHL5md5viFzRw7v4ET2po5blHTwAI1IiIiIiJjpWThCJSKH5g8AGzb38fDWzt5cncvf35iNxt3dPHrh7cP9GMLzLdWrJrfyLHzG1nckmbRrDT9+7IsyeRn7tStIiIiIjIifUKsEQubB7sfXfZcvwhaf67AA1v28/iubp7Z18dju7pp397FLRt2HDAYhp8+TXM6zqJZaRY1p3xXplkp2qIuTYtmpZjXmJqxU4aJiIiIzFRKFmpYKh5y+orZnL7iwFHxuUKRHZ39bNvfz90PP4ara2Hbvn627utj6/5+7n5yL/ujFS/LzaqLM6chydyGJHMaS/cJv68xSXPaz3fcnI7TUpegLhFq2lcRERGRI5iShRkoHgYsbqljcUsdjf07Wb36mIOO6cnk2ba/j61RErGzK8Ourgwd3f7+wS372NWVoSc78nSWiTCguS7OrHScxlSMhlSchmRIQzJGQzJOUzrGrHScovOJSEtdIlp5009JVp/0y9mn4oGSDhEREZEqULIgw6pPxjhmXiPHzBt9qeTebJ6Oriy7ujN09uUGFlzZ25tjX2+Ofb1Z9vXm6M7k2d+XY+u+PrqjJei7M/kxxRIYA8lDXZRs+MdhlFz4pKIuEZKKhxSKjsUtaZKx0CcpyRjJeEAqWiDFr8gZkChbnVFEREREDqZkQQ5LXSLG0tYYS1vHP6dxvlAcWMlxX2+W/X05ejIFuqPl63uzeboz0XL22Xy0rL1/vjeb55l9ubLjJrZkfWD4pMEgndziFz0pFEnFAxqTfon6RCzwrRv4KctKyUoQGIFBOh7SkIqRjIUE5ueAjofBYEISBgOLwcRCIx5E96ERG9j2y9knwoD6ZIx0PCQe+lUZh0tmnHNKckRERGTKKVmQqomFAa0NfsXb2fWJw369fKFIf7QiYkd3hkyuSFd/jp6sTyQy+QKZXJH+XGFgCflMtBri9p0d1Dc1k8kViYd+SfruTJ580ZErFOnPFXHOr7T45O5eerN5is4vhNKfK4zaHetwmPluY4mwLKkwo6M7w9FzG0jGg4Hl4n1y4ROTMPCJTWkp+MAGl4IvPS4lQs3pOLl8kVyhyK6OPSx85lESMX/NeCwgGQYDK1I6Bw3JGLEo0fGJkX+twIyCcxSLjll1CWKB4YDQjCDwSZTfLrsf2MbHFO0XERGR6UHJgtSMWBjQEK0lUVrdcawOdxXGfKE4sOR8vujIlRKR6EN4vujIFxy5YpF8wZEvFMkVo/uCX649X3BkC0V6olaSXKFIvlAkW/AJSy46Nhe1fGzf3z/4uoUimVyRXLFAvuATptItf9B2FGvRXy9XcEPezd4Jl8NkKU8iDk4s/L1vdRkpCeHgfeVJyTCv5behq6uT5gf6AXyCFSVcYSkJi5Kgfb1ZmtNxErGAwAYTsmQ8IB4GFJ1vASo6N9CCZADmW6RKyRHRdmA+OQzMtyYZg/ub03HOOmZOFX8iIiIyUylZEJkEvotRtaMYP5+QuKilwGhvb+eYY1eRLUt0Sq0vpa5PPVGLSz5KckpJSLHooq5Zxp6eDM75D7+F4oFL0RfdYPLit6EYtUgUyu5L+w88tvx+pNfloGNLt2xhpBj86xWKjkw2S3xPAecYSP4KUWJXdJAvFnEOmtJxOvtyA0miG5pzTaLVCxr55fueP3UXEBERGYGSBZEZLB4GBywPb2bRvoD6ZPXiqqaJtjKVuqll8kVy+aJvFQh8mZZanpwDhxtIRArFwSSj6FxZa8TgfdE5rb4uIiJVo2RBRGQS+ETLJ1vM0ERLRERqz7T7usrMzjOzjWa2ycw+XO14RESksg5VD5h3RfT8A2Z2ajXiFBGZCaZVsmBmIfBl4HzgOGCdmR1X3ahERKRSxlgPnA+sjG6XA1+paJAiIjPItEoWgNOBTc65x51zWeA64KIqxyQiIpUzlnrgIuBbzvsTMMvMFlY6UBGRmWC6jVloA54ue7wFOKP8ADO7HP9NEkC3mW2c4LXmAB0TPLeWqBw8lYOncqi9MlhW7QDG6ZD1wAjHtAHbyg9SfTHpVA4qgxKVg1dr5TBsfTHdkoXhVmM6YEJC59yVwJWHfSGzu51zaw/3dY50KgdP5eCpHFQG08Ah64ExHqP6YpKpHFQGJSoHb6aUw3TrhrQFWFL2eDGwtUqxiIhI5Y2lHlBdISJSIdMtWbgLWGlmK8wsAVwM3FjlmEREpHLGUg/cCLwlmhXpOcB+59y2oS8kIiKHb1p1Q3LO5c3s3cCvgBC4yjm3fooud9hN0zVC5eCpHDyVg8qgqkaqB8zsndHzXwVuAi4ANgG9wFunOCz9TngqB5VBicrBmxHlYM4d1M1TRERERERk2nVDEhERERGRaULJgoiIiIiIDGtGJgtmdp6ZbTSzTWb24WrHM5XM7Coz22lmD5Xtm21mN5vZo9F9S9lzH4nKZaOZvaw6UU8uM1tiZr8xsw1mtt7M3hvtn2nlkDKzO83s/qgcPhHtn1HlAH6VYDP7i5n9PHo848pADk11xcz7u1B9obpiKNUXMzBZMLMQ+DJwPnAcsM7MjqtuVFPqauC8Ifs+DNzqnFsJ3Bo9JiqHi4Hjo3P+MyqvI10e+Afn3LOA5wB/G73XmVYOGeCFzrmTgTXAedFMMjOtHADeC2woezwTy0BGoboCmJl/F6ovVFcMNePrixmXLACnA5ucc48757LAdcBFVY5pyjjnbgP2DNl9EXBNtH0N8Kqy/dc55zLOuSfwM42cXok4p5Jzbptz7t5ouwv/R9/GzCsH55zrjh7Go5tjhpWDmS0GXg58o2z3jCoDGRPVFTPw70L1heqKcqovvJmYLLQBT5c93hLtm0nml+Ykj+7nRftrvmzMbDlwCvBnZmA5RM2p9wE7gZudczOxHL4A/CNQLNs308pADk0/+xn+dzGT6wvVFQO+gOqLGZks2DD7NH+sV9NlY2YNwI+B9znnOkc7dJh9NVEOzrmCc24NfsXb083shFEOr7lyMLMLgZ3OuXvGesow+47oMpAx089+ZDVfNjO9vpjpdQWovig3E5OFLcCSsseLga1ViqVadpjZQoDofme0v2bLxszi+H/81zrnfhLtnnHlUOKc2wf8Ft+vciaVw9nAK81sM75byQvN7DvMrDKQsdHPfob+Xai+GDSD6wpQfTFgJiYLdwErzWyFmSXwg1FurHJMlXYjcEm0fQlwQ9n+i80saWYrgJXAnVWIb1KZmQHfBDY45z5X9tRMK4e5ZjYr2k4DLwbamUHl4Jz7iHNusXNuOf5v/3+cc29iBpWBjJnqihn4d6H6QnVFieqLQbFqB1Bpzrm8mb0b+BUQAlc559ZXOawpY2bfA84B5pjZFuBjwGeAH5jZ24CngNcDOOfWm9kPgIfxM0L8rXOuUJXAJ9fZwJuBB6M+mAD/xMwrh4XANdHsDAHwA+fcz83sj8yschjOTPtdkENQXTEj6wpQfQGqKw5lJv0uAGDO1UR3KhERERERmWQzsRuSiIiIiIiMgZIFEREREREZlpIFEREREREZlpIFEREREREZlpIFEREREREZlpIFkWGYWcHM7iu7fXgSX3u5mT00Wa8nIiLVo/pCat2MW2dBZIz6oqXuRURERqP6QmqaWhZExsHMNpvZv5nZndHtmGj/MjO71cweiO6XRvvnm9n1ZnZ/dDsreqnQzL5uZuvN7NfRKpkiIlIjVF9IrVCyIDK89JBm5TeWPdfpnDsd+BLwhWjfl4BvOedOAq4Froj2XwH8zjl3MnAqUFoBdiXwZefc8cA+4LVT+m5ERGSqqL6QmqYVnEWGYWbdzrmGYfZvBl7onHvczOLAdudcq5l1AAudc7lo/zbn3Bwz2wUsds5lyl5jOXCzc25l9PhDQNw596kKvDUREZlEqi+k1qllQWT83AjbIx0znEzZdgGNHxIRqUWqL+SIp2RBZPzeWHb/x2j7DuDiaPuvgNuj7VuBdwGYWWhmTZUKUkREqk71hRzxlJ2KDC9tZveVPf6lc640HV7SzP6MT7bXRfveA1xlZh8EdgFvjfa/F7jSzN6G/0boXcC2qQ5eREQqRvWF1DSNWRAZh6gP6lrnXEe1YxERkelL9YXUCnVDEhERERGRYallQUREREREhqWWBRERERERGZaSBRERERERGZaSBRERERERGZaSBRERERERGZaSBRERERERGdb/D9baH0aBe8cvAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 936x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s 2ms/step - loss: 518.6037 - mae: 21.1206 - mse: 518.6037\n",
      "MAE with the adagrad optimizer: 21.1206  reached in 41 s after 500 epochs\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 936x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "for optimizer in ['rmsprop', 'adam', 'adagrad']:\n",
    "    start_time = time.time()\n",
    "    model = initialize_model()\n",
    "    model = compile_model(model, optimizer)\n",
    "\n",
    "    es = EarlyStopping(patience=10, restore_best_weights=True)\n",
    "\n",
    "    history = model.fit(X_train, y_train, \n",
    "                      batch_size=16, \n",
    "                      epochs=500, \n",
    "                      validation_split=0.3,\n",
    "                      callbacks=[es], verbose=0)\n",
    "\n",
    "    res = model.evaluate(X_test, y_test)[1]\n",
    "    end_time = time.time()\n",
    "\n",
    "    print(f'MAE with the {optimizer} optimizer: {res:.4f}  reached in {(end_time - start_time):.0f} s after {len(history.epoch)} epochs')\n",
    "    plot_loss_mae(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ **Question** ❓ Now, reproduce the same plots and results but for different learning rates.\n",
    "\n",
    "*Remark*: There is a chance that the y-axis is too large for you to visualize the results. In that case, rewrite the plot function to plot only the epochs > 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s 2ms/step - loss: 20.7376 - mae: 2.9701 - mse: 20.7376\n",
      "MAE with the rmsprop optimizer: 2.9701  reached in 17 s after 182 epochs\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 936x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s 2ms/step - loss: 26.1763 - mae: 3.0703 - mse: 26.1763\n",
      "MAE with the adam optimizer: 3.0703  reached in 27 s after 420 epochs\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 936x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s 2ms/step - loss: 423.4464 - mae: 18.3917 - mse: 423.4464\n",
      "MAE with the adagrad optimizer: 18.3917  reached in 29 s after 500 epochs\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 936x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "for optimizer in ['rmsprop', 'adam', 'adagrad']:\n",
    "    start_time = time.time()\n",
    "    model = initialize_model()\n",
    "    model = compile_model(model, optimizer)\n",
    "\n",
    "    es = EarlyStopping(patience=10, restore_best_weights=True)\n",
    "\n",
    "    history = model.fit(X_train, y_train, \n",
    "                      batch_size=16, \n",
    "                      epochs=500, \n",
    "                      validation_split=0.3,\n",
    "                      callbacks=[es], verbose=0)\n",
    "\n",
    "    res = model.evaluate(X_test, y_test)[1]\n",
    "    end_time = time.time()\n",
    "\n",
    "    print(f'MAE with the {optimizer} optimizer: {res:.4f}  reached in {(end_time - start_time):.0f} s after {len(history.epoch)} epochs')\n",
    "    plot_loss_mae(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s 3ms/step - loss: 30.0026 - mae: 4.2622 - mse: 30.0026\n",
      "MAE with the 0.0001 learning rate: 4.2622 reached in 29 s after 500 epochs\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 936x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s 1ms/step - loss: 19.9910 - mae: 2.9553 - mse: 19.9910\n",
      "MAE with the 0.001 learning rate: 2.9553 reached in 17 s after 256 epochs\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAwsAAAEWCAYAAAApcQvWAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/d3fzzAAAACXBIWXMAAAsTAAALEwEAmpwYAABeJElEQVR4nO3deZxcZZn3/89Ve/W+ZO/sC4SwQ0DZFBQVcMEFhTw6gjgyOjrqz9FxmUVGx2f0GXVGRkcFZVBHxRVFh1GWAUVkx7AEmpCEQPakk/S+1XL//rhPJ5VOdae707V01/f9etWrzjl1TtV1dyV111X3Zs45REREREREhguVOgARERERESlPShZERERERCQvJQsiIiIiIpKXkgUREREREclLyYKIiIiIiOSlZEFERERERPJSsiAyRma22MycmUXGcO5VZvaHo30eERERkVJSsiDTkpltNrNBM5sx7Pja4Iv64hKFJiIi09B46h0zuzY4duawc68ys4yZdQ+7zStSMUQOo2RBprPngTVDO2Z2IpAsXTgiIjLNHbHeMTMD/gzYB1yZ5znud87VDLttL2TQIqNRsiDT2feAd+bsXwl8N/cEM6s3s++a2R4ze8HM/s7MQsFjYTP7opm1mdkm4LV5rv22me0ws21m9k9mFh5vkGY2z8xuNbN9ZrbBzN6T89iZZvaImXWa2S4z+3JwPGFm/2Vme82s3cweNrPZ431tERGZVEesd4DzgHnAh4ArzCxWpNhEJkTJgkxnDwB1ZnZc8CX+cuC/hp3z70A9sBR4Of5D/l3BY+8BXgecCqwGLht27XeANLA8OOfVwJ9PIM4fAlvxlcdlwP81s1cGj30F+Ipzrg5YBvw4OH5lEPcCoBl4L9A3gdcWEZHJM5Z650rgV8CPgv3XFTE+kXFTsiDT3dCvPK8CWoFtQw/kfJB/0jnX5ZzbDHwJ3zwM8Dbg35xzW5xz+4B/zrl2NnAx8GHnXI9zbjfwr8AV4wnOzBYA5wIfd871O+fWAt/KiSEFLDezGc65bufcAznHm4HlzrmMc+5R51zneF5bREQKYrR6pwp4K/AD51wK+CmHd0V6adBiPHTbWKS4RfLSbCwy3X0P+D2whMObgmcAMeCFnGMvAC3B9jxgy7DHhiwCosAO3/0U8Ml37vljMQ/Y55zrGvY6q4PtdwOfAVrN7HngH51zvw7KtQC42cwa8L9c/W1Q+YiISOmMVu+8Cd8ifVuw/33gTjOb6ZzbExx7wDl3blEiFRkDtSzItOacewE/4OwS4OfDHm7D/0K/KOfYQg7+CrQD/4U897EhW4ABYIZzriG41Tnnjh9niNuBJjOrzReDc+4559waYBbwBeCnZlbtnEs55/7RObcKOBvfjP1ORESkpI5Q71wJ1AAvmtlO4Cf4H57WIFKmlCxIJXg38ArnXE/uQedcBj8G4HNmVmtmi4CPcLB/6Y+BD5rZfDNrBD6Rc+0O4HbgS2ZWZ2YhM1tmZi8fT2DOuS3AH4F/DgYtnxTE+30AM3tH8ItTFmgPLsuY2QVmdmLQlaoTn/RkxvPaIiJSMPnqnRbglfgfd04JbifjfwjKNyuSSFlQsiDTnnNuo3PukREe/iugB9gE/AH4AXBj8NgNwG+Bx4HHOPwXonfiuzE9DezH9z2dO4EQ1wCL8a0MtwCfds7dETx2EbDOzLrxg52vcM71A3OC1+sEngF+x+GD6EREpARGqHfOA9Y65253zu0cugHXASeZ2QnBeWflWWfhjKIWQCSHOedKHYOIiIiIiJQhtSyIiIiIiEheBUsWzGyBmd1tZs+Y2Toz+1BwvMnM7jCz54L7xpxrPhksSvWsmb2mULGJiEjpTKR+GHb9RUE9scHMPpHvHBERmRwF64ZkZnOBuc65x4KZXh4F3ghchZ8q8vPBh3yjc+7jZrYKvzjVmfjpJO8EjgkGoYqIyDQx3vph2LVhYD1+DvutwMPAGufc00UsgohIxShYy4Jzbodz7rFguws/CLMFuBS/8i3B/RuD7UuBm51zA86554EN+MRBRESmkQnUD7nOBDY45zY55waBm4PrRESkAIqyKJuZLQZOBR4EZgfTTuKc22Fms4LTWvDLpA/ZysHFsXKf6xrgGoCqqqrTlyxZMqGYMpkM4XB4QtceSVtvmva+DKuiO8DCDNYuOPJFBVTIspYblXX6qZRyQmHKum7dujbn3MxJfdJJNMb6IVcLhy5+uBV4yQjPXZL6YltninTWsSzRRaR3N/2Nx4BNjX/D+v82/VRKOUFlPVoj1RcFTxbMrAb4GfBh51xnzmq3h52a59hhfaScc9cD1wOsXr3aPfLISDNijq61tZWVK1dO6Noj+bc71/Nvdz7Ho8f+B4aDd9125IsKqJBlLTcq6/RTKeWEwpTVzF448lmlMY764ZDL8hzL25+2VPXFdXc9x7/euZ773g7VP/0/cPVNsDBvPlN29P9t+qmUcoLKerRGqi8KOhuSmUXxFcH3nXNDc9TvCvqrDvVb3R0c38qhq+XOx887P+XEIz7Ty0YSkOorcTQiIuVnnPVDrrKvK1YvasQ5eGJgjj+wp7W0AYmIHIVCzoZkwLeBZ5xzX8556FYOrlR4JfDLnONXmFnczJYAK4CHChVfISWi/s+aCcch3V/iaEREyssE6odcDwMrzGyJmcWAK4LrysYpCxsIh4z79lRBtErJgohMaYXshnQO8GfAk2a2Njj2KeDzwI/N7N3Ai8BbAZxz68zsx/jVcNPA+6fqTEhDLQuZkFoWRETyGFf9YGbzgG855y5xzqXN7AP41dXDwI3OuXXFLsBoqmIRVs2t45EX22HGMUoWRGRKK1iy4Jz7A/n7lgK8coRrPgd8rlAxFctQy0I6pJYFkXxSqRRbt26lv//I/z9SqRTPPPNMEaIqvaMpayKRYP78+USj0UmOavKNt35wzm0HLsnZvw0o7WCwIzhtYQM/fXQr7pRjsc1/KHU4IlOW6ov8illfFGU2pEoz1LKQDsch1VviaETKz9atW6mtrWXx4sUcaVBrX18fyWSySJGV1kTL6pxj7969bN26lYnO+COT6/iWer5z/wvsq1pKc+ePoL8DEvWlDktkylF9kV8x64uCDnCuVEMtC6lQFQwqWRAZrr+/n+bm5iN+8MvYmBnNzc1j+uVNiuOEeT4x2OiCGcDbnithNCJTl+qLyTWR+kLJQgEMtSwMhhKQTUF6sMQRiZQfffBPLv09y8uK2TXEwiEe72nyB/ZvLmk8IlOZPt8m13j/nkoWCiAetCwMhILmoVRPCaMREZFii4ZDrJxbyx/3VfsD+54vbUAiIhOkZKEAEkHLwoAl/AF1RRIpK3v37uWUU07hlFNOYc6cObS0tBzYHxwcvSXwkUce4YMf/GCRIpWp7Ph59Ty2YxBXM0ctCyJTlOoLDXAuiKGWhX4LWhYG1bIgUk6am5tZu3YtANdeey01NTV89KMfPfB4Op0mEsn/8bh69WpWr15djDBlils1r44fPvQig3MXEd+vlgWRqUj1hVoWCmKoZaGfuD+gbkgiZe+qq67iIx/5CBdccAEf//jHeeihhzj77LM59dRTOfvss3n22WcBuOeee3jd614H+Irj6quv5vzzz2fp0qVcd911pSyClJljZ9cCsC82Ty0LItNIpdUXalkogKGWhV61LIgc0T/+ah1Pb+8c8fFsNksoNL7fNVbNq+PTrz9+3LGsX7+eO++8k3A4TGdnJ7///e+JRCLceeedfOpTn+JnP/vZYde0trZy991309XVxbHHHsv73ve+KbHWgRTeMbNrANjKbOZ2bodUP0QTJY5KZOpSfVEaShYKYKhlodfF/AGNWRCZEt761rcSDvv/vx0dHVx55ZU899xzmBmpVCrvNa997WuJx+PE43FmzZrFrl27mD9/fjHDljLVUBVjdl2c9YPNnIGD9hdg5rGlDktEJkEl1RdKFgrgQMsCQy0L3SWMRqS8HekXnWIuslNdXX1g++///u+54IILuOWWW9i8eTPnn39+3mvi8fiB7XA4TDqdLnSYMoUcM7uWP3U08nbwXZGULIhMmOqL0tCYhQKIhf2ftWeoZUGrOItMOR0dHbS0+AW1brrpptIGI1PWsbNreWC/747E/hdKG4yIFMR0ry+ULBRAKGTEIiG6skNTp2rMgshU8zd/8zd88pOf5JxzziGTyZQ6HJmijplTy7ZUDdlwHDpeLHU4IlIA072+UDekAolHQnS7YNCKkgWRsnXttdfmPX7WWWexfv36A/uf/exnATj//PMPNDEPv/app54qRIgyhS2fVYMjRF9yLtUdW0sdjogchUqtL9SyUCCJaJiedAQsrGRBRKRCLZvhuyDtj86C9i0ljkZEZPyULBRIPBJiIJOFWLXGLIiIVKj6qigzamLscDOhQ8mCiEw9ShYKJBENM5AOkgXNhiQiUrGWzqhhU7oRundBeqDU4YiIjIuShQKJR0IMpDIQrdI6CyIiFWzZrGqe7qn3Oxq3ICJTjJKFAolHQjktCxqzICJSqZbOqGH9QIPfUVckEZliCpYsmNmNZrbbzJ7KOfYjM1sb3Dab2drg+GIz68t57BuFiqtYEtEwfYOZYMyCkgURkVzjqSPyXLvZzJ4MznukaEFP0NKZ1Wx1M/yOBjmLyBRTyJaFm4CLcg845y53zp3inDsF+Bnw85yHNw495px7bwHjKopkNEx/OqOWBZEydf755/Pb3/72kGP/9m//xl/+5V+OeP4jj/jvpZdccgnt7e2HnXPttdfyxS9+cdTX/cUvfsHTTz99YP8f/uEfuPPOO8cZ/bRwE+OrI4a7IDh3deFCnBxLZ9aw0zXjMLUsiEwxqisKmCw4534P7Mv3mJkZ8Dbgh4V6/VJLRMP0p7IasyBSptasWcPNN998yLGbb76ZNWvWHPHa2267jYaGhgm97vAK4DOf+QwXXnjhhJ5rKqukOqKlIUnGIvREm6Bze6nDEZFxUF1RujEL5wG7nHPP5RxbYmZ/MrPfmdl5JYpr0hzshlSjlgWRMnTZZZfx61//moEBPzvN5s2b2b59Oz/4wQ9YvXo1xx9/PJ/+9KfzXrt48WLa2toA+NznPsexxx7LhRdeyLPPPnvgnBtuuIEzzjiDk08+mbe85S309vbyxz/+kVtvvZWPfexjnHLKKWzcuJGrrrqKn/70pwDcfffdnHrqqZx44olcffXVB2JbvHgxn/70pznttNM48cQTaW1tLeSfphzkqyNyOeB2M3vUzK4pYlwTEouEmFufZF+4Gbp2lDocERmHcqwr7rrrLl760pcWra4o1QrOazj0F6MdwELn3F4zOx34hZkd75zrHH5hUDFcA9DS0jLhP0RbW1tBK9yBnk66+wfY3zNAbX8nG0pYuRe6rOVEZZ0aUqkUfX19AETv/Hts98grWUadI2M2rud3s04gdeFnRz2nqqqK008/nV/+8pe8/vWv53vf+x5vectb+OhHP0pTUxOZTIZLLrmE1772tZx44olks1n6+/vp6+vDOUdfXx/33XcfP/zhD/njH/9IOp3m7LPP5qSTTqKvr4+LL76Yd7zjHYBvcv7GN77B+973Pl772tdy8cUX86Y3vQmATCbD4OAg+/fv55prruG2225jxYoV/Pmf/znXXXcdH/jAB3DOUV9fz3333cc3v/lNPv/5z/P1r3897991qv6bGGZ4HTHcOc657WY2C7jDzFqDlopDlFN9MSPh2NZdz6w9m9lcxu/RVP5cGa9KKetUL2ep64tyrCuuvPJKbr31Vo477rgJ1RVDf9ex/rsoerJgZhHgzcDpQ8eccwPAQLD9qJltBI4BDhu45py7HrgeYPXq1W7lypUTiqO1tZWJXjsWczZkST/fQ+Ps+bC5v6CvdSSFLms5UVmnhmeeeYZkMul3IhEIhUc8N5PNEB7l8bwiESJDzz+Kd7zjHdxyyy287W1v42c/+xk33ngjv/rVr7j++utJp9Ps2LGDTZs2ceaZZxIKhUgkEiSTScyMZDLJww8/zJvf/Gaam5sBuPTSS4lGoySTSTZu3MiaNWtob2+nu7ub17zmNSSTScLhMLFY7ED5h/ZffPFFFi9ezEknnQTA1Vdfzde+9jU+9rGPYWZcfvnlJJNJzjrrLH79618f/PvliEajU/bfxJB8dcRwzrntwf1uM7sFOBM4LFkop/pi5VODbHu6ibMGN5T1ezSVP1fGq1LKOtXLWQ71RbnVFUuXLuW4444jmUxOqK6A8dUXpWhZuBBodc4dmGzazGYC+5xzGTNbCqwANpUgtkmTjIXpS2Vw0Sos3Q/ZzKj/wEUq1sWfH/Xhwb6+ET/sjtYb3/hGPvKRj/DYY4/R19dHY2MjX/ziF3n44YdpbGzkqquuor+/f9TnsBF+xbrqqqv4xS9+wcknn8xNN93EPffcM+rzOOdGfTwejwO+wkin06OeO8UdVkfkMrNqIOSc6wq2Xw18ppgBTsTC5iqeH6iH7H5I9UG0MP+mRaa1EtUXlV5XFHLq1B8C9wPHmtlWM3t38NAVHN68/DLgCTN7HPgp8F7nXN6Bb1NFIhom6yATqfYHNG5BpOzU1NRw/vnnc/XVV7NmzRo6Ozuprq6mvr6eXbt28T//8z+jXv+yl72MW265hb6+Prq6uvjVr3514LGuri7mzp1LKpXi+9///oHjtbW1dHV1HfZcK1eu5IUXXmDDhg0AfO973+PlL3/5JJW0/IynjjCzeWZ2W7A7G/hDUF88BPy3c+43xYp7ohY0VbGbBr+jcQsiU0q51RWbN29m48aNQHHqioK1LDjn8g4Td85dlefYz/DT5E0biahvRUiFE/6PPNgDibqSxiQih1uzZg1vfvObufnmm1m5ciWnnnoqxx9/PEuXLuWcc84Z9drTTjuNyy+/nFNOOYVFixZx3nkH52b47Gc/y0te8hIWLVrEiSeeeOBD/4orruA973kP11133YHBagCJRIJvfvObvPWtbyWdTnPGGWfw3vdO+VmkRzTOOmI7cEmwvQk4uaDBFcCCxiQ7XZPf6doJTUtLG5CIjEs51RX/+Z//ydvf/nay2WxR6go7UnNGOVu9erUbmst2vArdh+/7D77A397yFI+/uYP6294HH3gEZqwo2OuNZqr3VxwPlXVqeOaZZzjuuOPGdG5fAbshlZujLWu+v6uZPToV1iIotFLXF3u7B7jiczdxR/xv4C3fhhMvO6rnK5Sp/LkyXpVS1qleTtUX+RWzvijV1KnTXjJoWRgIBd2QBg6b2ElERCpEU3WMrmiwirO6IYnIFKJkoUCGkoX+8FCycHi/MxERqQxmRm19MwOWgE4lCyIydShZKJChMQu9VuUPKFkQOcRU7gJZjvT3LH/zGqtosybo0irOIuOhz7fJNd6/p5KFAjmQLBD0J1OyIHJAIpFg7969qgAmiXOOvXv3kkgkSh2KjKKlMcn2bKNaFkTGQfXF5JpIfVGqFZynvUTU52G9oaFkobuE0YiUl/nz57N161b27NlzxHNTqRTRaLQIUZXe0ZQ1kUgwf/78SY5IJlNLQ5JtmXpO79qmX+pExkj1RX7FrC+ULBRIMuZbFnqyQ8mCBjiLDIlGoyxZsmRM5071mTzGo5LKWonmNST89Kldj4JzMMIiTSJykOqL/IpZVv24USCJSNANyYUhHFc3JBGRCjevPsku10goMwC9U3rdURGpIEoWCmSoZaFvMAvxWiULIiIVruWQhdk0yFlEpgYlCwUy1LLQn8pAvEbJgohIhZtdl2A3jX5Hg5xFZIpQslAgiZj/0/alMmpZEBERouEQmZq5fkctCyIyRShZKJBYOETIhloW6mBQsyGJiFS6WP0cv9G1s7SBiIiMkZKFAjEzEtFwkCzUajYkERFhRkMt+6weOtWyICJTg5KFAkpGw+qGJCIiB8yuS7Az2wRdGrMgIlODkoUC8i0LWYhpgLOIiPhkYXu2gUyHWhZEZGpQslBAiWhILQsiInLAnLoEu5xaFkRk6lCyUECJaJiBoQHO6X7IpEodkoiIlNCsujh7qCfUtw8y6VKHIyJyREoWCuiQMQug1gURkQo3py5Bm6vHcNDbVupwRESOSMlCASVjYfoGlSyIiIg3uy7BHtfgd7p3lTQWEZGxKFiyYGY3mtluM3sq59i1ZrbNzNYGt0tyHvukmW0ws2fN7DWFiquY4pFggLOSBRGRQ4y3jhh27UVBXbHBzD5RvKiPXnU8Qk+02e907y5tMCIiY1DIloWbgIvyHP9X59wpwe02ADNbBVwBHB9c8x9mFi5gbEWRjA2ts1DjDyhZEBEZchNjrCNyBXXD14CLgVXAmqAOmTJczSy/oZYFEZkCCpYsOOd+D+wb4+mXAjc75wacc88DG4AzCxVbsSQioSBZqPcHtDCbiAgw7joi15nABufcJufcIHAzvg6ZMqJ1s/2GWhZEZAqIlOA1P2Bm7wQeAf7aObcfaAEeyDlna3DsMGZ2DXANQEtLC62trRMKoq2tbcLXjlV/Tydd/YNs3L6XZcD2TU/TmV1U0NfMpxhlLRcq6/RTKeWEyirrKPLVEblagC05+1uBl+R7onKtL8Iho5skg1ta2V1m73cl/RuslLJWSjlBZS2UYicLXwc+C7jg/kvA1YDlOdflewLn3PXA9QCrV692K1eunFAgra2tTPTasZq3yTG4oZtlx58Gt8G8xirmFfg18ylGWcuFyjr9VEo5obLKOoKR6ohcU76+WPE87Nlaz+JoiqYye78r6d9gpZS1UsoJKmuhFHU2JOfcLudcxjmXBW7gYFejrcCCnFPnA1N+ectkLMxAOks2VucP9LWXNB4RkXI2Sh2Ra8rXF3Pq4ux2DaQ7dpY6FBGRIypqsmBmc3N23wQMzYJxK3CFmcXNbAmwAniomLEVQlXMj9Huy4YgVgP9HSWOSESkfI1SR+R6GFhhZkvMLIafHOPWYsQ3Wfz0qfVkuzTAWUTKX8G6IZnZD4HzgRlmthX4NHC+mZ2CbzLeDPwFgHNunZn9GHgaSAPvd85lChVbsSSjPlnoHcxQnWiA/vaSxiMiUi7GU0eY2TzgW865S5xzaTP7APBbIAzc6JxbV/wSTNzs+gSPuwZCvc+UOhQRkSMqWLLgnFuT5/C3Rzn/c8DnChVPKSRj/s/bN5iBZIO6IYmIBMZTRzjntgOX5OzfBhw2repUMTtYxTma6oRUP0QTpQ5JRGREWsG5gA50Q0plQC0LIiICzKqNs4dgSu0eTZ8qIuVNyUIBJWND3ZDSkKhXy4KIiBANhxiIz/A7WmtBRMqckoUCGhqzcKAbkloWREQEoGZoYTYNchaR8qZkoYAO64aklgUREQEiWsVZRKYIJQsFVBU7OBsSyQZI9UAmVdqgRESk5JINc/yGkgURKXNKFgookdsNKdHgD6p1QUSk4s1oqGGfqyHTpYXZRKS8KVkooKpg6tTewbRvWQCNWxARkWBhtgYG25UsiEh5U7JQQAfHLGQPtixoFWcRkYo3syYerOKsZEFEypuShQKKR0KYQV9uy4K6IYmIVLyZtXHaqMd69pQ6FBGRUSlZKCAzIxkN+wHOiWABHnVDEhGpeDNr4+xxDcT628C5UocjIjIiJQsFVhULH5w6FaBvf0njERGR0muuibHH1RPJ9MFgd6nDEREZkZKFAkvGwsGibI3+QO++0gYkIiIlF4+E6Yk2+x1NnyoiZUzJQoEd6IYUifnWhd62UockIiJlIF01029oFWcRKWNKFgosGYvQm8r4neqZ+gVJRES86ln+XsmCiJQxJQsFVhUN0z+Ykyz0qGVBREQgXDe0irNmRBKR8qVkocCqYmF6U2m/Uz0DNE2eiIgAVQ0zSbuQWhZEpKwpWSiwRCwYswBQM0vJgoiIADCjroq91JHq1MJsIlK+lCwUWFU0mA0JfDekvn2QSZc2KBERKbmZNXHaXD2pDiULIlK+CpYsmNmNZrbbzJ7KOfYvZtZqZk+Y2S1m1hAcX2xmfWa2Nrh9o1BxFduBdRbAd0MCzYgkIhVvPHVEnms3m9mTQX3xSNGCnmRDC7O5LnVDEpHyVciWhZuAi4YduwM4wTl3ErAe+GTOYxudc6cEt/cWMK6iSsYiB7shVQfT5KkrkojITYyvjhjugqC+WF2g+ArOJwv1hHpVJ4hI+SpYsuCc+z2wb9ix251zQ31wHgDmF+r1y0UyGmYwnSWTdQenyVOyICIVTnUEzKiJs4cGYv1tkM2WOhwRkbwiJXztq4Ef5ewvMbM/AZ3A3znn7s13kZldA1wD0NLSQmtr64RevK2tbcLXjkd3ezsAj697hvq+LpYB2597gs7UvIK/9pBilbUcqKzTT6WUEyqrrGMwvI7I5YDbzcwB33TOXZ/vpHKvLzJZRxv1hF2a9U8+TDZeP+mvMV6V9G+wUspaKeUElbVQSpIsmNnfAmng+8GhHcBC59xeMzsd+IWZHe+c6xx+bVApXA+wevVqt3LlygnF0NraykSvHY9F7S/AI3uZv2gps6Lz4DaYVxdhXhFee0ixyloOVNbpp1LKCZVV1tHkqSOGO8c5t93MZgF3mFlr0FJxiKlQX3w/PgMycMzcOphV+ve+kv4NVkpZK6WcoLIWStFnQzKzK4HXAW93zjkA59yAc25vsP0osBE4ptixFUJtwudj3QNpSNRDKKpuSCIiI8hXRwznnNse3O8GbgHOLF6EkyuTDMayaa0FESlTRU0WzOwi4OPAG5xzvTnHZ5pZONheCqwANhUztkKpieckC2Zaa0FEZAQj1RHDzqk2s9qhbeDVwFP5zp0KrHa239AqziJSpgo5deoPgfuBY81sq5m9G/gqUItvNs6dIvVlwBNm9jjwU+C9zrl9eZ94ijmQLPQHY/Zq50Ln9hJGJCJSeuOpI8xsnpndFlw6G/hDUF88BPy3c+43JSjCpIjWz/EbalkQkTJVsDELzrk1eQ5/e4Rzfwb8rFCxlFJN0A2payBIFurmwZ7KGHwjIjKScdYR24FLgu1NwMkFDK2oauqbGXRhoj17sFIHIyKSh1ZwLrDaeBTIaVmoa4GObZC/K66IiFSQmbUJ9lLPYKdaFkSkPClZKLCa3AHOAPUtkOqB/o4SRiUiIuVgZm2cva6OdIeSBREpT0oWCqw6HgZykoW6YH0FjVsQEal4M2vjtLl6nAY4i0iZUrJQYPFImFgkRFduNySAzm2lC0pERMrCjJo4e6kj1NdW6lBERPJSslAEtfEI3QMpv6NkQUREAjNr4+xx9cQG9mosm4iUJSULRVCTiORMnToHMD/IWUREKlpdIkK71RPJDsJAV6nDERE5jJKFIqiJRw6OWQhHoWa2xiyIiAhmRirR7He0YKeIlCElC0VQE48cHLMAfkYkdUMSEREgUzXTbyhZEJEypGShCGoTOS0L4McttL9YuoBERKRshGqULIhI+VKyUASHdEMCaFwMHVsgmy1ZTCIiUh5i9bP9hpIFESlDShaK4JABzuCThcwgdO0oWUwiIlIekkGykOnaXeJIREQOp2ShCGriUbqGtywA7N9cinBERKSMNNfX0O6qGWjfWepQREQOo2ShCGoTEQbTWQbSGX9AyYKIiARm1sbZ6ZpIt2uWPBEpP2NKFsys2sxCwfYxZvYGM4sWNrTpoyYeATjYFal+AVhIyYKITGlmVjfKYwuLGctUNqPGJwt0KVkQkfIz1paF3wMJM2sB7gLeBdxUqKCmmwPJwlBXpEgM6uYrWRCRqe6eoQ0zu2vYY78oaiRT2KzaODtcE9EeJQsiUn7GmiyYc64XeDPw7865NwGrChfW9FKT8MnCIWstNC5SsiAiU53lbDeN8piMYkZNnB2umeTAXkgPljocEZFDjDlZMLOzgLcD/x0cixQmpOmnNp4vWVisZEFEpjo3wna+fRlBMhZmf2SG39EseSJSZsb6hf/DwCeBW5xz68xsKXB3waKaZuqSfnhHV3/q4MHmZdCzG/o7IFFfoshERI7KLDP7CL4VYWibYH9m6cKaegaSc6Af6NzmW55FRMrEmFoWnHO/c869wTn3hWCgc5tz7oOjXWNmN5rZbjN7KudYk5ndYWbPBfeNOY990sw2mNmzZvaaCZeoDNUHyUJ7X06yMHOlv9+zvgQRiYhMihuAWqAmZ3to/1ujXTjeOmLYtRcFdcUGM/vEpJWmhFI18/xGp8YtiEh5GetsSD8wszozqwaeBp41s48d4bKbgIuGHfsEcJdzbgV+oPQngudfBVwBHB9c8x9mFh5zKcpcfZVPFjoPSRaO9fd7WksQkYjI0XPO/eNIN+C2I1x+E2OsI3IFdcPXgIvxY+fWBHXIlGYNLX6jc1tpAxERGWasYxZWOec6gTfiK4CFwJ+NdoFz7vfAvmGHLwW+E2x/J3i+oeM3O+cGnHPPAxuAM8cYW9mrjUcIh4z23pxkoWERRBJKFkRk2jCzVWb2GTN7Dvj6aOeOs47IdSawwTm3yTk3CNwcXDel1dU10kUSOpQsiEh5GeuYhWiwrsIbga8651JmNpHBa7OdczsAnHM7zGxWcLwFeCDnvK3BscOY2TXANQAtLS20tk7sy3ZbW9uEr52ImqixeftuWlsP/tkW1y4ivflRthY4jmKXtZRU1umnUsoJU7OsZrYIWBPc0sAiYLVzbvMEnm6kOiJXC7AlZ38r8JIRYps69UV/J9uzzcze2srOEv4bmIr/BieqUspaKeUElbVQxposfBPYDDwO/D6oHDonMY58U+zlTUacc9cD1wOsXr3arVy5ckIv2NraykSvnYjm2p0Qrz70NdedDC/eX/A4il3WUlJZp59KKSdMvbKa2R+Bevyv+5c5554zs+cnmCiM+WXzHJvy9cWq7i3seLKZJal9Jf03MNX+DR6NSilrpZQTVNZCGesA5+uccy3OuUuc9wJwwQReb5eZzQUI7ncHx7cCC3LOmw9Mq1Fe9VVROnLHLIAft9CxBQa6ShOUiMjR2YMf0Dybg7MfHc2UqSPVEbmmZX0xsybOFjeTUMeLpQ5FROQQYx3gXG9mXzazR4Lbl4DqCbzercCVwfaVwC9zjl9hZnEzWwKsAB6awPOXrfpknmRhVjAmb3dlNJmJyPTinLsUOBF4DPhHM3seaDSziY45G6mOyPUwsMLMlphZDD85xq0TfL2yMbM2zotuFpHBDuhrL3U4IiIHjHWA841AF/C24NYJ/OdoF5jZD4H7gWPNbKuZvRv4PPCqYPDbq4J9nHPrgB/jZ1r6DfB+51xm/MUpXw3J6KEDnAFmH+/vdz1Z/IBERCaBc67DOXejc+5VwEuBTwP/ZmZbRrtuPHWEmc0zs9uC10sDHwB+CzwD/DioQ6a0oWQBgPYXShuMiEiOsY5ZWOace0vO/j+a2drRLnDOrRnhoVeOcP7ngM+NMZ4pp6EqRnvv4LCDCyFeDzufyn+RiMgU4pzbBVwHXBeMbRvt3DHXEc657cAlOfu3ceSpWaeUpuoYWwiShf2bYe7JJY1HRGTIWJOFPjM71zn3BwAzOwfoK1xY0099Mkpnf5pM1hEOBePzzHzrwi4lCyIy9ZjZkbr/vKEogUwD0XCInkQLZPHJgohImRhrsvBe4LtmVh/s7+dgv1IZg6FVnDv7UjRWxw4+MOcEWPsDyGYhNNZeYSIiZeEs/DSmPwQeJP9MRTJGybomujrrqFWyICJlZKyzIT3unDsZOAk4yTl3KvCKgkY2zTQEqzgfNsh59gkw2A3tm4sflIjI0ZkDfAo4AfgKfpxBm3Pud86535U0siloZm2cHaE5alkQkbIyrp+ynXOdwUrOAB8pQDzT1lCy0D48WZhzor/f8USRIxIROTrOuYxz7jfOuSvxg5s3APeY2V+VOLQpaU5dgheyM5UsiEhZOZp+L2puHof6pO96dNgg59knQDgO2x4pQVQiIkcnmPL6zcB/Ae/HD3D+eWmjmprmNiR5dnAmbv8LkB4odTgiIsDYxyzkczQL71ScEbshRWJ+1outShZEZGoxs+/guyD9D/CPzjnN1nAU5tUnuDe7EHMZ2NOqGZFEpCyMmiyYWRf5kwIDkgWJaJoaGuC8v2fw8AfnnwGPfBsyKQhHixyZiMiE/RnQAxwDfNDsQIOzAc45V1eqwKaiuQ1JnnEL/c7Op5QsiEhZGLUbknOu1jlXl+dW65w7mlaJitOQjGIG+4YvzAYwfzWk+zWFqohMKc65UFAfDK8rapUojN+8+gSb3RzS4STs1GKdIlIeNFdnkUTCIRqrYrR15+mHOv8Mf6+uSCIiFWtuQ5IsIfZWL9OPRyJSNpQsFNGMmhh78yUL9fOhdi5seaj4QYmISFmoiUeoTUTYEl3mWxachgaKSOkpWSiiGTVx2rrzjFkw812Rtj5c/KBERKRszK1P0GqLob8d2l8sdTgiIkoWiqm5Jp6/GxL4rkj7n4eetuIGJSIiZWNufZIHUyv8zgt/LG0wIiIoWSgq3w0pT8sCaNyCiIgwryHB/V2zINkIm/9Q6nBERJQsFNOMmjjdA2n6U5nDH5x7ClhYXZFERCrY/MYq2nrTZBacA5vvLXU4IiJKFoppRo1fxXlPV56uSLEqmHMivPhAkaMSEZFysaCpCoC9M8+A9hc0bkFESk7JQhHNqIkDjDxuYdE5sO0RSI/wuIiITGsLGv16pxtrTvcHNtxVwmhERJQsFNVQsjDiuIVFZ/vF2bY9VsSoRESkXCwMWhZaMy3QuASe+VWJIxKRSqdkoYhm1B6pZeFsf/+CBrWJiFSipuoYVbEwL+7vg+NeD8//DvraSx2WiFSwoicLZnasma3NuXWa2YfN7Foz25Zz/JJix1ZozdV+zMKIyUJVE8xaBZvvK2JUIiLlY6Q6Ytg555tZR845/1CicCedmbGwqYot+/pg1aWQTcP635Q6LBGpYJFiv6Bz7lngFAAzCwPbgFuAdwH/6pz7YrFjKpZENExtPJJ/YbYhi86BtT+ATArC0eIFJyJSBkapI4a71zn3uiKGVjQLmqp4cW8vzDsXauf5rkgnX1HqsESkQpW6G9IrgY3OuRdKHEfRzKyLs6uzf+QTFp8DqR7Y8XjxghIRKU8VV0cALGisYsv+XpyZ74q04U4Y6C51WCJSoYresjDMFcAPc/Y/YGbvBB4B/to5t3/4BWZ2DXANQEtLC62trRN64ba2tglfezQaolk27dw/4muHB2exAtj98C3s666ZlNcsVVlLQWWdfiqlnFBZZR2j4XVErrPM7HFgO/BR59y64SdM1foinu6idzDDA2vX0VJ9MgvT/Wz7/XfoWvDKgr92Jf0brJSyVko5QWUtlJIlC2YWA94AfDI49HXgs4AL7r8EXD38Oufc9cD1AKtXr3YrV66c0Ou3trYy0WuPxjHrBrnzmd2jv/YfVjCr9zlmTVJ8pSprKais00+llBMqq6xHkqeOyPUYsMg51x2Mb/sFsGL4SVO1vtgT3sPXH2zD1c1h4Ukr4cF/oKXtPnjV+wv+2pX0b7BSylop5QSVtVBK2Q3pYuAx59wuAOfcLudcxjmXBW4AzixhbAUzryFJW/dA/lWchyw6G168HzLp4gUmIlJeDqkjcjnnOp1z3cH2bUDUzGYUO8BCOXZOLQDP7uyCcARWvwue/W/Y82yJIxORSlTKZGENOc3LZjY357E3AU8VPaIimNfgF9zZ2THKuIVlr4CBTtii1ZxFpGIdUkfkMrM5ZmbB9pn4umxvEWMrqJk1cZqrY7Tu6PIHXvI+iFbBH/6tpHGJSGUqSbJgZlXAq4Cf5xz+f2b2pJk9AVwA/H+liK3QWoJkYXt738gnLX8lhGPQ+t9FikpEpHzkqyPM7L1m9t5g9zLgqWDMwnXAFc45V/xIC8PMOHZOLa07O/2B6mY47Up48sfQ/mJpgxORilOSZME51+uca3bOdeQc+zPn3InOuZOcc29wzu0oRWyFNpQsbB0tWYjXwtLzofXXMH3qPxGRMRmhjviGc+4bwfZXnXPHO+dOds691Dn3x9JFWxgr59Sxflc3mWxQB5z9AcDgj/9e0rhEpPKUeurUijOnPoHZEVoWAFa+1v+CtPOJ4gQmIiJlY+WcWvpSGV7c1+sP1M+Hky+Hx74LndtLG5yIVBQlC0UWi4SYVRsfQ7LweghF4IkfFycwEREpGyvn+kHOT2/vPHjwZR+DbAbu+XyJohKRSqRkoQTmNSTZdqRkoboZlr8KnvyprxxERKRirJxTRywSYu2WnOWGGhfDGe+GP/0X7FlfsthEpLIoWSiBBY1VvLC398gnnnw5dO+E539f+KBERKRsxCIhTmyp508vth/6wMs+5mdG+t/PlCQuEak8ShZKYPmsGra199E3eIQWg2MugngdPPGj4gQmIiJl49QFDTy5rYPBdPbgweoZcPZfwTO/gi0Ply44EakYShZKYNnMGpyDTW3do58YTcKqS32lMNhTnOBERKQsnLqwkYF09uAUqkPOej9Uz4Q7P60Z80Sk4JQslMCyWdUAbNwzhgTgpMthsBtabytwVCIiUk5OXdgAwKMv7D/0gXgNvPzj8MJ98NwdxQ9MRCqKkoUSWNxcTchgw+4jtCwALDoHGhbCY98pfGAiIlI25jUkmd+Y5P6NeRanPv0qaFwCd16rSTBEpKCULJRAIhpmQVMVG/eMIVkIhWD1u2HzvbDr6cIHJyIiZePc5TO4f9Ne0pnsoQ+Eo/DKv4fd6+DJn5QmOBGpCEoWSmT5zBo2jqVlAeC0d0IkAQ/fUNigRESkrJyzfAZd/Wme2t55+IOr3gTzToXb/w562oofnIhUBCULJbJ8Vg2b9vSQGv5rUT5VTXDCZfD4zdDXXvDYRESkPJy9rBmA+zbkSQZCIXjDV6G/A379YQ12FpGCULJQIqvm1TGYyY6tKxLAme+BVC+s/UFhAxMRkbLRXBPnhJY67nh6V/4T5pwAF/ytnzVP02yLSAEoWSiR4+fVAfDUtjxNy/nMOwUWvAQe+iZkUoULTEREysrrTprH2i3tvDjSYp5n/xUsPBtu+xi0bylucCIy7SlZKJElM2pIRsM8ta1j7Bed+//B/s2w9vsFi0tERMrL60+eB8Ctj2/Lf0IoDG/6Orgs/OJ9kB1D91YRkTFSslAi4ZCxal4dT+cbtDaSYy6C+WfCPZ+HVF/hghMRkbLR0pBk9aJGfv6nbWSzI4xLaFwMF/2znznvrmuLGZ6ITHNKFkrohHl1rNveMfKH/3BmcOG10LUDHrq+oLGJiEj5ePtLF7JpTw+/f27PyCed+md+qu37vgKP3lS02ERkelOyUELHt9TTM5hhU9sYBzkDLD4Hlr8K7v2yZkYSEakQrz1xHrNq43z7D8+PfJIZXPIvsPQCuO1vYNtjxQtQRKYtJQsldMbiJgAe2LRvfBde+Gk/Vd49/1yAqEREpNzEIiGuPHsx9z7XxvpdXSOfGArDW74NNbPgv94CO58qXpAiMi2VJFkws81m9qSZrTWzR4JjTWZ2h5k9F9w3liK2YlrcXMWcugT3b9o7vgvnnAirr/ZdkXY8XpjgRERKJF8dMexxM7PrzGyDmT1hZqeVIs5i+z9nLiQRDXHjaK0LANXN8M5fQjQJ33m9EgYROSqlbFm4wDl3inNudbD/CeAu59wK4K5gf1ozM85a1syDm/bixruYziv+DqpnwY/eoZU7RWQ6Gl5H5LoYWBHcrgG+XtTISqSxOsabT5vPz/+0jT1dA6Of3LwMrvyVTxhuugRabytOkCIy7ZRTN6RLge8E298B3li6UIrnrKXNtHUPsmH3OMYtgF/V+YofQPdu+PE7IT1YmABFRMrPpcB3nfcA0GBmc0sdVDG857ylZLKOf//f5458cvMyeNf/QOMSuHkN3P73WqdHRMYtUqLXdcDtZuaAbzrnrgdmO+d2ADjndpjZrHwXmtk1+F+SaGlpobW1dUIBtLW1TfjayTQL/8H90z+s403HN4zz6mpqz/gULff/A+3ffxc7z/w7P8BtmHIpazGorNNPpZQTKqusR5CvjsjVAuSuPrY1OLYj96TpVl8MuXhFLd9/4AVePjdLS13siOfbudcx609fofGP19G7/ndsP/ufSFflrWLLrqyFVCllrZRygspaKKVKFs5xzm0PEoI7zGzMpQ0qjesBVq9e7VauXDmhAFpbW5notZNpJbDi3n2s2298ciLxrFwJsV4afvd5GuYshld9FkKHNhiVS1mLQWWdfiqlnFBZZT2Cw+oI59zvcx4//FcRn2AcemCa1RdDPt2yhP/9l7v52XMp/uPtJ43touNvhCcupupXH2L5ne+C134JjnuD6osKKGullBNU1kIpSTck59z24H43cAtwJrBrqBk5uN9dithK4RUrZ/Hg83vpHkhP7AnO/wSceQ3c/1X43qUawyAiU9oIdUSurcCCnP35wPbiRFd6M2vjvOe8pdz25E4ee3H/2C886a1wzd1+pqSfXAnfPA/W/UIrPovIqIqeLJhZtZnVDm0DrwaeAm4FrgxOuxL4ZbFjK5VXrJxFKuO4d/0oi+2Mxgwu/n/whq/ClofghlfA7spohhOR6WWUOiLXrcA7g1mRXgp0DHVjrRTvedlSZtTE+cyvnh77wp4AM4+Fv7gX3nQ9pPt90vD1s/0iboM9BYtXRKauUrQszAb+YGaPAw8B/+2c+w3weeBVZvYc8KpgvyKcvqiRhqoo//PUzok/iRmc9mdw1W2Q6oNvvxo2/2HyghQRKY68dYSZvdfM3huccxuwCdgA3AD8ZWlCLZ2aeIRPXryStVva+cmjW458Qa5wBE6+HN7/ELz5W35thl99CL50HLMe+1fYu7EwQYvIlFT0MQvOuU3AyXmO7wVeWex4ykEkHOJ1J83lp49upXsgTU38KN6W+afDe+6C/7oMvv82ePtPgOZJi1VEpJBGqSO+kbPtgPcXM65y9ObTWvjRw1v47K+f4aylM1jYXDW+JwiFfdekEy+DFx+Ah2+gcd1PYP3NsOwVcPpVsOhcv26DiFSscpo6taK96dQW+lNZbl93FK0LQxoW+vm16+bBd99Aw/qfHP1ziohIWTEzvvS2kwkZvP8Hj9E3mJnoE8Gis+CyG9nwhlvh/E/B7mf8tNz/sgy+92bYdA+Mdz0gEZkWlCyUidMWNrKgKcktf9o2OU9YOxv+/E5Y8WrmPPZF+P2/6INeRGSaWdBUxZffdgpPbe/gQzf/icx4xi/kkUnOgPM/Dh9+0ndrPe+vYffT8N1L/Xi4x2+G9BEWhBORaUXJQpkwM954Sgv3bWhjd2f/5DxpsgHe9j06Fl0E//tP8F9vhq5dk/PcIiJSFi5cNZt/eN0qbn96F5/772cm50nDUVh8Drzy7+GDa+GSL8JAF9zyF/DPC+CrZ8APLofffNIPjtY4B5Fpq1TrLEgel57Swr//7wZufXw7f37e0sl50nCEHS/9NPXHX+hX77z+5fD6r8Axr5mc5xcRkZJ71zlLeGFvLzfe9zxz6xO852WTVIcARBNw5ntg9bth093+tu952L8Znv89pHr9eXNPhoVnQzYFTctg1Rugfv7kxSEiJaFkoYwsn1XDSfPr+dlj23j3uUuwPKsxT4iF/Af9wrPgp++CH7wNVrwGXvN/YcbyyXkNEREpqb9/3Sr2dA3wudueIRwyrj53yeS+QCgEy1/pb0OyWdj/PDx7Gzz1c3jsO75Vor8Dbv9bmLUKZh/vbzOPg4YFUL8A4jWTG5uIFIyShTKz5syFfPLnT/LApn2ctWySZ6CYcwK89z548Bvwuy/A1870062+/ON+MLSIiExZ4ZDx5ctPJp3N8plfP03rzk7+4fXHH90Me0cSCkHzMjj7r/xtyP7NsPaHsP0xP433Ez869LrZJ8LsVdC4BOad4pOKeC1074Laub4brYiUBSULZeZNp7bwpduf5YZ7N01+sgAQicE5H4STLod7vwiP/Cc8/iM/iO30q6Bm5uS/poiIFEU8EuY/3n46X77jWf7jno3cv2kv/+8tJxemPhlN42K44JMH93v3Qdtz0LHFj2/YfC+8eD88+RNweVaQXvBSWHUpVM/0szXNWAEzjvVdokSkqJQslJlENMw7z1rMl+9Yz3O7ulgxu7YwL1Q7Gy75Fzjr/X4sw93/BPf8XzjuDXDeR3zfUxERmXLCIeNjr1nJ+cfO4q9//DhrbniAMxY38ufnLeXC42YTDk1SF9fxqGqChS8BXhIc+Li/G+yBnU/CnmdhsBuqZ8G+TfD4D+G3nzz0OSzkx0LMOs53a5pxDDQu8q0UkVgxSyNSUZQslKF3vHQR/3HPBr517/N84bKTCvtijYvh8u/5D+snf+JbGp7+hR/fcMxFcOwlMPOYwsYgIiKT7ozFTfzmw+dx80NbuPG+5/mL7z3K4uYqrj53CZedPp+qWBl8BYhVw8KX+luul/8N9LRBfztkUtD2LOx62k/juuspeOZXQDBNbCQJC870awzFqv0t2QhzTvJdnBL1RS6UyPRSBp8UMlxTdYzLTp/Pjx/eyv/3qmOYU1+EZtc5J/rbeX8ND3/bD1S789P+tvxCPwvG8gv1642IyBRSFYtw9blLeOdZi/jtul3ccO8m/uGX6/jyHeu57LT5XHTCHE5d2Fia1obRmPlusUNdY2evguPfdPDxwV7fArF3g+/O9MIfoW29b6kY7AGXs0Bd8wpYdgEseAlUz4CW0/34CBEZEyULZeqa85bx44e38oXftPKvl59SvBdO1PtuSOd9BDq2+abgB78BN6+BqmY46Qo49R2+GXiyZmsSEZGCioRDvPakuVxy4hwefWE/N9y7ie/cv5lv/eF5mqtjXHjcbF61ajYzMnnGD5SjWJWftGPOCXD8Gw99zDk/RmLHn2D7n2DrI/DYd+Gh6/3jFoKZx7EwG4b76/wMTc3LfGvE0MBrl4VYjT8XVN9JRVOyUKYWNlfxnpct4Wt3b2TNmQs5c0lT8YOob4GXfRTO+RBs/F9Y+3146JvwwNegYRGcfqXvrlQ3D+oX+lkxRESkbJkZqxc3sXpxE539KX737B5uf3oXtz25gx89soWwwSkL2zlraTNnLWvmtIWNJGPhUoc9PmZQ3exbw5df6I8NdPkfwDq3wZYHYftaXMdenxRsuNP/MDZcKAIYJOrgmIt9nRiOgoVh5rG+HsT5QdhVMyCsr1QyPelfdhl7/wXL+eXa7XziZ09w24fOIxEt0Qd2OOoXcTvmNX4F6PX/A0/+FO76zMFzkk0wf7XvI7rytb5LUzhamnhFROSI6hJRXn/yPF5/8jwG01kefH4vv3pwPes7HF//3Ua+evcGYuEQpyxo4GXHzOCClbNYNbdu8tYAKqZ4Lcxa6W/BOhFbWltZuXKlb4nIpKBntx+/t/sZX3/17vPJRPsLvt7r3Tvy81vIJw89bRCJ+2Ri5rF+atho0k8akmz0rffVQdcq5/Qjm0wJShbKWFUswhfechJv/9aDfPmO9XzqkuNKHZKfRen0q/ytczvsWud/qXnxAf8hu+EuPyVrJAnHXuynaF16vqa7ExEpY7FIiPNWzGRmZi8rV66kqz/FIy/s54GNe7lvYxtfvH09X7x9PXWJCCvn1rFqbh0r59Ry3Nw6jpldO/VaH3KZ+fF49fP97diL85+XzfqxEJlB372pp81f27PH14d7N/pEIN3vx0889TO/ON1IQhGYfwbUzPbXVM/0A7Kj1X6Wp0gcMP8atXOhdk4hSi9yREoWytw5y2ew5syFfOveTVwcDEQrG3XzDi7mdvpV/r5nL2y62w82W3cLrPu5/0CctcoPKpt/hp+1ommZflERESlTtYkoFxw7iwuOnQXA7q5+7nl2D2u3tNO6o5OfPLKFnkE/iNgMlsyo5vh59T6JmFvLgsYqWhqSUzuJGC4UAkK+1WHxuUc+3zkY6PSDsbc9Aql+3zrRuxdCYd81astDfoanSNwP1P7T90Z+vuqZEI77pKFmNnRth3DM16tzTvQ/2oVjfgbDuaf4Vg0tbieTQMnCFPCpS1byu2d386Gb1/LL959DY3UZz0hU3QwnXuZvF38BNt7tPwC3P+Z/ZXn0P/15iQZoOQ3mneo/dJe83H94iohI2ZlVm+BtqxfwttULAMhmHVv39/H0jk6eCW6PvbCfXz2+/ZDrls2s5qT5DSxsqmJRcxWLmqtZPquG+mQFdFM1892OEvVQ9/ojn59J+1aKwR5o3wzZjO8G5bJ+5qe253x3qa4dfr92tt9/8JuQTUG83p872HXI066I1kJ1o08kLOzr2li1n7SkqtkP5O7eCdEqqJkFNXN8YpJsDG4Nvs4287GFIlA3txB/MSlTShamgNpElK++/TSuuP4B/uK/HuU77zpzavxaE47CMa/2N/BNuG3rYetDsPVh34x731fg3i/5hXiWnh/cXu6bgkVEpCyFQsbC5ioWNldx0QkHu8d09KZ4dlcX29v7eHFfL4+9uJ8HN+3lF2u34dzB62fXxTlmdi3LZ9WwdGYN8xuSzGtIMrchQW08MjXHRRytcM6X8BnLx35d9x4/rmLeqX7sRMcW2PEEdGyFvv10bN9EU9Igmw5uGf+lv3Ob7z7c3+kTj/QAdO/y3ayOpHaeTzrS/T5hyWZ8/LVzoa7FL5oXrYJ9G6Gv/WDSlKiDeB3gfOISq/bjSRINPilJNvrtdD+k+vx4j549fg2N6hnj/pPK5FCyMEWctrCRf7nsJD78o7W857uP8B/vOI26xBT7ZSYUOjjA7LR3+mOpPlj/W3jmVj/j0pM/9sfj9RCv8R8uc06CuSf5AWJzT9b82CIiZaq+Kpp39r7+VIat+/vY3NbDhj3drN/VxYbd3dz80Bb6UplDzjWDmTVxFs+oZklzNYtnVLO4uYqWxiRz65PMqIlVZjIxktz1KMB/sW5YeGB3d2srTStXju25nIO+/f4Lel+7XxSvb7/fBv/lfqALdqz1X/Yjcd9iEYr4L/hdO6HjRd8dOZv2yUPNLP9DYX+Hv7nMyK8/mqoZ/jtBvNYnJKGglSQUgaal4LLM2vki7D7Gv2ayySdFffv8YPVsysefzfgWldo5/tpUH+CCZKbxYJkisYNJzNDfJpOCzACkB/1rJ+orYlrdoicLZrYA+C4wB8gC1zvnvmJm1wLvAfYEp37KOXdbseMrZ5ee0kIq4/j4z57gjV+9j2/+2emsmD3FvzhHk36O7OPf6Fsedj8Nm+/1TaypXujeDZvugSdu9udbCGau9N2WTn2Hn2NbRKaNkeqIYeecD/wSeD449HPn3GeQspWIhlk+q4bls2q4kNkHjmezjt1dA2zv6GN7ex872vvp7E+xo6OfzW093NW6i7buQ3/pjkdCtDQmWdBYxfzGJPMbq1jQ5O/nNyZprlYyMWFmUNXkb0cjk/b3w6eTdc5/gTcLukz1+OTjQGLS7pOTSMwP9B7s9i0K+1+APa3+e0Hfftj/vL8+Gww4f+pnANSHE7C+N1/B/Jf7bHr8ZYkk/XePdP/hiU4o4pOSZCOk+6CvI2hhmedbiWrn+oSicxuEov6Yhfz4lWjCt77Eavy6IdEqn6hkUr5MmUHf2hOr9q1E2/8Ei872PTF69tC4Zw8MHOcTn2jSD7Jvf9FPab/4nPGXc7Q/waQ+29ikgb92zj1mZrXAo2Z2R/DYvzrnvliCmKaMy06fz4LGJO//wZ+49Gv38c9vPpFLT2kpdViTIxQ6uMjOcF27/C8Z2x7zXZgeudEvsHPuh2HZK2DGsWqiFJke8tYRzrmnh513r3PudSWITyZRKGTMqU8wpz7BaSNM4NHZn+LFvb0+mejoZ1t7H1v397JlXx9PbG1nf2/qkPOT0XCQRCRZ0HQwoWhIRolHw8ypTzCrNl6M4lWukdacMPO9BobEaydnlqfBXrAQz23czMplwRS2vXv9F/GqpqAFIOS/iFsIetv82A/n/Jd08MlKf4f/gj70Zb2nzXfNAogk/JfySMK3PGQGg1aLvb7lIlrlWyEyg/6Le+d2/wW/v8OXMZP20/O6rD831Te+Vpb6hfDswd/QZwP8Kc95539q6icLzrkdwI5gu8vMngGmybfd4njJ0mZ+/Vfn8oEfPMaHbl7L7U/v4hMXrWRBU1WpQyuc2tlQG6z1AP4/5v/8jR/vcO+X/LFQhOWxeph1jJ95qXaOz8DjNX7miPr5fl+zMImUrVHqiOHJglSIukSUE1rqOaGlPu/jXf0pn0Ds62PL/l627u9jyz5//8gL++nqP/zXZDNoTIRZ0NzGnPoEc+uTwX2COXV+f1ZdvHTrG8n4xHK+/0STflXuhgWHnxcJJoipnVO6qWidC1pWnE9MUr2+BWWw13dxCseCblBBd6iBLr9dP9/3tujvhOpm1j+3gWNamnyyMtgTLJA737dETLKSjlkws8XAqcCDwDnAB8zsncAj+F+W9pcwvLI2pz7Bzde8lK/dvZGv/24Ddzy9i3eds5i/fPly6qum2FiGiahqgrd8Cy681jdN7lkPPbvp3r6RhoFt8NAN/j/dcOGcubTrF/jsvr7Fz/6QbIDm5dC4WAvKiZSBYXXEcGeZ2ePAduCjzrl1xYxNykdtIsrKOVFWzqnL+3hHX4ot+3rpHkjTl8qwq6OfHR39tL64kz6L8XxbD3/cuDdvUtFcHTvQElGbiFKTiFAbj1ATj1CT8Pe1iQg18ai/H3o8ESEZDas7lBxu6N+EWdAVKTF6t6+aWYduB/vZWC00L/O3AjOXOz1BEZlZDfA74HPOuZ+b2WygDXDAZ4G5zrmr81x3DXANQEtLy+l33nnnhF6/ra2NGTOmR7eVPT1pvvPYXu7a2E1VNMSrV9RyyTF1LGjwGfR0KuuRHCirc4RS3UT692HpXiJ9e4n27iDas5No706iPTuJ9O4ilO4nnDp0mjlnYQZrWhisXUS6ejbpeCOZeCOZRCPpeCPpRDOZRAPZaI1vziyRSnlfK6WcUJiyHnfccY8651ZP6pMWwfA6YthjdUDWOddtZpcAX3HOrcjzHKovxqmSy9qbyrK3J82e3jRtPWnaejPBfZp9fWn6Uo6ewSy9qSyDmSN/dwoZVEVDNCXDNFVFqI2HmFEVYVZ1hEQ0RCJiJKMhkhG/nYiGSAbHYmHDAfGwHXXCUcnv6XRWzPqiJMmCmUWBXwO/dc59Oc/ji4FfO+dGHb26evVq98gjj0wohtahZd6nkae3d/K1ezZw+7qdpDKOly5t4u0vWcTiSAcnHl8Gqz8XwYTe1/4O34zXsxf2boC9z/n5rPdu8H0a+0Zo4LJwMBAsmKt6aLtuvm+dqJvr56puXl6QNSSm47/hfCqlnFCYsprZlEsWjlRH5Dl/M7DaOdc20jmqL8ZGZR2bwXSWnoE03QNpuvr9ffdAiq7+nP3+NB19KfZ0DbCne4D23kG2tffRn8qO+XVq4xGaamIko2ES0TDJaJhkzN/HoyG/HxxLHLIdojYepbE6RvuuLZx43DHs6xkkGQ3TO5ghZMaCpiRVsek1Kab+/R6dkeqLUsyGZMC3gWdyKwEzmxv0VQV4E/BUsWOb6lbNq+Nr/+c09nQN8JNHt/CDB1/kr374J+oTIda8aPyfMxeysHkaj2uYqKH5n5uWwoIzDn88k/JjJHp2++nkuvccXIXzwG0ftG2Anvv9wKlc4bhvZqxf4GdMCIX9rAixar+ydbzWd3saGmMRTcKMFf5epMKMVEcMO2cOsMs558zsTCAE7C1imFLhYpEQsUhs3IukZrOOzv4UvYMZegfT9A5m6Bk4uD1035fK4Bzs6uynsy9FXypDXypL/2CGPV0D9KUy9Ae3vuD87Ki//W7Je7ShKkoiEqalMUlVLEzWObJZiISNmTVxZtTGiYQOtmyEQ3YgWakKEpSqWIRwCMKhEM3VMeqTUXoG03T0pgiHjNqE76JVm4hQHYsQCqlr1lRTipTyHODPgCfNbG1w7FPAGjM7Bd8NaTPwFyWIbVqYWRvnL89fzntftox7N7TxzTvXccO9m/jG7zZyQksdr1w5m1etms3x8+rUn3IswtFggPXsI58LfoaD9hd9q0Tndj8dbKrfL5TT3+kHNGXTvuXi6V/kfw4LQcMiP2tCvO7gFGw1s/w4i2gVNd0GTfgEJJvySQ3Ot2bEaipi7meZlkaqIxYCOOe+AVwGvM/M0kAfcIUrVZ9akXEIhYyGqhgNk/y7nXOOwUyW/sFskFhk6OpPsa9nkHUbXqC2aSZN1TH6U1mqYmHSWceWfb3s7OinL5Vhy75eegbShMwImdGbyrBpTw9t3QNkc/5rpbOOo/mfFjKCcR4+gQiHjIF0lur4wbEg1fEIHX0p9vcOUpeIUJeM0pCMMqsuQV0iQjgUIhIyYpEQdckI2Syks1lCZuzZ2UtnfJ/v0hW0xsSjIfoHs+zp7qezL00yFvZjToLXikVCmIEB0XCIcMiIhI6++9d0UorZkP6Af0+G05oKkywUMl5+zExmZ+fQMHcxv1i7jTuf3sV1//scX7nrOWbUxDhzSRMvWdLMmUuaOHZ2rTL+yRBNwsxj/e1IUn0HV8Ds2AK9+2GwC3Y/4xexCUVhoPPgFGw9ew5cOh/gDyM8r4UPLmpXPcsnENXNfr7ocMwP5q6f7xe5cVl/S9T5+aJj1b71w8I59yG/UJ5mkpICG6WOyD3nq8BXixORSPkzM+KRMPFImHoOnaBjjtvHypWLJ+V1nHMMpLP0DWboHWrVGMyQdY5UJsvenkE6+lJUxyLUJ6NknW9J8d2zDnbTGjqWzTpikRA9gxm6+32Xra7+FDWJCM3VcfZ0D7BxTw/tvYN05hmAnt+OI58yBpGQEQkb0VCIWCREfTJKxrmgRSdLyKCxKkZtIsJAOkvvYIbGqijNNfEDCUcouA/n3Pvt0IHt8LBzQiHDMHZ39RMyoy4RpS7pE6xYxI9niUVC7NzeS0dsL7FIiGg4RMiMLft7WTKjmmMmeQ2u6dVZTUY0pz7Be1++jPe+fBlt3QPc3bqbP27cy4Ob9nLbkzsB3xx5xuImTl3YwEktDZzQUkdD1fiaWGWcosmD3Y1yZzw4/k35z3fOJxeDPWxe+zsW12Z8MhGKHpzBqXu3PzbQ7eeN7tkDnVthx+MHV54c7Mr//KMJx3yXqmzaj83IDPrYE3XBitu1PrkY6IRYrW/xSDYcXAEz0eDPxQ4mKOGYn487m/FjOxL1/jnDMbWMiIiUGTMjEfxin39VjMIZ6qKVzjjS2SwD6Sxd/WlCBpFQiKxzPLthE7PnLfBf6NM+kelPZ4lHQsyqjVOXjNI3mKF7IH1gzMlg2o8hcc63nKQzWVLBfTrrk6CBdJaO3hSRsJGI+DEhGefY35uisy91YLzIvp5Bdnf1k8lCJuuvz+Tc0llHNrj3+9kDLSPDu5FVx/xsWt0DoyVJhydGH3zFcj7y6jH8WDkOShYq0IyaOG9dvYC3rvZzEG/Z18uDz+/joef38tDz+7jj6V0Hzl3QlOTEYH7rE4ObEogSMjuQYPQ3Hw8THdyU6oeu7X6shRlgfi7n3jbf2pHN+MVisplglcw0dO30CUIo4q+LxPy80ANdQevHVr/oTKLODxbfsdavxpnqmUhBgwVwEiwjCnc1HxxbEokf/Fsc0voR9t23cvdDEZ9ERRLBnNVxfz+0P9DlyxiJ5Tw2dF7s4H0k4W9VzT4pUiIjIlJUVbHIEQdkR7qSrFwxNWdDymYdGeeTiKxzB6beTWeyBwbND6SzDKazDGaybNj0PHNbFjCY9slMJuuY35hk2ayaI7/YOClZEBY0VbGgqYrLTp8PQHvvIE9t6+TJbR08ta2DJ7d1HGh9AJjfeGgCcczsWmbXxdW/byqJJvyA7qalhX+t9KBv4ehr91/OIfiib/6xbPCryd4NfmGaVJ8f15Hug1Q/PW3baUiE/KxVXTuCsRkErROZnMRm+H5wywzmX3NjooZW8cwVigQJSuRgknLgFjp0P5LwSc9gD+zbFCSAVRCJs7CvH+6r9glOKOL/DoPd0LgILv+vySuDiIiUlVDICGEMXwcwEg7RWH34YPpET5KVy4uTGClZkMM0VMU4d8UMzs3Jzjt6Uzy13ScOT2719//z1MEEojoWZunMGpbOrGbJjGq/PcNvV8f1z6yiRWKHLCQzohGWp9/Z2krD0U4P55xPGtL9QSIS3GLBF/P0wMEuWpmcx3OPpfqgp83PipUeOPS5XcYnPUMJSjadcwv2h84Z6ITObb4FY84JPskY7IHMIC5l/u+VSftYIwk/sL1h0dGVX0REZIL0LU7GpL4qyjnLZ3DO8kMTiHXbO9i4p5sNu7vZ1NbDI5v3c+vj2w+ZLWF2XZxFTdXMqU8wtz5x4P74efUsaNJUrlIEZge7GJWxLRU0R7iIiEwNShZkwuqropy9fAZnD2sG609l2Ly3h+f39LCprYeNe7rZtr+Px7e285t1/QcGEwHEIyHm1CdYMauWFbNraGlIMq8hwbyGJHPrk9QlIureJCIiIlIiShZk0iWiYVbOqWPlnLrDHnPB7AHb9vfxpy372bq/j637e1m/q5t7nt1Neth0ANWxsE8cGpLMqx9KIvz90HZieAc/EREREZkUShakqMyMpuoYTdUxTpxff8hjmaxjT9cA2zv62N7ex472/oPbHf08vb2Ttu7DB6o2VceYW59gdl2CaKaPFS8YM2vjzKyNMyu4n1kbn3bL2ouIiIgUmr49SdkIh4w5wZiG0xbmn8F5IJ1hZ0c/29v7gySij+0dfntXZz879vdyx4YNeZe9r4lHaAqWoq9PRqmvih7cDm4NwX1dzjm1cXWFEhERkcqkZEGmlHgkzKLmahY1V+d9vLW1lRXHHMv+3kF2dw6wp3uAPV3+trurn33B6pIdfSm2t/cd2B7e/SlXyDgkoagbtj20bPzQ0vG1iWA/ESxfn4gcmC9ZREREZCpRsiDTTjhkzKiJM6NmbDPfOOfoHczQ0ZeivTd1IIHoDO7b+4YSjPSBx7bs6/Xn9KfJjJJoDAkZVMciJGNhquM+eaiOh0nGIlTHwv54LEJVLBwsPBOmKh4+eM2Ba8NURSNUxcP0p7I455SEiIiISMEoWZCKZ2ZUB60C8xqSR74gh3OOgXT2wNLxXf0Hl5DvDva7B9J096fpGUzTN5ihZzBD32CanoEMHb2D7GjP0DuYoXcwTc9g5pDZoo4c+/Mko2ES0TDhkBEJGSEzImGjKhahsSpKXSJKPBoiHgkRi4SIR8LEg3u/Hwoez9kfdm4iGiIW9gPJHY66hG9VCYeUqIiIiExnShZEjoKZkQi+rI+1JeNI0pksvamMTywG0kEi4ZOJ3O0Xtu6gur6J3sEMfakM2WCZ+HTW3/cMpNnfm2Ljnm4GM1kGUtngPsNAOjtq16uxOphchA9JOnITjkjIiIZDwc2I5G6HQkQjRjTkj0XCRiy4j4T9tbt3dbKuZ6s/FgoeCwXPEzKfJAXPFw5eKxzyz5l77lAylXuvVhkREZHRKVkQKTORcIi6cIi6RHTU81pb+1m58tgJv04m6xhMZxlIZ4J7vz0wtJ0a/phv8TA40B3rkGtSh17fn8rQPZAmnXGkMtng5khnsqSy/lg64xgMHnOj5i57JlzO0YRDRtgOJhBmwbGhmxnhsL8PDWu5Gbou9xayg4nIwVuIsOHvQxzy3FkHGeeIhHyS1N25nzlb1x/yen2pDLWJKO8+d0lB/gYiIiKjUbIgUqHCISMZjJcoB5mhBCLrSKWzpLJZsll49rnnWLxkKemsI51xpLPZYff+On+9P35gO0hMsgdaXPzzZzIHW2Byjzvn48i4g9ccuDbPsazzMWWcI5Xyrzt0LOuGXT90C54n4xwhM0IG6SBx609lyLr2w/42Jy9oULIgIiIloWRBRMqC/8U9SFxyenS110RHnP1qumltbeWYY471XcqCpCMeCREJh0odmoiIVCglCyIiZSQUMkIYESCuT2gRESkx/VwlIiIiIiJ5KVkQEREREZG8yi5ZMLOLzOxZM9tgZp8odTwiIlJcR6oHzLsuePwJMzutFHGKiFSCskoWzCwMfA24GFgFrDGzVaWNSkREimWM9cDFwIrgdg3w9aIGKSJSQcoqWQDOBDY45zY55waBm4FLSxyTiIgUz1jqgUuB7zrvAaDBzOYWO1ARkUpQbnNttABbcva3Ai/JPcHMrsH/kgTQbWbPTvC1ZgBtE7x2qlFZp6dKKWullBMKU9ZFk/x8hXbEemCEc1qAHbknqb6YEJV1+qmUcoLKerTy1hfllixYnmOHrOvqnLseuP6oX8jsEefc6qN9nqlAZZ2eKqWslVJOqKyyjuKI9cAYz1F9MQEq6/RTKeUElbVQyq0b0lZgQc7+fGB7iWIREZHiG0s9oLpCRKRIyi1ZeBhYYWZLzCwGXAHcWuKYRESkeMZSD9wKvDOYFemlQIdzbsfwJxIRkaNXVt2QnHNpM/sA8FsgDNzonFtXoJc76qbpKURlnZ4qpayVUk6orLLmNVI9YGbvDR7/BnAbcAmwAegF3lXgsCrpfVFZp59KKSeorAVhzh3WzVNERERERKTsuiGJiIiIiEiZULIgIiIiIiJ5VWSyYGYXmdmzZrbBzD5R6ngmm5ltNrMnzWytmT0SHGsyszvM7LngvrHUcY6Xmd1oZrvN7KmcYyOWy8w+GbzHz5rZa0oT9cSMUNZrzWxb8L6uNbNLch6bymVdYGZ3m9kzZrbOzD4UHJ9W7+0o5ZyW7+t0oLpiatYVoPpiOn6uVEpdAWVYXzjnKuqGHzC3EVgKxIDHgVWljmuSy7gZmDHs2P8DPhFsfwL4QqnjnEC5XgacBjx1pHIBq4L3Ng4sCd7zcKnLcJRlvRb4aJ5zp3pZ5wKnBdu1wPqgTNPqvR2lnNPyfZ3qN9UVU7euCGJXfTHNPlcqpa44QllL8r5WYsvCmcAG59wm59wgcDNwaYljKoZLge8E298B3li6UCbGOfd7YN+wwyOV61LgZufcgHPuefysKWcWI87JMEJZRzLVy7rDOfdYsN0FPINfjXdavbejlHMkU7Kc04jqiilaV4Dqi1FM2bJWSl0B5VdfVGKy0AJsydnfyuhvwFTkgNvN7FEzuyY4NtsF85AH97NKFt3kGqlc0/V9/oCZPRE0Ow81tU6bsprZYuBU4EGm8Xs7rJwwzd/XKaoS/v6VVFfANP5MGcG0/VyplLoCyqO+qMRkwfIcm27zx57jnDsNuBh4v5m9rNQBlcB0fJ+/DiwDTgF2AF8Kjk+LsppZDfAz4MPOuc7RTs1zbMqUN085p/X7OoVVwt9fdYU3Hd/rafu5Uil1BZRPfVGJycJWYEHO/nxge4liKQjn3PbgfjdwC74papeZzQUI7neXLsJJNVK5pt377Jzb5ZzLOOeywA0cbGKc8mU1syj+A/H7zrmfB4en3Xubr5zT+X2d4qb937/C6gqYhp8pI5munyuVUldAedUXlZgsPAysMLMlZhYDrgBuLXFMk8bMqs2sdmgbeDXwFL6MVwanXQn8sjQRTrqRynUrcIWZxc1sCbACeKgE8U2aoQ/DwJvw7ytM8bKamQHfBp5xzn0556Fp9d6OVM7p+r5OA6orplddAdPsM2U00/FzpVLqCijD+qKYo7vL5QZcgh9ZvhH421LHM8llW4ofEf84sG6ofEAzcBfwXHDfVOpYJ1C2H+Kb3VL4LPrdo5UL+NvgPX4WuLjU8U9CWb8HPAk8EXwwzJ0mZT0X31z6BLA2uF0y3d7bUco5Ld/X6XBTXTE164qgHKovptnnSqXUFUcoa0neVwteQERERERE5BCV2A1JRERERETGQMmCiIiIiIjkpWRBRERERETyUrIgIiIiIiJ5KVkQEREREZG8lCyI5GFmGTNbm3P7xCQ+92Ize+rIZ4qISLlTfSHTXaTUAYiUqT7n3CmlDkJERMqe6guZ1tSyIDIOZrbZzL5gZg8Ft+XB8UVmdpeZPRHcLwyOzzazW8zs8eB2dvBUYTO7wczWmdntZpYsWaFERGTSqb6Q6ULJgkh+yWHNypfnPNbpnDsT+Crwb8GxrwLfdc6dBHwfuC44fh3wO+fcycBp+JVSwS/F/jXn3PFAO/CWgpZGREQKRfWFTGtawVkkDzPrds7V5Dm+GXiFc26TmUWBnc65ZjNrwy+7ngqO73DOzTCzPcB859xAznMsBu5wzq0I9j8ORJ1z/1SEoomIyCRSfSHTnVoWRMbPjbA90jn5DORsZ9D4IRGR6Uj1hUx5ShZExu/ynPv7g+0/AlcE228H/hBs3wW8D8DMwmZWV6wgRUSk5FRfyJSn7FQkv6SZrc3Z/41zbmg6vLiZPYhPttcExz4I3GhmHwP2AO8Kjn8IuN7M3o3/Reh9wI5CBy8iIkWj+kKmNY1ZEBmHoA/qaudcW6ljERGR8qX6QqYLdUMSEREREZG81LIgIiIiIiJ5qWVBRERERETyUrIgIiIiIiJ5KVkQEREREZG8lCyIiIiIiEheShZERERERCSv/x/BJHkDOuUPBAAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 936x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s 2ms/step - loss: 18.6599 - mae: 2.9165 - mse: 18.6599\n",
      "MAE with the 0.01 learning rate: 2.9165 reached in 4 s after 64 epochs\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 936x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s 2ms/step - loss: 18.6428 - mae: 3.0764 - mse: 18.6428\n",
      "MAE with the 0.1 learning rate: 3.0764 reached in 2 s after 27 epochs\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 936x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s 2ms/step - loss: 83.5767 - mae: 6.7119 - mse: 83.5767\n",
      "MAE with the 1 learning rate: 6.7119 reached in 1 s after 13 epochs\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 936x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s 2ms/step - loss: 84.2247 - mae: 6.5172 - mse: 84.2247\n",
      "MAE with the 5 learning rate: 6.5172 reached in 4 s after 72 epochs\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 936x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "learning_rates = [0.0001, 0.001, 0.01, 0.1, 1, 5]\n",
    "\n",
    "for lr in learning_rates:\n",
    "    start_time = time.time()\n",
    "    opt = Adam(learning_rate=lr)\n",
    "    model = initialize_model()\n",
    "    model = compile_model(model, opt)  \n",
    "    \n",
    "    es = EarlyStopping(monitor='val_loss', mode='min', patience=10, min_delta=0.01, restore_best_weights=True)\n",
    "    history = model.fit(X_train, y_train, batch_size=20, epochs=500, validation_split=0.3, callbacks=[es], verbose=0)\n",
    "    #shuffle=True\n",
    "    \n",
    "    res = model.evaluate(X_test, y_test)[1]\n",
    "    end_time = time.time()\n",
    "\n",
    "    print(f'MAE with the {lr} learning rate: {res:.4f} reached in {(end_time - start_time):.0f} s after {len(history.epoch)} epochs')\n",
    "    plot_loss_mae(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. The loss\n",
    "\n",
    "⚠️ It is important to **clearly understand the different between metrics and losses**. \n",
    "\n",
    "* The `loss functions` are computed *during* the training procedure\n",
    "* The `metrics` are computed *after* training your models !\n",
    "* Some metrics can be used as loss functions too... as long as they are differentiable ! (e.g. the *MSE*)\n",
    "\n",
    "❓ **Question** ❓ Run the same neural network, once with the `mae` as the loss, and once with the `mse`.  \n",
    "\n",
    "In both case, compare `mae_train`, `mae_val`, `mse_train`, `mse_val` and conclude."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# YOUR CODE HERE\n",
    "# Building your network\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import Dense\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "def initialize_model():\n",
    "  model = models.Sequential()\n",
    "  model.add(layers.Dense(10, activation='relu',input_shape=(X_train.shape[1],)))\n",
    "  model.add(layers.Dense(7, activation='relu'))\n",
    "  # regression - no activation function in the last layer\n",
    "  model.add(layers.Dense(1))\n",
    "  model.compile(loss='mae', optimizer=optimizer,metrics=['mae', 'mse'])\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = initialize_model();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_17\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_51 (Dense)             (None, 10)                140       \n",
      "_________________________________________________________________\n",
      "dense_52 (Dense)             (None, 7)                 77        \n",
      "_________________________________________________________________\n",
      "dense_53 (Dense)             (None, 1)                 8         \n",
      "=================================================================\n",
      "Total params: 225\n",
      "Trainable params: 225\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "9/9 [==============================] - 1s 35ms/step - loss: 21.9877 - mae: 21.9877 - mse: 570.9547 - val_loss: 23.0313 - val_mae: 23.0313 - val_mse: 635.7171\n",
      "Epoch 2/10\n",
      "9/9 [==============================] - 0s 7ms/step - loss: 21.7678 - mae: 21.7678 - mse: 548.6847 - val_loss: 22.9797 - val_mae: 22.9797 - val_mse: 633.2699\n",
      "Epoch 3/10\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 21.5090 - mae: 21.5090 - mse: 545.8920 - val_loss: 22.9382 - val_mae: 22.9382 - val_mse: 631.3068\n",
      "Epoch 4/10\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 21.8838 - mae: 21.8838 - mse: 553.9406 - val_loss: 22.9021 - val_mae: 22.9021 - val_mse: 629.6025\n",
      "Epoch 5/10\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 20.8501 - mae: 20.8501 - mse: 515.8263 - val_loss: 22.8700 - val_mae: 22.8700 - val_mse: 628.0790\n",
      "Epoch 6/10\n",
      "9/9 [==============================] - 0s 20ms/step - loss: 21.4402 - mae: 21.4402 - mse: 531.7177 - val_loss: 22.8402 - val_mae: 22.8402 - val_mse: 626.6761\n",
      "Epoch 7/10\n",
      "9/9 [==============================] - 0s 13ms/step - loss: 21.4437 - mae: 21.4437 - mse: 536.1897 - val_loss: 22.8126 - val_mae: 22.8126 - val_mse: 625.3755\n",
      "Epoch 8/10\n",
      "9/9 [==============================] - 0s 11ms/step - loss: 21.4576 - mae: 21.4576 - mse: 534.9549 - val_loss: 22.7864 - val_mae: 22.7864 - val_mse: 624.1447\n",
      "Epoch 9/10\n",
      "9/9 [==============================] - 0s 12ms/step - loss: 21.1506 - mae: 21.1506 - mse: 519.8157 - val_loss: 22.7616 - val_mae: 22.7616 - val_mse: 622.9777\n",
      "Epoch 10/10\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 21.1811 - mae: 21.1811 - mse: 530.3406 - val_loss: 22.7378 - val_mae: 22.7378 - val_mse: 621.8530\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "earlystop = EarlyStopping(monitor = 'val_loss',min_delta = 0.005,patience = 15, verbose = 1,restore_best_weights = True, mode='auto')\n",
    "history=model.fit(X_train_N, y_train, epochs=10, validation_split=0.3, validation_data=(X_test, y_test), callbacks=[earlystop])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❗️ Countrary to first intuition, it can be sometimes better to use the MSE as the loss function in order to get the best MAE possible in the end!\n",
    "\n",
    "<details>\n",
    "    <summary>▶ Why?</summary>\n",
    "\n",
    "Well, even the Deep Learning research community is still trying to answer these types of questions rigorously.\n",
    "    \n",
    "One thing for sure: In Deep Learning, you will never really reach the \"global minimum\" of the true loss function (the one computed using your entire training set as one single \"batch\"). So, in your first model (minimizing the MAE loss), your global MAE minimum has clearly **not** been reached (otherwise you could never beat it). \n",
    "\n",
    "Why? It may well be that the minimization process of the second model has performed better. Maybe because the loss function \"energy map\" is \"smoother\" or more \"convex\" in the case of MSE loss? Or maybe your hyper-parameter are best suited to the MSE than to the MAE loss?\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 - 0s - loss: 22.3749 - mae: 22.3749 - mse: 583.6859\n",
      "Testing set Mean Abs Error: 22.37\n"
     ]
    }
   ],
   "source": [
    "loss, mae, mse = model.evaluate(X_test_N, y_test, verbose=2)\n",
    "\n",
    "print(\"Testing set Mean Abs Error: {:5.2f}\".format(mae))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_predictions = model.predict(X_test_N).flatten()\n",
    "train_predictions = model.predict(X_train_N).flatten()\n",
    "\n",
    "plt.scatter(y_test, test_predictions)\n",
    "plt.xlabel('True Values')\n",
    "plt.ylabel('Predictions')\n",
    "plt.axis('equal')\n",
    "plt.axis('square')\n",
    "plt.xlim([0,plt.xlim()[1]])\n",
    "plt.ylim([0,plt.ylim()[1]])\n",
    "_ = plt.plot([-100, 100], [-100, 100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "error = test_predictions - y_test\n",
    "plt.hist(error, bins = 25)\n",
    "plt.xlabel(\"Prediction Error\")\n",
    "_ = plt.ylabel(\"Count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error:  583.6858426626791\n",
      "Mean Absolute Error:  22.37489907301086\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "mse = mean_squared_error(y_test, test_predictions)\n",
    "print('Mean Squared Error: ',mse)\n",
    "mae = mean_absolute_error(y_test, test_predictions)\n",
    "print('Mean Absolute Error: ',mae)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🧪 Test your model best performance\n",
    "\n",
    "❓ Save your best model performance on the test set at `mae_test` and check it out below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>mae</th>\n",
       "      <th>mse</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_mae</th>\n",
       "      <th>val_mse</th>\n",
       "      <th>epoch</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>21.308456</td>\n",
       "      <td>21.308456</td>\n",
       "      <td>531.012695</td>\n",
       "      <td>22.840227</td>\n",
       "      <td>22.840227</td>\n",
       "      <td>626.676147</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>21.277691</td>\n",
       "      <td>21.277691</td>\n",
       "      <td>529.685303</td>\n",
       "      <td>22.812559</td>\n",
       "      <td>22.812559</td>\n",
       "      <td>625.375488</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>21.249044</td>\n",
       "      <td>21.249044</td>\n",
       "      <td>528.471863</td>\n",
       "      <td>22.786402</td>\n",
       "      <td>22.786402</td>\n",
       "      <td>624.144714</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>21.222107</td>\n",
       "      <td>21.222107</td>\n",
       "      <td>527.294067</td>\n",
       "      <td>22.761646</td>\n",
       "      <td>22.761646</td>\n",
       "      <td>622.977661</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>21.196667</td>\n",
       "      <td>21.196667</td>\n",
       "      <td>526.210999</td>\n",
       "      <td>22.737837</td>\n",
       "      <td>22.737837</td>\n",
       "      <td>621.853027</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        loss        mae         mse   val_loss    val_mae     val_mse  epoch\n",
       "5  21.308456  21.308456  531.012695  22.840227  22.840227  626.676147      5\n",
       "6  21.277691  21.277691  529.685303  22.812559  22.812559  625.375488      6\n",
       "7  21.249044  21.249044  528.471863  22.786402  22.786402  624.144714      7\n",
       "8  21.222107  21.222107  527.294067  22.761646  22.761646  622.977661      8\n",
       "9  21.196667  21.196667  526.210999  22.737837  22.737837  621.853027      9"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hist = pd.DataFrame(history.history)\n",
    "hist['epoch'] = history.epoch\n",
    "hist.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('ft.h5')\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "model = load_model('ft.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 : Save and load a model\n",
    "\n",
    "❓ **Question** ❓  Save your model using `.save_model(model, 'name_of_my_model')` method that you can find [here](https://www.tensorflow.org/api_docs/python/tf/keras/models/save_model).."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ **Question** ❓ Now, in a variable that you will call `loaded_model`, load the model you just saved thanks to `.load_model('name_of_your_model')` [(documentation here)](https://www.tensorflow.org/api_docs/python/tf/keras/models/load_model), and evaluate it on the test data to check that it gives the same result as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss : 22.3749\n",
      "Test Accuracy : 22.3749\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE\n",
    "# Evaluate the model on test data\n",
    "score = model.evaluate(X_test,y_test,verbose=0)\n",
    "print('Test Loss : {:.4f}'.format(score[0]))\n",
    "print('Test Accuracy : {:.4f}'.format(score[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Optional) `Exponential Decay` for the Optimizer's Learning Rate\n",
    "\n",
    "The next question is not essential and can be skipped as many algorithms can be run without such optimization. \n",
    "\n",
    "Instead of keeping a fixed learning rate, you can change it from one iteration to the other, with the intuition that at first, you need large learning rates, and as the neural network converges and get closer to the minimum loss value, you can decrease the value of the learning rate. This is called a **`scheduler`**. \n",
    "\n",
    "❓ **Question** ❓ Use the [Exponential Decay Scheduler](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/schedules/ExponentialDecay) in the `adam` optimizer and run it on the previous data. Start with the following:\n",
    "\n",
    "```python\n",
    "initial_learning_rate = 0.001 # start with default ADAM value\n",
    "\n",
    "lr_schedule = ExponentialDecay(\n",
    "    # Every 5000 iterations, multiply the learning rate by 0.7\n",
    "    initial_learning_rate, decay_steps=5000, decay_rate=0.7,\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5000\n",
      "\n",
      "Epoch 00001: LearningRateScheduler reducing learning rate to 0.01.\n",
      "11/11 [==============================] - 1s 56ms/step - loss: 4971.1802 - mae: 61.9902 - mse: 4971.1802 - val_loss: 2999.6919 - val_mae: 48.8938 - val_mse: 2999.6919\n",
      "Epoch 2/5000\n",
      "\n",
      "Epoch 00002: LearningRateScheduler reducing learning rate to 0.009048374180359595.\n",
      "11/11 [==============================] - 0s 32ms/step - loss: 1298.9586 - mae: 26.2493 - mse: 1298.9586 - val_loss: 522.9866 - val_mae: 19.1673 - val_mse: 522.9866\n",
      "Epoch 3/5000\n",
      "\n",
      "Epoch 00003: LearningRateScheduler reducing learning rate to 0.008187307530779819.\n",
      "11/11 [==============================] - 0s 27ms/step - loss: 704.9385 - mae: 21.7052 - mse: 704.9385 - val_loss: 409.5523 - val_mae: 16.3942 - val_mse: 409.5523\n",
      "Epoch 4/5000\n",
      "\n",
      "Epoch 00004: LearningRateScheduler reducing learning rate to 0.007408182206817179.\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 287.1128 - mae: 12.7402 - mse: 287.1128 - val_loss: 230.4807 - val_mae: 11.7146 - val_mse: 230.4807\n",
      "Epoch 5/5000\n",
      "\n",
      "Epoch 00005: LearningRateScheduler reducing learning rate to 0.006703200460356393.\n",
      "11/11 [==============================] - 0s 16ms/step - loss: 268.4409 - mae: 12.7885 - mse: 268.4409 - val_loss: 193.2314 - val_mae: 10.9624 - val_mse: 193.2314\n",
      "Epoch 6/5000\n",
      "\n",
      "Epoch 00006: LearningRateScheduler reducing learning rate to 0.006065306597126334.\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 204.5859 - mae: 11.0188 - mse: 204.5859 - val_loss: 184.4960 - val_mae: 11.0848 - val_mse: 184.4960\n",
      "Epoch 7/5000\n",
      "\n",
      "Epoch 00007: LearningRateScheduler reducing learning rate to 0.005488116360940264.\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 193.5471 - mae: 10.5228 - mse: 193.5471 - val_loss: 166.9614 - val_mae: 10.4318 - val_mse: 166.9614\n",
      "Epoch 8/5000\n",
      "\n",
      "Epoch 00008: LearningRateScheduler reducing learning rate to 0.004965853037914095.\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 179.3576 - mae: 10.1642 - mse: 179.3576 - val_loss: 160.0800 - val_mae: 10.1552 - val_mse: 160.0800\n",
      "Epoch 9/5000\n",
      "\n",
      "Epoch 00009: LearningRateScheduler reducing learning rate to 0.004493289641172216.\n",
      "11/11 [==============================] - 0s 26ms/step - loss: 172.4001 - mae: 9.9314 - mse: 172.4001 - val_loss: 154.7965 - val_mae: 9.9274 - val_mse: 154.7965\n",
      "Epoch 10/5000\n",
      "\n",
      "Epoch 00010: LearningRateScheduler reducing learning rate to 0.004065696597405992.\n",
      "11/11 [==============================] - 0s 16ms/step - loss: 166.0974 - mae: 9.7178 - mse: 166.0974 - val_loss: 150.7223 - val_mae: 9.8513 - val_mse: 150.7223\n",
      "Epoch 11/5000\n",
      "\n",
      "Epoch 00011: LearningRateScheduler reducing learning rate to 0.0036787944117144234.\n",
      "11/11 [==============================] - 0s 17ms/step - loss: 160.3121 - mae: 9.5298 - mse: 160.3121 - val_loss: 146.0142 - val_mae: 9.6089 - val_mse: 146.0142\n",
      "Epoch 12/5000\n",
      "\n",
      "Epoch 00012: LearningRateScheduler reducing learning rate to 0.003328710836980796.\n",
      "11/11 [==============================] - 0s 18ms/step - loss: 155.3462 - mae: 9.3395 - mse: 155.3462 - val_loss: 142.9915 - val_mae: 9.4889 - val_mse: 142.9915\n",
      "Epoch 13/5000\n",
      "\n",
      "Epoch 00013: LearningRateScheduler reducing learning rate to 0.0030119421191220205.\n",
      "11/11 [==============================] - 0s 21ms/step - loss: 150.7302 - mae: 9.1754 - mse: 150.7302 - val_loss: 140.4113 - val_mae: 9.3615 - val_mse: 140.4113\n",
      "Epoch 14/5000\n",
      "\n",
      "Epoch 00014: LearningRateScheduler reducing learning rate to 0.002725317930340126.\n",
      "11/11 [==============================] - 0s 25ms/step - loss: 147.3723 - mae: 9.0735 - mse: 147.3723 - val_loss: 139.0736 - val_mae: 9.4261 - val_mse: 139.0736\n",
      "Epoch 15/5000\n",
      "\n",
      "Epoch 00015: LearningRateScheduler reducing learning rate to 0.0024659696394160645.\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 143.4297 - mae: 8.9684 - mse: 143.4297 - val_loss: 137.2880 - val_mae: 9.3319 - val_mse: 137.2880\n",
      "Epoch 16/5000\n",
      "\n",
      "Epoch 00016: LearningRateScheduler reducing learning rate to 0.0022313016014842983.\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 140.6933 - mae: 8.8516 - mse: 140.6933 - val_loss: 135.4426 - val_mae: 9.2147 - val_mse: 135.4426\n",
      "Epoch 17/5000\n",
      "\n",
      "Epoch 00017: LearningRateScheduler reducing learning rate to 0.002018965179946554.\n",
      "11/11 [==============================] - 0s 15ms/step - loss: 138.3098 - mae: 8.7392 - mse: 138.3098 - val_loss: 133.7795 - val_mae: 9.0757 - val_mse: 133.7795\n",
      "Epoch 18/5000\n",
      "\n",
      "Epoch 00018: LearningRateScheduler reducing learning rate to 0.001826835240527346.\n",
      "11/11 [==============================] - 0s 15ms/step - loss: 136.0905 - mae: 8.6267 - mse: 136.0905 - val_loss: 132.3043 - val_mae: 8.9692 - val_mse: 132.3043\n",
      "Epoch 19/5000\n",
      "\n",
      "Epoch 00019: LearningRateScheduler reducing learning rate to 0.0016529888822158654.\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 133.9165 - mae: 8.5492 - mse: 133.9165 - val_loss: 131.1007 - val_mae: 8.9653 - val_mse: 131.1007\n",
      "Epoch 20/5000\n",
      "\n",
      "Epoch 00020: LearningRateScheduler reducing learning rate to 0.0014956861922263505.\n",
      "11/11 [==============================] - 0s 16ms/step - loss: 131.9898 - mae: 8.5235 - mse: 131.9898 - val_loss: 130.1600 - val_mae: 8.9594 - val_mse: 130.1600\n",
      "Epoch 21/5000\n",
      "\n",
      "Epoch 00021: LearningRateScheduler reducing learning rate to 0.0013533528323661271.\n",
      "11/11 [==============================] - 0s 20ms/step - loss: 130.4841 - mae: 8.4994 - mse: 130.4841 - val_loss: 129.4704 - val_mae: 8.9536 - val_mse: 129.4704\n",
      "Epoch 22/5000\n",
      "\n",
      "Epoch 00022: LearningRateScheduler reducing learning rate to 0.001224564282529819.\n",
      "11/11 [==============================] - 0s 27ms/step - loss: 129.4246 - mae: 8.4865 - mse: 129.4246 - val_loss: 128.5876 - val_mae: 8.9063 - val_mse: 128.5876\n",
      "Epoch 23/5000\n",
      "\n",
      "Epoch 00023: LearningRateScheduler reducing learning rate to 0.0011080315836233387.\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 128.0216 - mae: 8.4099 - mse: 128.0216 - val_loss: 127.5491 - val_mae: 8.8098 - val_mse: 127.5491\n",
      "Epoch 24/5000\n",
      "\n",
      "Epoch 00024: LearningRateScheduler reducing learning rate to 0.001002588437228037.\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 126.8966 - mae: 8.3306 - mse: 126.8966 - val_loss: 126.8223 - val_mae: 8.7441 - val_mse: 126.8223\n",
      "Epoch 25/5000\n",
      "\n",
      "Epoch 00025: LearningRateScheduler reducing learning rate to 0.0009071795328941247.\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 125.9010 - mae: 8.2855 - mse: 125.9010 - val_loss: 126.2804 - val_mae: 8.7202 - val_mse: 126.2804\n",
      "Epoch 26/5000\n",
      "\n",
      "Epoch 00026: LearningRateScheduler reducing learning rate to 0.0008208499862389881.\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 125.0946 - mae: 8.2353 - mse: 125.0946 - val_loss: 125.7274 - val_mae: 8.6665 - val_mse: 125.7274\n",
      "Epoch 27/5000\n",
      "\n",
      "Epoch 00027: LearningRateScheduler reducing learning rate to 0.0007427357821433388.\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 124.3039 - mae: 8.2006 - mse: 124.3039 - val_loss: 125.2494 - val_mae: 8.6551 - val_mse: 125.2494\n",
      "Epoch 28/5000\n",
      "\n",
      "Epoch 00028: LearningRateScheduler reducing learning rate to 0.0006720551273974976.\n",
      "11/11 [==============================] - 0s 32ms/step - loss: 123.6289 - mae: 8.1846 - mse: 123.6289 - val_loss: 124.8193 - val_mae: 8.6404 - val_mse: 124.8193\n",
      "Epoch 29/5000\n",
      "\n",
      "Epoch 00029: LearningRateScheduler reducing learning rate to 0.0006081006262521795.\n",
      "11/11 [==============================] - 0s 18ms/step - loss: 122.8809 - mae: 8.1511 - mse: 122.8809 - val_loss: 124.3825 - val_mae: 8.6025 - val_mse: 124.3825\n",
      "Epoch 30/5000\n",
      "\n",
      "Epoch 00030: LearningRateScheduler reducing learning rate to 0.0005502322005640721.\n",
      "11/11 [==============================] - 0s 23ms/step - loss: 122.3595 - mae: 8.1266 - mse: 122.3595 - val_loss: 124.0602 - val_mae: 8.5979 - val_mse: 124.0602\n",
      "Epoch 31/5000\n",
      "\n",
      "Epoch 00031: LearningRateScheduler reducing learning rate to 0.0004978706836786395.\n",
      "11/11 [==============================] - 0s 48ms/step - loss: 121.7021 - mae: 8.1096 - mse: 121.7021 - val_loss: 123.7991 - val_mae: 8.5920 - val_mse: 123.7991\n",
      "Epoch 32/5000\n",
      "\n",
      "Epoch 00032: LearningRateScheduler reducing learning rate to 0.000450492023935578.\n",
      "11/11 [==============================] - 0s 31ms/step - loss: 121.1581 - mae: 8.1061 - mse: 121.1581 - val_loss: 123.5149 - val_mae: 8.5923 - val_mse: 123.5149\n",
      "Epoch 33/5000\n",
      "\n",
      "Epoch 00033: LearningRateScheduler reducing learning rate to 0.0004076220397836621.\n",
      "11/11 [==============================] - 0s 30ms/step - loss: 120.6243 - mae: 8.0950 - mse: 120.6243 - val_loss: 123.1351 - val_mae: 8.5688 - val_mse: 123.1351\n",
      "Epoch 34/5000\n",
      "\n",
      "Epoch 00034: LearningRateScheduler reducing learning rate to 0.00036883167401239995.\n",
      "11/11 [==============================] - 0s 31ms/step - loss: 120.2576 - mae: 8.0860 - mse: 120.2576 - val_loss: 122.8245 - val_mae: 8.5603 - val_mse: 122.8245\n",
      "Epoch 35/5000\n",
      "\n",
      "Epoch 00035: LearningRateScheduler reducing learning rate to 0.0003337326996032607.\n",
      "11/11 [==============================] - 0s 18ms/step - loss: 119.7083 - mae: 8.0576 - mse: 119.7083 - val_loss: 122.4894 - val_mae: 8.5292 - val_mse: 122.4894\n",
      "Epoch 36/5000\n",
      "\n",
      "Epoch 00036: LearningRateScheduler reducing learning rate to 0.00030197383422318503.\n",
      "11/11 [==============================] - 0s 25ms/step - loss: 119.3858 - mae: 8.0285 - mse: 119.3858 - val_loss: 122.2107 - val_mae: 8.4993 - val_mse: 122.2107\n",
      "Epoch 37/5000\n",
      "\n",
      "Epoch 00037: LearningRateScheduler reducing learning rate to 0.0002732372244729256.\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 119.0894 - mae: 8.0021 - mse: 119.0894 - val_loss: 122.0087 - val_mae: 8.4772 - val_mse: 122.0087\n",
      "Epoch 38/5000\n",
      "\n",
      "Epoch 00038: LearningRateScheduler reducing learning rate to 0.00024723526470339386.\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 118.7781 - mae: 7.9857 - mse: 118.7781 - val_loss: 121.8336 - val_mae: 8.4683 - val_mse: 121.8336\n",
      "Epoch 39/5000\n",
      "\n",
      "Epoch 00039: LearningRateScheduler reducing learning rate to 0.00022370771856165591.\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 118.4961 - mae: 7.9704 - mse: 118.4961 - val_loss: 121.6539 - val_mae: 8.4501 - val_mse: 121.6539\n",
      "Epoch 40/5000\n",
      "\n",
      "Epoch 00040: LearningRateScheduler reducing learning rate to 0.0002024191144580438.\n",
      "11/11 [==============================] - 0s 35ms/step - loss: 118.2762 - mae: 7.9561 - mse: 118.2762 - val_loss: 121.5090 - val_mae: 8.4397 - val_mse: 121.5090\n",
      "Epoch 41/5000\n",
      "\n",
      "Epoch 00041: LearningRateScheduler reducing learning rate to 0.00018315638888734178.\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 118.0354 - mae: 7.9469 - mse: 118.0354 - val_loss: 121.3789 - val_mae: 8.4340 - val_mse: 121.3789\n",
      "Epoch 42/5000\n",
      "\n",
      "Epoch 00042: LearningRateScheduler reducing learning rate to 0.00016572675401761236.\n",
      "11/11 [==============================] - 0s 24ms/step - loss: 117.8207 - mae: 7.9412 - mse: 117.8207 - val_loss: 121.2519 - val_mae: 8.4308 - val_mse: 121.2519\n",
      "Epoch 43/5000\n",
      "\n",
      "Epoch 00043: LearningRateScheduler reducing learning rate to 0.00014995576820477704.\n",
      "11/11 [==============================] - 0s 17ms/step - loss: 117.6200 - mae: 7.9385 - mse: 117.6200 - val_loss: 121.1464 - val_mae: 8.4273 - val_mse: 121.1464\n",
      "Epoch 44/5000\n",
      "\n",
      "Epoch 00044: LearningRateScheduler reducing learning rate to 0.00013568559012200934.\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 117.4544 - mae: 7.9308 - mse: 117.4544 - val_loss: 121.0393 - val_mae: 8.4184 - val_mse: 121.0393\n",
      "Epoch 45/5000\n",
      "\n",
      "Epoch 00045: LearningRateScheduler reducing learning rate to 0.00012277339903068437.\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 117.2615 - mae: 7.9228 - mse: 117.2615 - val_loss: 120.9579 - val_mae: 8.4161 - val_mse: 120.9579\n",
      "Epoch 46/5000\n",
      "\n",
      "Epoch 00046: LearningRateScheduler reducing learning rate to 0.00011108996538242307.\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 117.0843 - mae: 7.9213 - mse: 117.0843 - val_loss: 120.8699 - val_mae: 8.4215 - val_mse: 120.8699\n",
      "Epoch 47/5000\n",
      "\n",
      "Epoch 00047: LearningRateScheduler reducing learning rate to 0.00010051835744633576.\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 116.9906 - mae: 7.9336 - mse: 116.9906 - val_loss: 120.8467 - val_mae: 8.4354 - val_mse: 120.8467\n",
      "Epoch 48/5000\n",
      "\n",
      "Epoch 00048: LearningRateScheduler reducing learning rate to 9.095277101695816e-05.\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 116.8584 - mae: 7.9350 - mse: 116.8584 - val_loss: 120.7859 - val_mae: 8.4348 - val_mse: 120.7859\n",
      "Epoch 49/5000\n",
      "\n",
      "Epoch 00049: LearningRateScheduler reducing learning rate to 8.229747049020023e-05.\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 116.7457 - mae: 7.9374 - mse: 116.7457 - val_loss: 120.7376 - val_mae: 8.4410 - val_mse: 120.7376\n",
      "Epoch 50/5000\n",
      "\n",
      "Epoch 00050: LearningRateScheduler reducing learning rate to 7.446583070924338e-05.\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 116.6603 - mae: 7.9387 - mse: 116.6603 - val_loss: 120.6719 - val_mae: 8.4374 - val_mse: 120.6719\n",
      "Epoch 51/5000\n",
      "\n",
      "Epoch 00051: LearningRateScheduler reducing learning rate to 6.737946999085467e-05.\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 116.5661 - mae: 7.9346 - mse: 116.5661 - val_loss: 120.6227 - val_mae: 8.4350 - val_mse: 120.6227\n",
      "Epoch 52/5000\n",
      "\n",
      "Epoch 00052: LearningRateScheduler reducing learning rate to 6.096746565515633e-05.\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 116.4891 - mae: 7.9334 - mse: 116.4891 - val_loss: 120.5768 - val_mae: 8.4341 - val_mse: 120.5768\n",
      "Epoch 53/5000\n",
      "\n",
      "Epoch 00053: LearningRateScheduler reducing learning rate to 5.5165644207607714e-05.\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 116.4147 - mae: 7.9313 - mse: 116.4147 - val_loss: 120.5299 - val_mae: 8.4314 - val_mse: 120.5299\n",
      "Epoch 54/5000\n",
      "\n",
      "Epoch 00054: LearningRateScheduler reducing learning rate to 4.991593906910213e-05.\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 116.3587 - mae: 7.9309 - mse: 116.3587 - val_loss: 120.5095 - val_mae: 8.4326 - val_mse: 120.5095\n",
      "Epoch 55/5000\n",
      "\n",
      "Epoch 00055: LearningRateScheduler reducing learning rate to 4.516580942612666e-05.\n",
      "11/11 [==============================] - 0s 27ms/step - loss: 116.3033 - mae: 7.9287 - mse: 116.3033 - val_loss: 120.4673 - val_mae: 8.4285 - val_mse: 120.4673\n",
      "Epoch 56/5000\n",
      "\n",
      "Epoch 00056: LearningRateScheduler reducing learning rate to 4.086771438464067e-05.\n",
      "11/11 [==============================] - 0s 17ms/step - loss: 116.2438 - mae: 7.9240 - mse: 116.2438 - val_loss: 120.4250 - val_mae: 8.4241 - val_mse: 120.4250\n",
      "Epoch 57/5000\n",
      "\n",
      "Epoch 00057: LearningRateScheduler reducing learning rate to 3.697863716482929e-05.\n",
      "11/11 [==============================] - 0s 15ms/step - loss: 116.1940 - mae: 7.9185 - mse: 116.1940 - val_loss: 120.3726 - val_mae: 8.4167 - val_mse: 120.3726\n",
      "Epoch 58/5000\n",
      "\n",
      "Epoch 00058: LearningRateScheduler reducing learning rate to 3.3459654574712724e-05.\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 116.1288 - mae: 7.9121 - mse: 116.1288 - val_loss: 120.3317 - val_mae: 8.4123 - val_mse: 120.3317\n",
      "Epoch 59/5000\n",
      "\n",
      "Epoch 00059: LearningRateScheduler reducing learning rate to 3.027554745375813e-05.\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 116.0888 - mae: 7.9103 - mse: 116.0888 - val_loss: 120.3130 - val_mae: 8.4125 - val_mse: 120.3130\n",
      "Epoch 60/5000\n",
      "\n",
      "Epoch 00060: LearningRateScheduler reducing learning rate to 2.7394448187683685e-05.\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 116.0445 - mae: 7.9083 - mse: 116.0445 - val_loss: 120.2834 - val_mae: 8.4093 - val_mse: 120.2834\n",
      "Epoch 61/5000\n",
      "\n",
      "Epoch 00061: LearningRateScheduler reducing learning rate to 2.4787521766663585e-05.\n",
      "11/11 [==============================] - 0s 15ms/step - loss: 116.0073 - mae: 7.9053 - mse: 116.0073 - val_loss: 120.2586 - val_mae: 8.4066 - val_mse: 120.2586\n",
      "Epoch 62/5000\n",
      "\n",
      "Epoch 00062: LearningRateScheduler reducing learning rate to 2.2428677194858013e-05.\n",
      "11/11 [==============================] - 0s 15ms/step - loss: 115.9684 - mae: 7.9021 - mse: 115.9684 - val_loss: 120.2289 - val_mae: 8.4027 - val_mse: 120.2289\n",
      "Epoch 63/5000\n",
      "\n",
      "Epoch 00063: LearningRateScheduler reducing learning rate to 2.029430636295734e-05.\n",
      "11/11 [==============================] - 0s 25ms/step - loss: 115.9379 - mae: 7.8988 - mse: 115.9379 - val_loss: 120.2038 - val_mae: 8.3993 - val_mse: 120.2038\n",
      "Epoch 64/5000\n",
      "\n",
      "Epoch 00064: LearningRateScheduler reducing learning rate to 1.8363047770289057e-05.\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 115.9119 - mae: 7.8953 - mse: 115.9119 - val_loss: 120.1819 - val_mae: 8.3959 - val_mse: 120.1819\n",
      "Epoch 65/5000\n",
      "\n",
      "Epoch 00065: LearningRateScheduler reducing learning rate to 1.661557273173934e-05.\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 115.8898 - mae: 7.8932 - mse: 115.8898 - val_loss: 120.1667 - val_mae: 8.3945 - val_mse: 120.1667\n",
      "Epoch 66/5000\n",
      "\n",
      "Epoch 00066: LearningRateScheduler reducing learning rate to 1.5034391929775723e-05.\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 115.8723 - mae: 7.8922 - mse: 115.8723 - val_loss: 120.1544 - val_mae: 8.3939 - val_mse: 120.1544\n",
      "Epoch 67/5000\n",
      "\n",
      "Epoch 00067: LearningRateScheduler reducing learning rate to 1.3603680375478928e-05.\n",
      "11/11 [==============================] - 0s 17ms/step - loss: 115.8494 - mae: 7.8916 - mse: 115.8494 - val_loss: 120.1425 - val_mae: 8.3938 - val_mse: 120.1425\n",
      "Epoch 68/5000\n",
      "\n",
      "Epoch 00068: LearningRateScheduler reducing learning rate to 1.230911902673481e-05.\n",
      "11/11 [==============================] - 0s 23ms/step - loss: 115.8323 - mae: 7.8917 - mse: 115.8323 - val_loss: 120.1316 - val_mae: 8.3938 - val_mse: 120.1316\n",
      "Epoch 69/5000\n",
      "\n",
      "Epoch 00069: LearningRateScheduler reducing learning rate to 1.1137751478448024e-05.\n",
      "11/11 [==============================] - 0s 32ms/step - loss: 115.8146 - mae: 7.8912 - mse: 115.8146 - val_loss: 120.1214 - val_mae: 8.3932 - val_mse: 120.1214\n",
      "Epoch 70/5000\n",
      "\n",
      "Epoch 00070: LearningRateScheduler reducing learning rate to 1.0077854290485104e-05.\n",
      "11/11 [==============================] - 0s 18ms/step - loss: 115.7973 - mae: 7.8903 - mse: 115.7973 - val_loss: 120.1109 - val_mae: 8.3924 - val_mse: 120.1109\n",
      "Epoch 71/5000\n",
      "\n",
      "Epoch 00071: LearningRateScheduler reducing learning rate to 9.118819655545163e-06.\n",
      "11/11 [==============================] - 0s 20ms/step - loss: 115.7852 - mae: 7.8899 - mse: 115.7852 - val_loss: 120.1046 - val_mae: 8.3922 - val_mse: 120.1046\n",
      "Epoch 72/5000\n",
      "\n",
      "Epoch 00072: LearningRateScheduler reducing learning rate to 8.25104923265904e-06.\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 115.7689 - mae: 7.8892 - mse: 115.7689 - val_loss: 120.0966 - val_mae: 8.3918 - val_mse: 120.0966\n",
      "Epoch 73/5000\n",
      "\n",
      "Epoch 00073: LearningRateScheduler reducing learning rate to 7.465858083766792e-06.\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 115.7563 - mae: 7.8890 - mse: 115.7563 - val_loss: 120.0894 - val_mae: 8.3917 - val_mse: 120.0894\n",
      "Epoch 74/5000\n",
      "\n",
      "Epoch 00074: LearningRateScheduler reducing learning rate to 6.755387751938438e-06.\n",
      "11/11 [==============================] - 0s 21ms/step - loss: 115.7446 - mae: 7.8885 - mse: 115.7446 - val_loss: 120.0801 - val_mae: 8.3908 - val_mse: 120.0801\n",
      "Epoch 75/5000\n",
      "\n",
      "Epoch 00075: LearningRateScheduler reducing learning rate to 6.112527611295723e-06.\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 115.7353 - mae: 7.8874 - mse: 115.7353 - val_loss: 120.0707 - val_mae: 8.3894 - val_mse: 120.0707\n",
      "Epoch 76/5000\n",
      "\n",
      "Epoch 00076: LearningRateScheduler reducing learning rate to 5.530843701478336e-06.\n",
      "11/11 [==============================] - 0s 25ms/step - loss: 115.7241 - mae: 7.8864 - mse: 115.7241 - val_loss: 120.0650 - val_mae: 8.3889 - val_mse: 120.0650\n",
      "Epoch 77/5000\n",
      "\n",
      "Epoch 00077: LearningRateScheduler reducing learning rate to 5.004514334406104e-06.\n",
      "11/11 [==============================] - 0s 20ms/step - loss: 115.7136 - mae: 7.8861 - mse: 115.7136 - val_loss: 120.0608 - val_mae: 8.3890 - val_mse: 120.0608\n",
      "Epoch 78/5000\n",
      "\n",
      "Epoch 00078: LearningRateScheduler reducing learning rate to 4.528271828867969e-06.\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 115.7056 - mae: 7.8861 - mse: 115.7056 - val_loss: 120.0567 - val_mae: 8.3891 - val_mse: 120.0567\n",
      "Epoch 79/5000\n",
      "\n",
      "Epoch 00079: LearningRateScheduler reducing learning rate to 4.097349789797864e-06.\n",
      "11/11 [==============================] - 0s 16ms/step - loss: 115.6986 - mae: 7.8865 - mse: 115.6986 - val_loss: 120.0554 - val_mae: 8.3898 - val_mse: 120.0554\n",
      "Epoch 80/5000\n",
      "\n",
      "Epoch 00080: LearningRateScheduler reducing learning rate to 3.707435404590882e-06.\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 115.7009 - mae: 7.8874 - mse: 115.7009 - val_loss: 120.0565 - val_mae: 8.3910 - val_mse: 120.0565\n",
      "Epoch 81/5000\n",
      "\n",
      "Epoch 00081: LearningRateScheduler reducing learning rate to 3.3546262790251185e-06.\n",
      "11/11 [==============================] - 0s 22ms/step - loss: 115.6904 - mae: 7.8879 - mse: 115.6904 - val_loss: 120.0554 - val_mae: 8.3915 - val_mse: 120.0554\n",
      "Epoch 82/5000\n",
      "\n",
      "Epoch 00082: LearningRateScheduler reducing learning rate to 3.035391380788668e-06.\n",
      "11/11 [==============================] - 0s 18ms/step - loss: 115.6863 - mae: 7.8880 - mse: 115.6863 - val_loss: 120.0526 - val_mae: 8.3913 - val_mse: 120.0526\n",
      "Epoch 83/5000\n",
      "\n",
      "Epoch 00083: LearningRateScheduler reducing learning rate to 2.7465356997214204e-06.\n",
      "11/11 [==============================] - 0s 17ms/step - loss: 115.6816 - mae: 7.8878 - mse: 115.6816 - val_loss: 120.0495 - val_mae: 8.3909 - val_mse: 120.0495\n",
      "Epoch 84/5000\n",
      "\n",
      "Epoch 00084: LearningRateScheduler reducing learning rate to 2.4851682710795183e-06.\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 115.6773 - mae: 7.8874 - mse: 115.6773 - val_loss: 120.0465 - val_mae: 8.3906 - val_mse: 120.0465\n",
      "Epoch 85/5000\n",
      "\n",
      "Epoch 00085: LearningRateScheduler reducing learning rate to 2.248673241788482e-06.\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 115.6738 - mae: 7.8872 - mse: 115.6738 - val_loss: 120.0443 - val_mae: 8.3905 - val_mse: 120.0443\n",
      "Epoch 86/5000\n",
      "\n",
      "Epoch 00086: LearningRateScheduler reducing learning rate to 2.0346836901064417e-06.\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 115.6702 - mae: 7.8870 - mse: 115.6702 - val_loss: 120.0418 - val_mae: 8.3902 - val_mse: 120.0418\n",
      "Epoch 87/5000\n",
      "\n",
      "Epoch 00087: LearningRateScheduler reducing learning rate to 1.841057936675792e-06.\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 115.6664 - mae: 7.8867 - mse: 115.6664 - val_loss: 120.0398 - val_mae: 8.3901 - val_mse: 120.0398\n",
      "Epoch 88/5000\n",
      "\n",
      "Epoch 00088: LearningRateScheduler reducing learning rate to 1.6658581098763324e-06.\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 115.6638 - mae: 7.8867 - mse: 115.6638 - val_loss: 120.0380 - val_mae: 8.3900 - val_mse: 120.0380\n",
      "Epoch 89/5000\n",
      "\n",
      "Epoch 00089: LearningRateScheduler reducing learning rate to 1.507330750954765e-06.\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 115.6605 - mae: 7.8863 - mse: 115.6605 - val_loss: 120.0353 - val_mae: 8.3896 - val_mse: 120.0353\n",
      "Epoch 90/5000\n",
      "\n",
      "Epoch 00090: LearningRateScheduler reducing learning rate to 1.363889264820114e-06.\n",
      "11/11 [==============================] - 0s 15ms/step - loss: 115.6573 - mae: 7.8861 - mse: 115.6573 - val_loss: 120.0332 - val_mae: 8.3893 - val_mse: 120.0332\n",
      "Epoch 91/5000\n",
      "\n",
      "Epoch 00091: LearningRateScheduler reducing learning rate to 1.2340980408667956e-06.\n",
      "11/11 [==============================] - 0s 15ms/step - loss: 115.6554 - mae: 7.8858 - mse: 115.6554 - val_loss: 120.0313 - val_mae: 8.3891 - val_mse: 120.0313\n",
      "Epoch 92/5000\n",
      "\n",
      "Epoch 00092: LearningRateScheduler reducing learning rate to 1.1166580849011478e-06.\n",
      "11/11 [==============================] - 0s 22ms/step - loss: 115.6533 - mae: 7.8856 - mse: 115.6533 - val_loss: 120.0295 - val_mae: 8.3888 - val_mse: 120.0295\n",
      "Epoch 93/5000\n",
      "\n",
      "Epoch 00093: LearningRateScheduler reducing learning rate to 1.0103940183709325e-06.\n",
      "11/11 [==============================] - 0s 18ms/step - loss: 115.6511 - mae: 7.8853 - mse: 115.6511 - val_loss: 120.0276 - val_mae: 8.3886 - val_mse: 120.0276\n",
      "Epoch 94/5000\n",
      "\n",
      "Epoch 00094: LearningRateScheduler reducing learning rate to 9.142423147817327e-07.\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 115.6487 - mae: 7.8851 - mse: 115.6487 - val_loss: 120.0260 - val_mae: 8.3884 - val_mse: 120.0260\n",
      "Epoch 95/5000\n",
      "\n",
      "Epoch 00095: LearningRateScheduler reducing learning rate to 8.272406555663223e-07.\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 115.6473 - mae: 7.8849 - mse: 115.6473 - val_loss: 120.0245 - val_mae: 8.3881 - val_mse: 120.0245\n",
      "Epoch 96/5000\n",
      "\n",
      "Epoch 00096: LearningRateScheduler reducing learning rate to 7.48518298877006e-07.\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 115.6455 - mae: 7.8848 - mse: 115.6455 - val_loss: 120.0236 - val_mae: 8.3881 - val_mse: 120.0236\n",
      "Epoch 97/5000\n",
      "\n",
      "Epoch 00097: LearningRateScheduler reducing learning rate to 6.772873649085378e-07.\n",
      "11/11 [==============================] - 0s 22ms/step - loss: 115.6441 - mae: 7.8847 - mse: 115.6441 - val_loss: 120.0228 - val_mae: 8.3880 - val_mse: 120.0228\n",
      "Epoch 98/5000\n",
      "\n",
      "Epoch 00098: LearningRateScheduler reducing learning rate to 6.128349505322203e-07.\n",
      "11/11 [==============================] - 0s 18ms/step - loss: 115.6428 - mae: 7.8846 - mse: 115.6428 - val_loss: 120.0218 - val_mae: 8.3879 - val_mse: 120.0218\n",
      "Epoch 99/5000\n",
      "\n",
      "Epoch 00099: LearningRateScheduler reducing learning rate to 5.545159943217694e-07.\n",
      "11/11 [==============================] - 0s 16ms/step - loss: 115.6417 - mae: 7.8846 - mse: 115.6417 - val_loss: 120.0211 - val_mae: 8.3879 - val_mse: 120.0211\n",
      "Epoch 100/5000\n",
      "\n",
      "Epoch 00100: LearningRateScheduler reducing learning rate to 5.017468205617528e-07.\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 115.6405 - mae: 7.8846 - mse: 115.6405 - val_loss: 120.0205 - val_mae: 8.3879 - val_mse: 120.0205\n",
      "Epoch 101/5000\n",
      "\n",
      "Epoch 00101: LearningRateScheduler reducing learning rate to 4.5399929762484854e-07.\n",
      "11/11 [==============================] - 0s 15ms/step - loss: 115.6396 - mae: 7.8845 - mse: 115.6396 - val_loss: 120.0198 - val_mae: 8.3878 - val_mse: 120.0198\n",
      "Epoch 102/5000\n",
      "\n",
      "Epoch 00102: LearningRateScheduler reducing learning rate to 4.1079555225300653e-07.\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 115.6388 - mae: 7.8845 - mse: 115.6388 - val_loss: 120.0195 - val_mae: 8.3878 - val_mse: 120.0195\n",
      "Epoch 103/5000\n",
      "\n",
      "Epoch 00103: LearningRateScheduler reducing learning rate to 3.7170318684126665e-07.\n",
      "11/11 [==============================] - 0s 15ms/step - loss: 115.6380 - mae: 7.8845 - mse: 115.6380 - val_loss: 120.0191 - val_mae: 8.3878 - val_mse: 120.0191\n",
      "Epoch 104/5000\n",
      "\n",
      "Epoch 00104: LearningRateScheduler reducing learning rate to 3.3633095185718967e-07.\n",
      "11/11 [==============================] - 0s 31ms/step - loss: 115.6374 - mae: 7.8844 - mse: 115.6374 - val_loss: 120.0186 - val_mae: 8.3878 - val_mse: 120.0186\n",
      "Epoch 105/5000\n",
      "\n",
      "Epoch 00105: LearningRateScheduler reducing learning rate to 3.0432483008403627e-07.\n",
      "11/11 [==============================] - 0s 27ms/step - loss: 115.6367 - mae: 7.8844 - mse: 115.6367 - val_loss: 120.0181 - val_mae: 8.3877 - val_mse: 120.0181\n",
      "Epoch 106/5000\n",
      "\n",
      "Epoch 00106: LearningRateScheduler reducing learning rate to 2.753644934974716e-07.\n",
      "11/11 [==============================] - 0s 26ms/step - loss: 115.6362 - mae: 7.8844 - mse: 115.6362 - val_loss: 120.0178 - val_mae: 8.3877 - val_mse: 120.0178\n",
      "Epoch 107/5000\n",
      "\n",
      "Epoch 00107: LearningRateScheduler reducing learning rate to 2.491600973150316e-07.\n",
      "11/11 [==============================] - 0s 27ms/step - loss: 115.6357 - mae: 7.8843 - mse: 115.6357 - val_loss: 120.0175 - val_mae: 8.3877 - val_mse: 120.0175\n",
      "Epoch 108/5000\n",
      "\n",
      "Epoch 00108: LearningRateScheduler reducing learning rate to 2.254493791321217e-07.\n",
      "11/11 [==============================] - 0s 18ms/step - loss: 115.6353 - mae: 7.8843 - mse: 115.6353 - val_loss: 120.0173 - val_mae: 8.3877 - val_mse: 120.0173\n",
      "Epoch 109/5000\n",
      "\n",
      "Epoch 00109: LearningRateScheduler reducing learning rate to 2.0399503411171924e-07.\n",
      "11/11 [==============================] - 0s 24ms/step - loss: 115.6349 - mae: 7.8843 - mse: 115.6349 - val_loss: 120.0170 - val_mae: 8.3876 - val_mse: 120.0170\n",
      "Epoch 110/5000\n",
      "\n",
      "Epoch 00110: LearningRateScheduler reducing learning rate to 1.8458233995780558e-07.\n",
      "11/11 [==============================] - 0s 15ms/step - loss: 115.6345 - mae: 7.8843 - mse: 115.6345 - val_loss: 120.0166 - val_mae: 8.3876 - val_mse: 120.0166\n",
      "Epoch 111/5000\n",
      "\n",
      "Epoch 00111: LearningRateScheduler reducing learning rate to 1.670170079024566e-07.\n",
      "11/11 [==============================] - 0s 22ms/step - loss: 115.6340 - mae: 7.8842 - mse: 115.6340 - val_loss: 120.0163 - val_mae: 8.3876 - val_mse: 120.0163\n",
      "Epoch 112/5000\n",
      "\n",
      "Epoch 00112: LearningRateScheduler reducing learning rate to 1.5112323819855007e-07.\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 115.6337 - mae: 7.8842 - mse: 115.6337 - val_loss: 120.0161 - val_mae: 8.3876 - val_mse: 120.0161\n",
      "Epoch 113/5000\n",
      "\n",
      "Epoch 00113: LearningRateScheduler reducing learning rate to 1.367419606568094e-07.\n",
      "11/11 [==============================] - 0s 23ms/step - loss: 115.6333 - mae: 7.8842 - mse: 115.6333 - val_loss: 120.0159 - val_mae: 8.3875 - val_mse: 120.0159\n",
      "Epoch 114/5000\n",
      "\n",
      "Epoch 00114: LearningRateScheduler reducing learning rate to 1.2372924261788223e-07.\n",
      "11/11 [==============================] - 0s 18ms/step - loss: 115.6330 - mae: 7.8842 - mse: 115.6330 - val_loss: 120.0157 - val_mae: 8.3875 - val_mse: 120.0157\n",
      "Epoch 115/5000\n",
      "\n",
      "Epoch 00115: LearningRateScheduler reducing learning rate to 1.119548484259094e-07.\n",
      "11/11 [==============================] - 0s 16ms/step - loss: 115.6327 - mae: 7.8842 - mse: 115.6327 - val_loss: 120.0155 - val_mae: 8.3875 - val_mse: 120.0155\n",
      "Epoch 116/5000\n",
      "\n",
      "Epoch 00116: LearningRateScheduler reducing learning rate to 1.0130093598630711e-07.\n",
      "11/11 [==============================] - 0s 25ms/step - loss: 115.6326 - mae: 7.8842 - mse: 115.6326 - val_loss: 120.0154 - val_mae: 8.3875 - val_mse: 120.0154\n",
      "Epoch 117/5000\n",
      "\n",
      "Epoch 00117: LearningRateScheduler reducing learning rate to 9.166087736247602e-08.\n",
      "11/11 [==============================] - 0s 16ms/step - loss: 115.6324 - mae: 7.8841 - mse: 115.6324 - val_loss: 120.0153 - val_mae: 8.3875 - val_mse: 120.0153\n",
      "Epoch 118/5000\n",
      "\n",
      "Epoch 00118: LearningRateScheduler reducing learning rate to 8.293819160757357e-08.\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 115.6323 - mae: 7.8841 - mse: 115.6323 - val_loss: 120.0153 - val_mae: 8.3875 - val_mse: 120.0153\n",
      "Epoch 119/5000\n",
      "\n",
      "Epoch 00119: LearningRateScheduler reducing learning rate to 7.504557915076859e-08.\n",
      "11/11 [==============================] - 0s 15ms/step - loss: 115.6322 - mae: 7.8841 - mse: 115.6322 - val_loss: 120.0152 - val_mae: 8.3875 - val_mse: 120.0152\n",
      "Epoch 120/5000\n",
      "\n",
      "Epoch 00120: LearningRateScheduler reducing learning rate to 6.79040480737947e-08.\n",
      "11/11 [==============================] - 0s 18ms/step - loss: 115.6321 - mae: 7.8841 - mse: 115.6321 - val_loss: 120.0151 - val_mae: 8.3875 - val_mse: 120.0151\n",
      "Epoch 121/5000\n",
      "\n",
      "Epoch 00121: LearningRateScheduler reducing learning rate to 6.14421235332821e-08.\n",
      "11/11 [==============================] - 0s 16ms/step - loss: 115.6320 - mae: 7.8841 - mse: 115.6320 - val_loss: 120.0151 - val_mae: 8.3875 - val_mse: 120.0151\n",
      "Epoch 122/5000\n",
      "\n",
      "Epoch 00122: LearningRateScheduler reducing learning rate to 5.5595132416501365e-08.\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 115.6319 - mae: 7.8841 - mse: 115.6319 - val_loss: 120.0150 - val_mae: 8.3875 - val_mse: 120.0150\n",
      "Epoch 123/5000\n",
      "\n",
      "Epoch 00123: LearningRateScheduler reducing learning rate to 5.030455607111439e-08.\n",
      "11/11 [==============================] - 0s 18ms/step - loss: 115.6318 - mae: 7.8841 - mse: 115.6318 - val_loss: 120.0150 - val_mae: 8.3875 - val_mse: 120.0150\n",
      "Epoch 124/5000\n",
      "\n",
      "Epoch 00124: LearningRateScheduler reducing learning rate to 4.551744463083231e-08.\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 115.6318 - mae: 7.8841 - mse: 115.6318 - val_loss: 120.0149 - val_mae: 8.3875 - val_mse: 120.0149\n",
      "Epoch 125/5000\n",
      "\n",
      "Epoch 00125: LearningRateScheduler reducing learning rate to 4.118588707535708e-08.\n",
      "11/11 [==============================] - 0s 18ms/step - loss: 115.6318 - mae: 7.8841 - mse: 115.6318 - val_loss: 120.0149 - val_mae: 8.3875 - val_mse: 120.0149\n",
      "Epoch 126/5000\n",
      "\n",
      "Epoch 00126: LearningRateScheduler reducing learning rate to 3.726653172078671e-08.\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 115.6318 - mae: 7.8841 - mse: 115.6318 - val_loss: 120.0149 - val_mae: 8.3875 - val_mse: 120.0149\n",
      "Epoch 127/5000\n",
      "\n",
      "Epoch 00127: LearningRateScheduler reducing learning rate to 3.372015234139179e-08.\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 115.6318 - mae: 7.8841 - mse: 115.6318 - val_loss: 120.0149 - val_mae: 8.3875 - val_mse: 120.0149\n",
      "Epoch 128/5000\n",
      "\n",
      "Epoch 00128: LearningRateScheduler reducing learning rate to 3.051125558036417e-08.\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3875 - val_mse: 120.0149\n",
      "Epoch 129/5000\n",
      "\n",
      "Epoch 00129: LearningRateScheduler reducing learning rate to 2.7607725720371988e-08.\n",
      "11/11 [==============================] - 0s 16ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3875 - val_mse: 120.0149\n",
      "Epoch 130/5000\n",
      "\n",
      "Epoch 00130: LearningRateScheduler reducing learning rate to 2.498050325866635e-08.\n",
      "11/11 [==============================] - 0s 32ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3875 - val_mse: 120.0149\n",
      "Epoch 131/5000\n",
      "\n",
      "Epoch 00131: LearningRateScheduler reducing learning rate to 2.260329406981054e-08.\n",
      "11/11 [==============================] - 0s 15ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3875 - val_mse: 120.0149\n",
      "Epoch 132/5000\n",
      "\n",
      "Epoch 00132: LearningRateScheduler reducing learning rate to 2.045230624523486e-08.\n",
      "11/11 [==============================] - 0s 16ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3875 - val_mse: 120.0149\n",
      "Epoch 133/5000\n",
      "\n",
      "Epoch 00133: LearningRateScheduler reducing learning rate to 1.8506011975819047e-08.\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3875 - val_mse: 120.0149\n",
      "Epoch 134/5000\n",
      "\n",
      "Epoch 00134: LearningRateScheduler reducing learning rate to 1.674493209434266e-08.\n",
      "11/11 [==============================] - 0s 17ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3875 - val_mse: 120.0149\n",
      "Epoch 135/5000\n",
      "\n",
      "Epoch 00135: LearningRateScheduler reducing learning rate to 1.515144112143249e-08.\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3875 - val_mse: 120.0149\n",
      "Epoch 136/5000\n",
      "\n",
      "Epoch 00136: LearningRateScheduler reducing learning rate to 1.3709590863840845e-08.\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 137/5000\n",
      "\n",
      "Epoch 00137: LearningRateScheduler reducing learning rate to 1.2404950799567113e-08.\n",
      "11/11 [==============================] - 0s 20ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 138/5000\n",
      "\n",
      "Epoch 00138: LearningRateScheduler reducing learning rate to 1.1224463652343422e-08.\n",
      "11/11 [==============================] - 0s 23ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 139/5000\n",
      "\n",
      "Epoch 00139: LearningRateScheduler reducing learning rate to 1.0156314710024902e-08.\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 140/5000\n",
      "\n",
      "Epoch 00140: LearningRateScheduler reducing learning rate to 9.189813578979572e-09.\n",
      "11/11 [==============================] - 0s 17ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 141/5000\n",
      "\n",
      "Epoch 00141: LearningRateScheduler reducing learning rate to 8.315287191035679e-09.\n",
      "11/11 [==============================] - 0s 20ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 142/5000\n",
      "\n",
      "Epoch 00142: LearningRateScheduler reducing learning rate to 7.5239829921642e-09.\n",
      "11/11 [==============================] - ETA: 0s - loss: 115.4088 - mae: 7.8578 - mse: 115.408 - 0s 32ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 143/5000\n",
      "\n",
      "Epoch 00143: LearningRateScheduler reducing learning rate to 6.8079813439763305e-09.\n",
      "11/11 [==============================] - 0s 35ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 144/5000\n",
      "\n",
      "Epoch 00144: LearningRateScheduler reducing learning rate to 6.160116261320527e-09.\n",
      "11/11 [==============================] - 0s 18ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 145/5000\n",
      "\n",
      "Epoch 00145: LearningRateScheduler reducing learning rate to 5.5739036926945955e-09.\n",
      "11/11 [==============================] - 0s 27ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 146/5000\n",
      "\n",
      "Epoch 00146: LearningRateScheduler reducing learning rate to 5.0434766256788806e-09.\n",
      "11/11 [==============================] - 0s 22ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 147/5000\n",
      "\n",
      "Epoch 00147: LearningRateScheduler reducing learning rate to 4.563526367903986e-09.\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 148/5000\n",
      "\n",
      "Epoch 00148: LearningRateScheduler reducing learning rate to 4.1292494158732645e-09.\n",
      "11/11 [==============================] - 0s 20ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 149/5000\n",
      "\n",
      "Epoch 00149: LearningRateScheduler reducing learning rate to 3.73629937988526e-09.\n",
      "11/11 [==============================] - 0s 22ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 150/5000\n",
      "\n",
      "Epoch 00150: LearningRateScheduler reducing learning rate to 3.380743483904737e-09.\n",
      "11/11 [==============================] - 0s 23ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 151/5000\n",
      "\n",
      "Epoch 00151: LearningRateScheduler reducing learning rate to 3.059023205018258e-09.\n",
      "11/11 [==============================] - 1s 47ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 152/5000\n",
      "\n",
      "Epoch 00152: LearningRateScheduler reducing learning rate to 2.7679186585408024e-09.\n",
      "11/11 [==============================] - 0s 33ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 153/5000\n",
      "\n",
      "Epoch 00153: LearningRateScheduler reducing learning rate to 2.5045163723276173e-09.\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 154/5000\n",
      "\n",
      "Epoch 00154: LearningRateScheduler reducing learning rate to 2.26618012776571e-09.\n",
      "11/11 [==============================] - 0s 17ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 155/5000\n",
      "\n",
      "Epoch 00155: LearningRateScheduler reducing learning rate to 2.0505245756119266e-09.\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 156/5000\n",
      "\n",
      "Epoch 00156: LearningRateScheduler reducing learning rate to 1.8553913626159783e-09.\n",
      "11/11 [==============================] - 0s 18ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 157/5000\n",
      "\n",
      "Epoch 00157: LearningRateScheduler reducing learning rate to 1.6788275299956602e-09.\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 158/5000\n",
      "\n",
      "Epoch 00158: LearningRateScheduler reducing learning rate to 1.5190659675689613e-09.\n",
      "11/11 [==============================] - 0s 15ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 159/5000\n",
      "\n",
      "Epoch 00159: LearningRateScheduler reducing learning rate to 1.374507727921396e-09.\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 160/5000\n",
      "\n",
      "Epoch 00160: LearningRateScheduler reducing learning rate to 1.2437060236028696e-09.\n",
      "11/11 [==============================] - 0s 30ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 161/5000\n",
      "\n",
      "Epoch 00161: LearningRateScheduler reducing learning rate to 1.1253517471925912e-09.\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 162/5000\n",
      "\n",
      "Epoch 00162: LearningRateScheduler reducing learning rate to 1.0182603693119987e-09.\n",
      "11/11 [==============================] - ETA: 0s - loss: 129.1127 - mae: 7.9320 - mse: 129.112 - 0s 17ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 163/5000\n",
      "\n",
      "Epoch 00163: LearningRateScheduler reducing learning rate to 9.213600834566135e-10.\n",
      "11/11 [==============================] - 0s 16ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 164/5000\n",
      "\n",
      "Epoch 00164: LearningRateScheduler reducing learning rate to 8.336810789962771e-10.\n",
      "11/11 [==============================] - 0s 17ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 165/5000\n",
      "\n",
      "Epoch 00165: LearningRateScheduler reducing learning rate to 7.543458349844232e-10.\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 166/5000\n",
      "\n",
      "Epoch 00166: LearningRateScheduler reducing learning rate to 6.82560337633487e-10.\n",
      "11/11 [==============================] - 0s 17ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 167/5000\n",
      "\n",
      "Epoch 00167: LearningRateScheduler reducing learning rate to 6.176061335580364e-10.\n",
      "11/11 [==============================] - 0s 23ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 168/5000\n",
      "\n",
      "Epoch 00168: LearningRateScheduler reducing learning rate to 5.588331392518268e-10.\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 169/5000\n",
      "\n",
      "Epoch 00169: LearningRateScheduler reducing learning rate to 5.05653134833552e-10.\n",
      "11/11 [==============================] - 0s 17ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 170/5000\n",
      "\n",
      "Epoch 00170: LearningRateScheduler reducing learning rate to 4.575338769445796e-10.\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 171/5000\n",
      "\n",
      "Epoch 00171: LearningRateScheduler reducing learning rate to 4.139937718785167e-10.\n",
      "11/11 [==============================] - 0s 22ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 172/5000\n",
      "\n",
      "Epoch 00172: LearningRateScheduler reducing learning rate to 3.745970556295245e-10.\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 173/5000\n",
      "\n",
      "Epoch 00173: LearningRateScheduler reducing learning rate to 3.389494326196924e-10.\n",
      "11/11 [==============================] - 0s 22ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 174/5000\n",
      "\n",
      "Epoch 00174: LearningRateScheduler reducing learning rate to 3.066941294563555e-10.\n",
      "11/11 [==============================] - 0s 21ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 175/5000\n",
      "\n",
      "Epoch 00175: LearningRateScheduler reducing learning rate to 2.775083242240747e-10.\n",
      "11/11 [==============================] - 0s 21ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 176/5000\n",
      "\n",
      "Epoch 00176: LearningRateScheduler reducing learning rate to 2.5109991557439817e-10.\n",
      "11/11 [==============================] - 0s 18ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 177/5000\n",
      "\n",
      "Epoch 00177: LearningRateScheduler reducing learning rate to 2.2720459927738556e-10.\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 178/5000\n",
      "\n",
      "Epoch 00178: LearningRateScheduler reducing learning rate to 2.0558322297604486e-10.\n",
      "11/11 [==============================] - 0s 18ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 179/5000\n",
      "\n",
      "Epoch 00179: LearningRateScheduler reducing learning rate to 1.860193926691551e-10.\n",
      "11/11 [==============================] - 0s 31ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 180/5000\n",
      "\n",
      "Epoch 00180: LearningRateScheduler reducing learning rate to 1.6831730696737537e-10.\n",
      "11/11 [==============================] - 0s 36ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 181/5000\n",
      "\n",
      "Epoch 00181: LearningRateScheduler reducing learning rate to 1.522997974471263e-10.\n",
      "11/11 [==============================] - 0s 45ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 182/5000\n",
      "\n",
      "Epoch 00182: LearningRateScheduler reducing learning rate to 1.378065554894572e-10.\n",
      "11/11 [==============================] - 0s 32ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 183/5000\n",
      "\n",
      "Epoch 00183: LearningRateScheduler reducing learning rate to 1.246925278575099e-10.\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 184/5000\n",
      "\n",
      "Epoch 00184: LearningRateScheduler reducing learning rate to 1.1282646495496605e-10.\n",
      "11/11 [==============================] - 0s 18ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 185/5000\n",
      "\n",
      "Epoch 00185: LearningRateScheduler reducing learning rate to 1.0208960723597601e-10.\n",
      "11/11 [==============================] - 0s 23ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 186/5000\n",
      "\n",
      "Epoch 00186: LearningRateScheduler reducing learning rate to 9.237449661970594e-11.\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 187/5000\n",
      "\n",
      "Epoch 00187: LearningRateScheduler reducing learning rate to 8.358390101374608e-11.\n",
      "11/11 [==============================] - 0s 21ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 188/5000\n",
      "\n",
      "Epoch 00188: LearningRateScheduler reducing learning rate to 7.56298411826514e-11.\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 189/5000\n",
      "\n",
      "Epoch 00189: LearningRateScheduler reducing learning rate to 6.843271022217988e-11.\n",
      "11/11 [==============================] - 0s 18ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 190/5000\n",
      "\n",
      "Epoch 00190: LearningRateScheduler reducing learning rate to 6.192047682664017e-11.\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 191/5000\n",
      "\n",
      "Epoch 00191: LearningRateScheduler reducing learning rate to 5.602796437537268e-11.\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 192/5000\n",
      "\n",
      "Epoch 00192: LearningRateScheduler reducing learning rate to 5.069619862322287e-11.\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 193/5000\n",
      "\n",
      "Epoch 00193: LearningRateScheduler reducing learning rate to 4.5871817466475085e-11.\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 194/5000\n",
      "\n",
      "Epoch 00194: LearningRateScheduler reducing learning rate to 4.1506536876982234e-11.\n",
      "11/11 [==============================] - 0s 23ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 195/5000\n",
      "\n",
      "Epoch 00195: LearningRateScheduler reducing learning rate to 3.7556667659382895e-11.\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 196/5000\n",
      "\n",
      "Epoch 00196: LearningRateScheduler reducing learning rate to 3.398267819495071e-11.\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 197/5000\n",
      "\n",
      "Epoch 00197: LearningRateScheduler reducing learning rate to 3.074879879586606e-11.\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 198/5000\n",
      "\n",
      "Epoch 00198: LearningRateScheduler reducing learning rate to 2.7822663710158628e-11.\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 199/5000\n",
      "\n",
      "Epoch 00199: LearningRateScheduler reducing learning rate to 2.517498719438278e-11.\n",
      "11/11 [==============================] - 0s 16ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 200/5000\n",
      "\n",
      "Epoch 00200: LearningRateScheduler reducing learning rate to 2.277927041205363e-11.\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 201/5000\n",
      "\n",
      "Epoch 00201: LearningRateScheduler reducing learning rate to 2.061153622438558e-11.\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 202/5000\n",
      "\n",
      "Epoch 00202: LearningRateScheduler reducing learning rate to 1.865008921902767e-11.\n",
      "11/11 [==============================] - 0s 23ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 203/5000\n",
      "\n",
      "Epoch 00203: LearningRateScheduler reducing learning rate to 1.687529857508526e-11.\n",
      "11/11 [==============================] - 0s 30ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 204/5000\n",
      "\n",
      "Epoch 00204: LearningRateScheduler reducing learning rate to 1.5269401591266087e-11.\n",
      "11/11 [==============================] - 0s 16ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 205/5000\n",
      "\n",
      "Epoch 00205: LearningRateScheduler reducing learning rate to 1.3816325910795359e-11.\n",
      "11/11 [==============================] - 0s 23ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 206/5000\n",
      "\n",
      "Epoch 00206: LearningRateScheduler reducing learning rate to 1.2501528663867425e-11.\n",
      "11/11 [==============================] - 0s 15ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 207/5000\n",
      "\n",
      "Epoch 00207: LearningRateScheduler reducing learning rate to 1.1311850917716326e-11.\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 208/5000\n",
      "\n",
      "Epoch 00208: LearningRateScheduler reducing learning rate to 1.0235385977594125e-11.\n",
      "11/11 [==============================] - 0s 35ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 209/5000\n",
      "\n",
      "Epoch 00209: LearningRateScheduler reducing learning rate to 9.261360220567754e-12.\n",
      "11/11 [==============================] - 0s 24ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 210/5000\n",
      "\n",
      "Epoch 00210: LearningRateScheduler reducing learning rate to 8.38002526947946e-12.\n",
      "11/11 [==============================] - ETA: 0s - loss: 117.9177 - mae: 8.2838 - mse: 117.917 - 0s 18ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 211/5000\n",
      "\n",
      "Epoch 00211: LearningRateScheduler reducing learning rate to 7.582560427911907e-12.\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 212/5000\n",
      "\n",
      "Epoch 00212: LearningRateScheduler reducing learning rate to 6.860984399693441e-12.\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 213/5000\n",
      "\n",
      "Epoch 00213: LearningRateScheduler reducing learning rate to 6.208075409403602e-12.\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 214/5000\n",
      "\n",
      "Epoch 00214: LearningRateScheduler reducing learning rate to 5.6172989244172994e-12.\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 215/5000\n",
      "\n",
      "Epoch 00215: LearningRateScheduler reducing learning rate to 5.082742255105915e-12.\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 216/5000\n",
      "\n",
      "Epoch 00216: LearningRateScheduler reducing learning rate to 4.599055378652316e-12.\n",
      "11/11 [==============================] - 0s 23ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 217/5000\n",
      "\n",
      "Epoch 00217: LearningRateScheduler reducing learning rate to 4.161397394224149e-12.\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 218/5000\n",
      "\n",
      "Epoch 00218: LearningRateScheduler reducing learning rate to 3.7653880736113436e-12.\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 219/5000\n",
      "\n",
      "Epoch 00219: LearningRateScheduler reducing learning rate to 3.407064022429891e-12.\n",
      "11/11 [==============================] - 0s 42ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 220/5000\n",
      "\n",
      "Epoch 00220: LearningRateScheduler reducing learning rate to 3.082839013138669e-12.\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 221/5000\n",
      "\n",
      "Epoch 00221: LearningRateScheduler reducing learning rate to 2.7894680928689248e-12.\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 222/5000\n",
      "\n",
      "Epoch 00222: LearningRateScheduler reducing learning rate to 2.5240151068452067e-12.\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 223/5000\n",
      "\n",
      "Epoch 00223: LearningRateScheduler reducing learning rate to 2.28382331236157e-12.\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 224/5000\n",
      "\n",
      "Epoch 00224: LearningRateScheduler reducing learning rate to 2.0664887892075804e-12.\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 225/5000\n",
      "\n",
      "Epoch 00225: LearningRateScheduler reducing learning rate to 1.8698363804268408e-12.\n",
      "11/11 [==============================] - 0s 23ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 226/5000\n",
      "\n",
      "Epoch 00226: LearningRateScheduler reducing learning rate to 1.6918979226151306e-12.\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 227/5000\n",
      "\n",
      "Epoch 00227: LearningRateScheduler reducing learning rate to 1.5308925478794763e-12.\n",
      "11/11 [==============================] - 0s 16ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 228/5000\n",
      "\n",
      "Epoch 00228: LearningRateScheduler reducing learning rate to 1.3852088603137548e-12.\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 229/5000\n",
      "\n",
      "Epoch 00229: LearningRateScheduler reducing learning rate to 1.2533888086068348e-12.\n",
      "11/11 [==============================] - 0s 20ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 230/5000\n",
      "\n",
      "Epoch 00230: LearningRateScheduler reducing learning rate to 1.1341130933749742e-12.\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 231/5000\n",
      "\n",
      "Epoch 00231: LearningRateScheduler reducing learning rate to 1.026187963170189e-12.\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 232/5000\n",
      "\n",
      "Epoch 00232: LearningRateScheduler reducing learning rate to 9.285332670144928e-13.\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 233/5000\n",
      "\n",
      "Epoch 00233: LearningRateScheduler reducing learning rate to 8.401716438858868e-13.\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 234/5000\n",
      "\n",
      "Epoch 00234: LearningRateScheduler reducing learning rate to 7.602187409607351e-13.\n",
      "11/11 [==============================] - 0s 22ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 235/5000\n",
      "\n",
      "Epoch 00235: LearningRateScheduler reducing learning rate to 6.878743627134586e-13.\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 236/5000\n",
      "\n",
      "Epoch 00236: LearningRateScheduler reducing learning rate to 6.224144622907783e-13.\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 237/5000\n",
      "\n",
      "Epoch 00237: LearningRateScheduler reducing learning rate to 5.631838950074272e-13.\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 238/5000\n",
      "\n",
      "Epoch 00238: LearningRateScheduler reducing learning rate to 5.095898614379546e-13.\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 239/5000\n",
      "\n",
      "Epoch 00239: LearningRateScheduler reducing learning rate to 4.610959744808222e-13.\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 240/5000\n",
      "\n",
      "Epoch 00240: LearningRateScheduler reducing learning rate to 4.172168910160013e-13.\n",
      "11/11 [==============================] - 0s 18ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 241/5000\n",
      "\n",
      "Epoch 00241: LearningRateScheduler reducing learning rate to 3.775134544279098e-13.\n",
      "11/11 [==============================] - 0s 21ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 242/5000\n",
      "\n",
      "Epoch 00242: LearningRateScheduler reducing learning rate to 3.4158829937838527e-13.\n",
      "11/11 [==============================] - 1s 50ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 243/5000\n",
      "\n",
      "Epoch 00243: LearningRateScheduler reducing learning rate to 3.0908187484083206e-13.\n",
      "11/11 [==============================] - 0s 27ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 244/5000\n",
      "\n",
      "Epoch 00244: LearningRateScheduler reducing learning rate to 2.796688455926927e-13.\n",
      "11/11 [==============================] - 0s 27ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 245/5000\n",
      "\n",
      "Epoch 00245: LearningRateScheduler reducing learning rate to 2.5305483615118915e-13.\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 246/5000\n",
      "\n",
      "Epoch 00246: LearningRateScheduler reducing learning rate to 2.289734845645553e-13.\n",
      "11/11 [==============================] - 0s 15ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 247/5000\n",
      "\n",
      "Epoch 00247: LearningRateScheduler reducing learning rate to 2.0718377657208855e-13.\n",
      "11/11 [==============================] - 0s 21ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 248/5000\n",
      "\n",
      "Epoch 00248: LearningRateScheduler reducing learning rate to 1.8746763345242745e-13.\n",
      "11/11 [==============================] - 0s 24ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 249/5000\n",
      "\n",
      "Epoch 00249: LearningRateScheduler reducing learning rate to 1.6962772941840653e-13.\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 250/5000\n",
      "\n",
      "Epoch 00250: LearningRateScheduler reducing learning rate to 1.5348551671425313e-13.\n",
      "11/11 [==============================] - 0s 20ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 251/5000\n",
      "\n",
      "Epoch 00251: LearningRateScheduler reducing learning rate to 1.388794386496402e-13.\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 252/5000\n",
      "\n",
      "Epoch 00252: LearningRateScheduler reducing learning rate to 1.256633126860237e-13.\n",
      "11/11 [==============================] - 0s 40ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 253/5000\n",
      "\n",
      "Epoch 00253: LearningRateScheduler reducing learning rate to 1.13704867392667e-13.\n",
      "11/11 [==============================] - 0s 15ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 254/5000\n",
      "\n",
      "Epoch 00254: LearningRateScheduler reducing learning rate to 1.0288441862970217e-13.\n",
      "11/11 [==============================] - 0s 15ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 255/5000\n",
      "\n",
      "Epoch 00255: LearningRateScheduler reducing learning rate to 9.309367170903036e-14.\n",
      "11/11 [==============================] - 0s 22ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 256/5000\n",
      "\n",
      "Epoch 00256: LearningRateScheduler reducing learning rate to 8.423463754468647e-14.\n",
      "11/11 [==============================] - 0s 33ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 257/5000\n",
      "\n",
      "Epoch 00257: LearningRateScheduler reducing learning rate to 7.621865194512891e-14.\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 258/5000\n",
      "\n",
      "Epoch 00258: LearningRateScheduler reducing learning rate to 6.896548823221181e-14.\n",
      "11/11 [==============================] - 0s 24ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 259/5000\n",
      "\n",
      "Epoch 00259: LearningRateScheduler reducing learning rate to 6.240255430562401e-14.\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 260/5000\n",
      "\n",
      "Epoch 00260: LearningRateScheduler reducing learning rate to 5.646416611674951e-14.\n",
      "11/11 [==============================] - 0s 18ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 261/5000\n",
      "\n",
      "Epoch 00261: LearningRateScheduler reducing learning rate to 5.109089028063325e-14.\n",
      "11/11 [==============================] - 0s 26ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 262/5000\n",
      "\n",
      "Epoch 00262: LearningRateScheduler reducing learning rate to 4.622894924668663e-14.\n",
      "11/11 [==============================] - 0s 16ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 263/5000\n",
      "\n",
      "Epoch 00263: LearningRateScheduler reducing learning rate to 4.182968307488728e-14.\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 264/5000\n",
      "\n",
      "Epoch 00264: LearningRateScheduler reducing learning rate to 3.7849062430743565e-14.\n",
      "11/11 [==============================] - 0s 39ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 265/5000\n",
      "\n",
      "Epoch 00265: LearningRateScheduler reducing learning rate to 3.4247247924915805e-14.\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 266/5000\n",
      "\n",
      "Epoch 00266: LearningRateScheduler reducing learning rate to 3.098819138721826e-14.\n",
      "11/11 [==============================] - 0s 35ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 267/5000\n",
      "\n",
      "Epoch 00267: LearningRateScheduler reducing learning rate to 2.8039275084414687e-14.\n",
      "11/11 [==============================] - 0s 26ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 268/5000\n",
      "\n",
      "Epoch 00268: LearningRateScheduler reducing learning rate to 2.537098527098176e-14.\n",
      "11/11 [==============================] - 0s 16ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 269/5000\n",
      "\n",
      "Epoch 00269: LearningRateScheduler reducing learning rate to 2.2956616805623547e-14.\n",
      "11/11 [==============================] - 0s 17ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 270/5000\n",
      "\n",
      "Epoch 00270: LearningRateScheduler reducing learning rate to 2.0772005877241298e-14.\n",
      "11/11 [==============================] - 0s 15ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 271/5000\n",
      "\n",
      "Epoch 00271: LearningRateScheduler reducing learning rate to 1.879528816539083e-14.\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 272/5000\n",
      "\n",
      "Epoch 00272: LearningRateScheduler reducing learning rate to 1.7006680014814044e-14.\n",
      "11/11 [==============================] - 0s 20ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 273/5000\n",
      "\n",
      "Epoch 00273: LearningRateScheduler reducing learning rate to 1.5388280433968074e-14.\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 274/5000\n",
      "\n",
      "Epoch 00274: LearningRateScheduler reducing learning rate to 1.3923891935884977e-14.\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 275/5000\n",
      "\n",
      "Epoch 00275: LearningRateScheduler reducing learning rate to 1.2598858428277863e-14.\n",
      "11/11 [==============================] - 0s 15ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 276/5000\n",
      "\n",
      "Epoch 00276: LearningRateScheduler reducing learning rate to 1.1399918530443554e-14.\n",
      "11/11 [==============================] - 0s 15ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 277/5000\n",
      "\n",
      "Epoch 00277: LearningRateScheduler reducing learning rate to 1.0315072848906821e-14.\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 278/5000\n",
      "\n",
      "Epoch 00278: LearningRateScheduler reducing learning rate to 9.333463883457665e-15.\n",
      "11/11 [==============================] - 0s 22ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 279/5000\n",
      "\n",
      "Epoch 00279: LearningRateScheduler reducing learning rate to 8.44526736163973e-15.\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 280/5000\n",
      "\n",
      "Epoch 00280: LearningRateScheduler reducing learning rate to 7.641593914129444e-15.\n",
      "11/11 [==============================] - 0s 17ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 281/5000\n",
      "\n",
      "Epoch 00281: LearningRateScheduler reducing learning rate to 6.914400106940203e-15.\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 282/5000\n",
      "\n",
      "Epoch 00282: LearningRateScheduler reducing learning rate to 6.256407940031327e-15.\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 283/5000\n",
      "\n",
      "Epoch 00283: LearningRateScheduler reducing learning rate to 5.6610320066376145e-15.\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 284/5000\n",
      "\n",
      "Epoch 00284: LearningRateScheduler reducing learning rate to 5.122313584304918e-15.\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 285/5000\n",
      "\n",
      "Epoch 00285: LearningRateScheduler reducing learning rate to 4.6348609979929765e-15.\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 286/5000\n",
      "\n",
      "Epoch 00286: LearningRateScheduler reducing learning rate to 4.1937956583795444e-15.\n",
      "11/11 [==============================] - 0s 17ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 287/5000\n",
      "\n",
      "Epoch 00287: LearningRateScheduler reducing learning rate to 3.794703235298559e-15.\n",
      "11/11 [==============================] - 0s 21ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 288/5000\n",
      "\n",
      "Epoch 00288: LearningRateScheduler reducing learning rate to 3.4335894776402456e-15.\n",
      "11/11 [==============================] - 0s 39ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 289/5000\n",
      "\n",
      "Epoch 00289: LearningRateScheduler reducing learning rate to 3.1068402375434455e-15.\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 290/5000\n",
      "\n",
      "Epoch 00290: LearningRateScheduler reducing learning rate to 2.8111852987890345e-15.\n",
      "11/11 [==============================] - 0s 15ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 291/5000\n",
      "\n",
      "Epoch 00291: LearningRateScheduler reducing learning rate to 2.543665647376923e-15.\n",
      "11/11 [==============================] - 0s 33ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 292/5000\n",
      "\n",
      "Epoch 00292: LearningRateScheduler reducing learning rate to 2.3016038567192993e-15.\n",
      "11/11 [==============================] - 0s 39ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 293/5000\n",
      "\n",
      "Epoch 00293: LearningRateScheduler reducing learning rate to 2.0825772910554944e-15.\n",
      "11/11 [==============================] - 0s 27ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 294/5000\n",
      "\n",
      "Epoch 00294: LearningRateScheduler reducing learning rate to 1.884393858898981e-15.\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 295/5000\n",
      "\n",
      "Epoch 00295: LearningRateScheduler reducing learning rate to 1.7050700738489696e-15.\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 296/5000\n",
      "\n",
      "Epoch 00296: LearningRateScheduler reducing learning rate to 1.5428112031918877e-15.\n",
      "11/11 [==============================] - 0s 23ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 297/5000\n",
      "\n",
      "Epoch 00297: LearningRateScheduler reducing learning rate to 1.395993305613098e-15.\n",
      "11/11 [==============================] - 0s 16ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 298/5000\n",
      "\n",
      "Epoch 00298: LearningRateScheduler reducing learning rate to 1.2631469782464381e-15.\n",
      "11/11 [==============================] - 0s 24ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 299/5000\n",
      "\n",
      "Epoch 00299: LearningRateScheduler reducing learning rate to 1.1429426503964337e-15.\n",
      "11/11 [==============================] - 0s 23ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 300/5000\n",
      "\n",
      "Epoch 00300: LearningRateScheduler reducing learning rate to 1.0341772767478841e-15.\n",
      "11/11 [==============================] - 0s 32ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 301/5000\n",
      "\n",
      "Epoch 00301: LearningRateScheduler reducing learning rate to 9.357622968840176e-16.\n",
      "11/11 [==============================] - 0s 22ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 302/5000\n",
      "\n",
      "Epoch 00302: LearningRateScheduler reducing learning rate to 8.467127406079322e-16.\n",
      "11/11 [==============================] - 0s 18ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 303/5000\n",
      "\n",
      "Epoch 00303: LearningRateScheduler reducing learning rate to 7.661373700298315e-16.\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 304/5000\n",
      "\n",
      "Epoch 00304: LearningRateScheduler reducing learning rate to 6.932297597586548e-16.\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 305/5000\n",
      "\n",
      "Epoch 00305: LearningRateScheduler reducing learning rate to 6.272602259257088e-16.\n",
      "11/11 [==============================] - 0s 24ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 306/5000\n",
      "\n",
      "Epoch 00306: LearningRateScheduler reducing learning rate to 5.675685232632723e-16.\n",
      "11/11 [==============================] - 0s 18ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 307/5000\n",
      "\n",
      "Epoch 00307: LearningRateScheduler reducing learning rate to 5.135572371480209e-16.\n",
      "11/11 [==============================] - 0s 16ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 308/5000\n",
      "\n",
      "Epoch 00308: LearningRateScheduler reducing learning rate to 4.646858044746957e-16.\n",
      "11/11 [==============================] - 0s 31ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 309/5000\n",
      "\n",
      "Epoch 00309: LearningRateScheduler reducing learning rate to 4.2046510351884724e-16.\n",
      "11/11 [==============================] - 0s 30ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 310/5000\n",
      "\n",
      "Epoch 00310: LearningRateScheduler reducing learning rate to 3.8045255864221565e-16.\n",
      "11/11 [==============================] - 0s 22ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 311/5000\n",
      "\n",
      "Epoch 00311: LearningRateScheduler reducing learning rate to 3.442477108469977e-16.\n",
      "11/11 [==============================] - 0s 21ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 312/5000\n",
      "\n",
      "Epoch 00312: LearningRateScheduler reducing learning rate to 3.114882098475865e-16.\n",
      "11/11 [==============================] - 0s 16ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 313/5000\n",
      "\n",
      "Epoch 00313: LearningRateScheduler reducing learning rate to 2.8184618754713294e-16.\n",
      "11/11 [==============================] - 0s 35ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 314/5000\n",
      "\n",
      "Epoch 00314: LearningRateScheduler reducing learning rate to 2.550249766234271e-16.\n",
      "11/11 [==============================] - 0s 18ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 315/5000\n",
      "\n",
      "Epoch 00315: LearningRateScheduler reducing learning rate to 2.3075614138262244e-16.\n",
      "11/11 [==============================] - 0s 16ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 316/5000\n",
      "\n",
      "Epoch 00316: LearningRateScheduler reducing learning rate to 2.0879679116459335e-16.\n",
      "11/11 [==============================] - 0s 15ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 317/5000\n",
      "\n",
      "Epoch 00317: LearningRateScheduler reducing learning rate to 1.8892714941156384e-16.\n",
      "11/11 [==============================] - 0s 22ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 318/5000\n",
      "\n",
      "Epoch 00318: LearningRateScheduler reducing learning rate to 1.7094835407045315e-16.\n",
      "11/11 [==============================] - 0s 15ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 319/5000\n",
      "\n",
      "Epoch 00319: LearningRateScheduler reducing learning rate to 1.5468046731460617e-16.\n",
      "11/11 [==============================] - 0s 36ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 320/5000\n",
      "\n",
      "Epoch 00320: LearningRateScheduler reducing learning rate to 1.399606746655437e-16.\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 321/5000\n",
      "\n",
      "Epoch 00321: LearningRateScheduler reducing learning rate to 1.2664165549094176e-16.\n",
      "11/11 [==============================] - 0s 18ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 322/5000\n",
      "\n",
      "Epoch 00322: LearningRateScheduler reducing learning rate to 1.1459010857022307e-16.\n",
      "11/11 [==============================] - 0s 21ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 323/5000\n",
      "\n",
      "Epoch 00323: LearningRateScheduler reducing learning rate to 1.0368541797114079e-16.\n",
      "11/11 [==============================] - 0s 22ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 324/5000\n",
      "\n",
      "Epoch 00324: LearningRateScheduler reducing learning rate to 9.381844588498617e-17.\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 325/5000\n",
      "\n",
      "Epoch 00325: LearningRateScheduler reducing learning rate to 8.489044033871777e-17.\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 326/5000\n",
      "\n",
      "Epoch 00326: LearningRateScheduler reducing learning rate to 7.681204685202095e-17.\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 327/5000\n",
      "\n",
      "Epoch 00327: LearningRateScheduler reducing learning rate to 6.950241414763969e-17.\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 328/5000\n",
      "\n",
      "Epoch 00328: LearningRateScheduler reducing learning rate to 6.288838496461615e-17.\n",
      "11/11 [==============================] - 0s 20ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 329/5000\n",
      "\n",
      "Epoch 00329: LearningRateScheduler reducing learning rate to 5.690376387583466e-17.\n",
      "11/11 [==============================] - 0s 25ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 330/5000\n",
      "\n",
      "Epoch 00330: LearningRateScheduler reducing learning rate to 5.148865478193844e-17.\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 331/5000\n",
      "\n",
      "Epoch 00331: LearningRateScheduler reducing learning rate to 4.6588861451033977e-17.\n",
      "11/11 [==============================] - 0s 17ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 332/5000\n",
      "\n",
      "Epoch 00332: LearningRateScheduler reducing learning rate to 4.215534510458857e-17.\n",
      "11/11 [==============================] - 0s 15ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 333/5000\n",
      "\n",
      "Epoch 00333: LearningRateScheduler reducing learning rate to 3.8143733620850695e-17.\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 334/5000\n",
      "\n",
      "Epoch 00334: LearningRateScheduler reducing learning rate to 3.451387744374192e-17.\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 335/5000\n",
      "\n",
      "Epoch 00335: LearningRateScheduler reducing learning rate to 3.122944775260516e-17.\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 336/5000\n",
      "\n",
      "Epoch 00336: LearningRateScheduler reducing learning rate to 2.825757287115611e-17.\n",
      "11/11 [==============================] - 0s 16ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 337/5000\n",
      "\n",
      "Epoch 00337: LearningRateScheduler reducing learning rate to 2.5568509276699838e-17.\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 338/5000\n",
      "\n",
      "Epoch 00338: LearningRateScheduler reducing learning rate to 2.3135343916957528e-17.\n",
      "11/11 [==============================] - 0s 25ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 339/5000\n",
      "\n",
      "Epoch 00339: LearningRateScheduler reducing learning rate to 2.0933724855193765e-17.\n",
      "11/11 [==============================] - 0s 15ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 340/5000\n",
      "\n",
      "Epoch 00340: LearningRateScheduler reducing learning rate to 1.8941617547848824e-17.\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 341/5000\n",
      "\n",
      "Epoch 00341: LearningRateScheduler reducing learning rate to 1.713908431542013e-17.\n",
      "11/11 [==============================] - 0s 15ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 342/5000\n",
      "\n",
      "Epoch 00342: LearningRateScheduler reducing learning rate to 1.550808479946534e-17.\n",
      "11/11 [==============================] - 0s 16ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 343/5000\n",
      "\n",
      "Epoch 00343: LearningRateScheduler reducing learning rate to 1.403229540863091e-17.\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 344/5000\n",
      "\n",
      "Epoch 00344: LearningRateScheduler reducing learning rate to 1.2696945946663425e-17.\n",
      "11/11 [==============================] - 0s 21ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 345/5000\n",
      "\n",
      "Epoch 00345: LearningRateScheduler reducing learning rate to 1.1488671787321141e-17.\n",
      "11/11 [==============================] - 0s 35ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 346/5000\n",
      "\n",
      "Epoch 00346: LearningRateScheduler reducing learning rate to 1.039538011670222e-17.\n",
      "11/11 [==============================] - 0s 15ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 347/5000\n",
      "\n",
      "Epoch 00347: LearningRateScheduler reducing learning rate to 9.406128904299175e-18.\n",
      "11/11 [==============================] - 0s 20ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 348/5000\n",
      "\n",
      "Epoch 00348: LearningRateScheduler reducing learning rate to 8.511017391479462e-18.\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 349/5000\n",
      "\n",
      "Epoch 00349: LearningRateScheduler reducing learning rate to 7.701087001365415e-18.\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 350/5000\n",
      "\n",
      "Epoch 00350: LearningRateScheduler reducing learning rate to 6.968231678385812e-18.\n",
      "11/11 [==============================] - 0s 16ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 351/5000\n",
      "\n",
      "Epoch 00351: LearningRateScheduler reducing learning rate to 6.3051167601469895e-18.\n",
      "11/11 [==============================] - 0s 15ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 352/5000\n",
      "\n",
      "Epoch 00352: LearningRateScheduler reducing learning rate to 5.705105569666649e-18.\n",
      "11/11 [==============================] - 0s 23ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 353/5000\n",
      "\n",
      "Epoch 00353: LearningRateScheduler reducing learning rate to 5.162192993279735e-18.\n",
      "11/11 [==============================] - 0s 17ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 354/5000\n",
      "\n",
      "Epoch 00354: LearningRateScheduler reducing learning rate to 4.670945379442551e-18.\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 355/5000\n",
      "\n",
      "Epoch 00355: LearningRateScheduler reducing learning rate to 4.226446156921817e-18.\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 356/5000\n",
      "\n",
      "Epoch 00356: LearningRateScheduler reducing learning rate to 3.8242466280971355e-18.\n",
      "11/11 [==============================] - 0s 30ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 357/5000\n",
      "\n",
      "Epoch 00357: LearningRateScheduler reducing learning rate to 3.4603214449001314e-18.\n",
      "11/11 [==============================] - 0s 15ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 358/5000\n",
      "\n",
      "Epoch 00358: LearningRateScheduler reducing learning rate to 3.1310283217778916e-18.\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 359/5000\n",
      "\n",
      "Epoch 00359: LearningRateScheduler reducing learning rate to 2.8330715824749674e-18.\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 360/5000\n",
      "\n",
      "Epoch 00360: LearningRateScheduler reducing learning rate to 2.5634691757977136e-18.\n",
      "11/11 [==============================] - 0s 17ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 361/5000\n",
      "\n",
      "Epoch 00361: LearningRateScheduler reducing learning rate to 2.31952283024357e-18.\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 362/5000\n",
      "\n",
      "Epoch 00362: LearningRateScheduler reducing learning rate to 2.0987910487930497e-18.\n",
      "11/11 [==============================] - 0s 16ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 363/5000\n",
      "\n",
      "Epoch 00363: LearningRateScheduler reducing learning rate to 1.899064673586884e-18.\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 364/5000\n",
      "\n",
      "Epoch 00364: LearningRateScheduler reducing learning rate to 1.7183447759316562e-18.\n",
      "11/11 [==============================] - 0s 20ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 365/5000\n",
      "\n",
      "Epoch 00365: LearningRateScheduler reducing learning rate to 1.554822650349588e-18.\n",
      "11/11 [==============================] - 0s 15ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 366/5000\n",
      "\n",
      "Epoch 00366: LearningRateScheduler reducing learning rate to 1.4068617124461468e-18.\n",
      "11/11 [==============================] - 0s 21ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 367/5000\n",
      "\n",
      "Epoch 00367: LearningRateScheduler reducing learning rate to 1.272981119423418e-18.\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 368/5000\n",
      "\n",
      "Epoch 00368: LearningRateScheduler reducing learning rate to 1.1518409493076098e-18.\n",
      "11/11 [==============================] - 0s 23ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 369/5000\n",
      "\n",
      "Epoch 00369: LearningRateScheduler reducing learning rate to 1.0422287905595846e-18.\n",
      "11/11 [==============================] - 0s 17ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 370/5000\n",
      "\n",
      "Epoch 00370: LearningRateScheduler reducing learning rate to 9.430476078526809e-19.\n",
      "11/11 [==============================] - 0s 16ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 371/5000\n",
      "\n",
      "Epoch 00371: LearningRateScheduler reducing learning rate to 8.533047625744066e-19.\n",
      "11/11 [==============================] - 0s 15ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 372/5000\n",
      "\n",
      "Epoch 00372: LearningRateScheduler reducing learning rate to 7.721020781656125e-19.\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 373/5000\n",
      "\n",
      "Epoch 00373: LearningRateScheduler reducing learning rate to 6.986268508675703e-19.\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 374/5000\n",
      "\n",
      "Epoch 00374: LearningRateScheduler reducing learning rate to 6.32143715909605e-19.\n",
      "11/11 [==============================] - 0s 15ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 375/5000\n",
      "\n",
      "Epoch 00375: LearningRateScheduler reducing learning rate to 5.719872877313073e-19.\n",
      "11/11 [==============================] - ETA: 0s - loss: 160.1064 - mae: 9.2735 - mse: 160.106 - 0s 17ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 376/5000\n",
      "\n",
      "Epoch 00376: LearningRateScheduler reducing learning rate to 5.175555005801869e-19.\n",
      "11/11 [==============================] - 0s 27ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 377/5000\n",
      "\n",
      "Epoch 00377: LearningRateScheduler reducing learning rate to 4.683035828352842e-19.\n",
      "11/11 [==============================] - 0s 25ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 378/5000\n",
      "\n",
      "Epoch 00378: LearningRateScheduler reducing learning rate to 4.2373860474966706e-19.\n",
      "11/11 [==============================] - 0s 32ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 379/5000\n",
      "\n",
      "Epoch 00379: LearningRateScheduler reducing learning rate to 3.834145450438482e-19.\n",
      "11/11 [==============================] - ETA: 0s - loss: 142.8206 - mae: 8.7930 - mse: 142.820 - 0s 16ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 380/5000\n",
      "\n",
      "Epoch 00380: LearningRateScheduler reducing learning rate to 3.4692782697490973e-19.\n",
      "11/11 [==============================] - 0s 15ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 381/5000\n",
      "\n",
      "Epoch 00381: LearningRateScheduler reducing learning rate to 3.13913279204803e-19.\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 382/5000\n",
      "\n",
      "Epoch 00382: LearningRateScheduler reducing learning rate to 2.840404810428748e-19.\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 383/5000\n",
      "\n",
      "Epoch 00383: LearningRateScheduler reducing learning rate to 2.570104554845264e-19.\n",
      "11/11 [==============================] - 0s 24ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 384/5000\n",
      "\n",
      "Epoch 00384: LearningRateScheduler reducing learning rate to 2.3255267694886445e-19.\n",
      "11/11 [==============================] - 0s 16ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 385/5000\n",
      "\n",
      "Epoch 00385: LearningRateScheduler reducing learning rate to 2.1042236376776082e-19.\n",
      "11/11 [==============================] - 0s 17ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 386/5000\n",
      "\n",
      "Epoch 00386: LearningRateScheduler reducing learning rate to 1.9039802832864523e-19.\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 387/5000\n",
      "\n",
      "Epoch 00387: LearningRateScheduler reducing learning rate to 1.722792603520286e-19.\n",
      "11/11 [==============================] - 0s 17ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 388/5000\n",
      "\n",
      "Epoch 00388: LearningRateScheduler reducing learning rate to 1.558847211180742e-19.\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 389/5000\n",
      "\n",
      "Epoch 00389: LearningRateScheduler reducing learning rate to 1.4105032856773369e-19.\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 390/5000\n",
      "\n",
      "Epoch 00390: LearningRateScheduler reducing learning rate to 1.276276151143517e-19.\n",
      "11/11 [==============================] - 0s 18ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 391/5000\n",
      "\n",
      "Epoch 00391: LearningRateScheduler reducing learning rate to 1.1548224173015786e-19.\n",
      "11/11 [==============================] - 0s 18ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 392/5000\n",
      "\n",
      "Epoch 00392: LearningRateScheduler reducing learning rate to 1.0449265343612043e-19.\n",
      "11/11 [==============================] - 0s 26ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 393/5000\n",
      "\n",
      "Epoch 00393: LearningRateScheduler reducing learning rate to 9.454886273886541e-20.\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 394/5000\n",
      "\n",
      "Epoch 00394: LearningRateScheduler reducing learning rate to 8.555134883887122e-20.\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 395/5000\n",
      "\n",
      "Epoch 00395: LearningRateScheduler reducing learning rate to 7.741006159285781e-20.\n",
      "11/11 [==============================] - 0s 23ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 396/5000\n",
      "\n",
      "Epoch 00396: LearningRateScheduler reducing learning rate to 7.004352026168645e-20.\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 397/5000\n",
      "\n",
      "Epoch 00397: LearningRateScheduler reducing learning rate to 6.33779980237337e-20.\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 398/5000\n",
      "\n",
      "Epoch 00398: LearningRateScheduler reducing learning rate to 5.734678409208326e-20.\n",
      "11/11 [==============================] - 0s 27ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 399/5000\n",
      "\n",
      "Epoch 00399: LearningRateScheduler reducing learning rate to 5.188951605054619e-20.\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 400/5000\n",
      "\n",
      "Epoch 00400: LearningRateScheduler reducing learning rate to 4.6951575726311635e-20.\n",
      "11/11 [==============================] - 0s 25ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 401/5000\n",
      "\n",
      "Epoch 00401: LearningRateScheduler reducing learning rate to 4.248354255291589e-20.\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 402/5000\n",
      "\n",
      "Epoch 00402: LearningRateScheduler reducing learning rate to 3.8440698952601176e-20.\n",
      "11/11 [==============================] - 0s 15ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 403/5000\n",
      "\n",
      "Epoch 00403: LearningRateScheduler reducing learning rate to 3.478258278776922e-20.\n",
      "11/11 [==============================] - 0s 25ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 404/5000\n",
      "\n",
      "Epoch 00404: LearningRateScheduler reducing learning rate to 3.1472582402307064e-20.\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 405/5000\n",
      "\n",
      "Epoch 00405: LearningRateScheduler reducing learning rate to 2.847757019982746e-20.\n",
      "11/11 [==============================] - 0s 18ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 406/5000\n",
      "\n",
      "Epoch 00406: LearningRateScheduler reducing learning rate to 2.576757109154981e-20.\n",
      "11/11 [==============================] - 0s 21ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 407/5000\n",
      "\n",
      "Epoch 00407: LearningRateScheduler reducing learning rate to 2.331546249553593e-20.\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 408/5000\n",
      "\n",
      "Epoch 00408: LearningRateScheduler reducing learning rate to 2.1096702884774953e-20.\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 409/5000\n",
      "\n",
      "Epoch 00409: LearningRateScheduler reducing learning rate to 1.908908616733152e-20.\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 410/5000\n",
      "\n",
      "Epoch 00410: LearningRateScheduler reducing learning rate to 1.7272519440314178e-20.\n",
      "11/11 [==============================] - 0s 15ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 411/5000\n",
      "\n",
      "Epoch 00411: LearningRateScheduler reducing learning rate to 1.562882189334989e-20.\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 412/5000\n",
      "\n",
      "Epoch 00412: LearningRateScheduler reducing learning rate to 1.414154284892257e-20.\n",
      "11/11 [==============================] - 0s 15ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 413/5000\n",
      "\n",
      "Epoch 00413: LearningRateScheduler reducing learning rate to 1.2795797118463968e-20.\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 414/5000\n",
      "\n",
      "Epoch 00414: LearningRateScheduler reducing learning rate to 1.1578116026382892e-20.\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 415/5000\n",
      "\n",
      "Epoch 00415: LearningRateScheduler reducing learning rate to 1.0476312611033044e-20.\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 416/5000\n",
      "\n",
      "Epoch 00416: LearningRateScheduler reducing learning rate to 9.479359653504756e-21.\n",
      "11/11 [==============================] - 0s 16ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 417/5000\n",
      "\n",
      "Epoch 00417: LearningRateScheduler reducing learning rate to 8.57727931351148e-21.\n",
      "11/11 [==============================] - 0s 15ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 418/5000\n",
      "\n",
      "Epoch 00418: LearningRateScheduler reducing learning rate to 7.761043267810964e-21.\n",
      "11/11 [==============================] - 0s 15ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 419/5000\n",
      "\n",
      "Epoch 00419: LearningRateScheduler reducing learning rate to 7.02248235171143e-21.\n",
      "11/11 [==============================] - 0s 26ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 420/5000\n",
      "\n",
      "Epoch 00420: LearningRateScheduler reducing learning rate to 6.3542047993256535e-21.\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 421/5000\n",
      "\n",
      "Epoch 00421: LearningRateScheduler reducing learning rate to 5.74952226429356e-21.\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 422/5000\n",
      "\n",
      "Epoch 00422: LearningRateScheduler reducing learning rate to 5.202382880563641e-21.\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 423/5000\n",
      "\n",
      "Epoch 00423: LearningRateScheduler reducing learning rate to 4.7073106932836764e-21.\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 424/5000\n",
      "\n",
      "Epoch 00424: LearningRateScheduler reducing learning rate to 4.259350853603858e-21.\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 425/5000\n",
      "\n",
      "Epoch 00425: LearningRateScheduler reducing learning rate to 3.85402002888417e-21.\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 426/5000\n",
      "\n",
      "Epoch 00426: LearningRateScheduler reducing learning rate to 3.487261531994447e-21.\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 427/5000\n",
      "\n",
      "Epoch 00427: LearningRateScheduler reducing learning rate to 3.1554047206259756e-21.\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 428/5000\n",
      "\n",
      "Epoch 00428: LearningRateScheduler reducing learning rate to 2.855128260269682e-21.\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 429/5000\n",
      "\n",
      "Epoch 00429: LearningRateScheduler reducing learning rate to 2.5834268831839163e-21.\n",
      "11/11 [==============================] - 0s 17ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 430/5000\n",
      "\n",
      "Epoch 00430: LearningRateScheduler reducing learning rate to 2.337581310664818e-21.\n",
      "11/11 [==============================] - 0s 17ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 431/5000\n",
      "\n",
      "Epoch 00431: LearningRateScheduler reducing learning rate to 2.1151310375910805e-21.\n",
      "11/11 [==============================] - 0s 16ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 432/5000\n",
      "\n",
      "Epoch 00432: LearningRateScheduler reducing learning rate to 1.913849706861631e-21.\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 433/5000\n",
      "\n",
      "Epoch 00433: LearningRateScheduler reducing learning rate to 1.7317228272655537e-21.\n",
      "11/11 [==============================] - 0s 15ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 434/5000\n",
      "\n",
      "Epoch 00434: LearningRateScheduler reducing learning rate to 1.5669276117768932e-21.\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 435/5000\n",
      "\n",
      "Epoch 00435: LearningRateScheduler reducing learning rate to 1.4178147344894545e-21.\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 436/5000\n",
      "\n",
      "Epoch 00436: LearningRateScheduler reducing learning rate to 1.282891823608785e-21.\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 437/5000\n",
      "\n",
      "Epoch 00437: LearningRateScheduler reducing learning rate to 1.1608085252936149e-21.\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 438/5000\n",
      "\n",
      "Epoch 00438: LearningRateScheduler reducing learning rate to 1.050342988860803e-21.\n",
      "11/11 [==============================] - 0s 18ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 439/5000\n",
      "\n",
      "Epoch 00439: LearningRateScheduler reducing learning rate to 9.503896380929803e-22.\n",
      "11/11 [==============================] - 0s 23ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 440/5000\n",
      "\n",
      "Epoch 00440: LearningRateScheduler reducing learning rate to 8.599481062601812e-22.\n",
      "11/11 [==============================] - 0s 25ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 441/5000\n",
      "\n",
      "Epoch 00441: LearningRateScheduler reducing learning rate to 7.781132241133797e-22.\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 442/5000\n",
      "\n",
      "Epoch 00442: LearningRateScheduler reducing learning rate to 7.040659606463854e-22.\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 443/5000\n",
      "\n",
      "Epoch 00443: LearningRateScheduler reducing learning rate to 6.37065225958282e-22.\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 444/5000\n",
      "\n",
      "Epoch 00444: LearningRateScheduler reducing learning rate to 5.764404541765862e-22.\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 445/5000\n",
      "\n",
      "Epoch 00445: LearningRateScheduler reducing learning rate to 5.215848922086174e-22.\n",
      "11/11 [==============================] - 0s 18ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 446/5000\n",
      "\n",
      "Epoch 00446: LearningRateScheduler reducing learning rate to 4.719495271526123e-22.\n",
      "11/11 [==============================] - 0s 15ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 447/5000\n",
      "\n",
      "Epoch 00447: LearningRateScheduler reducing learning rate to 4.270375915920611e-22.\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 448/5000\n",
      "\n",
      "Epoch 00448: LearningRateScheduler reducing learning rate to 3.8639959178045464e-22.\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 449/5000\n",
      "\n",
      "Epoch 00449: LearningRateScheduler reducing learning rate to 3.496288089567749e-22.\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 450/5000\n",
      "\n",
      "Epoch 00450: LearningRateScheduler reducing learning rate to 3.163572287674355e-22.\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 451/5000\n",
      "\n",
      "Epoch 00451: LearningRateScheduler reducing learning rate to 2.862518580549394e-22.\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 452/5000\n",
      "\n",
      "Epoch 00452: LearningRateScheduler reducing learning rate to 2.5901139215042697e-22.\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 453/5000\n",
      "\n",
      "Epoch 00453: LearningRateScheduler reducing learning rate to 2.343631993152914e-22.\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 454/5000\n",
      "\n",
      "Epoch 00454: LearningRateScheduler reducing learning rate to 2.1206059215109493e-22.\n",
      "11/11 [==============================] - 0s 18ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 455/5000\n",
      "\n",
      "Epoch 00455: LearningRateScheduler reducing learning rate to 1.9188035866917316e-22.\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 456/5000\n",
      "\n",
      "Epoch 00456: LearningRateScheduler reducing learning rate to 1.7362052831002947e-22.\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 457/5000\n",
      "\n",
      "Epoch 00457: LearningRateScheduler reducing learning rate to 1.5709835055408608e-22.\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 458/5000\n",
      "\n",
      "Epoch 00458: LearningRateScheduler reducing learning rate to 1.4214846589306708e-22.\n",
      "11/11 [==============================] - 0s 18ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 459/5000\n",
      "\n",
      "Epoch 00459: LearningRateScheduler reducing learning rate to 1.286212508564553e-22.\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 460/5000\n",
      "\n",
      "Epoch 00460: LearningRateScheduler reducing learning rate to 1.163813205295103e-22.\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 461/5000\n",
      "\n",
      "Epoch 00461: LearningRateScheduler reducing learning rate to 1.0530617357553812e-22.\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 462/5000\n",
      "\n",
      "Epoch 00462: LearningRateScheduler reducing learning rate to 9.528496620133637e-23.\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 463/5000\n",
      "\n",
      "Epoch 00463: LearningRateScheduler reducing learning rate to 8.621740279526076e-23.\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 464/5000\n",
      "\n",
      "Epoch 00464: LearningRateScheduler reducing learning rate to 7.801273213502995e-23.\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 465/5000\n",
      "\n",
      "Epoch 00465: LearningRateScheduler reducing learning rate to 7.058883911899133e-23.\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 466/5000\n",
      "\n",
      "Epoch 00466: LearningRateScheduler reducing learning rate to 6.387142293058423e-23.\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 467/5000\n",
      "\n",
      "Epoch 00467: LearningRateScheduler reducing learning rate to 5.779325341079254e-23.\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 468/5000\n",
      "\n",
      "Epoch 00468: LearningRateScheduler reducing learning rate to 5.2293498196119355e-23.\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 469/5000\n",
      "\n",
      "Epoch 00469: LearningRateScheduler reducing learning rate to 4.731711388784468e-23.\n",
      "11/11 [==============================] - 0s 23ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 470/5000\n",
      "\n",
      "Epoch 00470: LearningRateScheduler reducing learning rate to 4.281429515919076e-23.\n",
      "11/11 [==============================] - 0s 17ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 471/5000\n",
      "\n",
      "Epoch 00471: LearningRateScheduler reducing learning rate to 3.873997628687187e-23.\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 472/5000\n",
      "\n",
      "Epoch 00472: LearningRateScheduler reducing learning rate to 3.50533801181874e-23.\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 473/5000\n",
      "\n",
      "Epoch 00473: LearningRateScheduler reducing learning rate to 3.1717609959573677e-23.\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 474/5000\n",
      "\n",
      "Epoch 00474: LearningRateScheduler reducing learning rate to 2.869928030209224e-23.\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 475/5000\n",
      "\n",
      "Epoch 00475: LearningRateScheduler reducing learning rate to 2.596818268803538e-23.\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 476/5000\n",
      "\n",
      "Epoch 00476: LearningRateScheduler reducing learning rate to 2.349698337452817e-23.\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 477/5000\n",
      "\n",
      "Epoch 00477: LearningRateScheduler reducing learning rate to 2.126094976824191e-23.\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 478/5000\n",
      "\n",
      "Epoch 00478: LearningRateScheduler reducing learning rate to 1.9237702893288215e-23.\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 479/5000\n",
      "\n",
      "Epoch 00479: LearningRateScheduler reducing learning rate to 1.7406993414905794e-23.\n",
      "11/11 [==============================] - 0s 15ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 480/5000\n",
      "\n",
      "Epoch 00480: LearningRateScheduler reducing learning rate to 1.5750498977312285e-23.\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 481/5000\n",
      "\n",
      "Epoch 00481: LearningRateScheduler reducing learning rate to 1.4251640827409352e-23.\n",
      "11/11 [==============================] - 0s 38ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 482/5000\n",
      "\n",
      "Epoch 00482: LearningRateScheduler reducing learning rate to 1.2895417889048926e-23.\n",
      "11/11 [==============================] - 0s 17ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 483/5000\n",
      "\n",
      "Epoch 00483: LearningRateScheduler reducing learning rate to 1.1668256627221738e-23.\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 484/5000\n",
      "\n",
      "Epoch 00484: LearningRateScheduler reducing learning rate to 1.0557875199556276e-23.\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 485/5000\n",
      "\n",
      "Epoch 00485: LearningRateScheduler reducing learning rate to 9.553160535512378e-24.\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 486/5000\n",
      "\n",
      "Epoch 00486: LearningRateScheduler reducing learning rate to 8.644057113036095e-24.\n",
      "11/11 [==============================] - 0s 17ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 487/5000\n",
      "\n",
      "Epoch 00487: LearningRateScheduler reducing learning rate to 7.821466319514939e-24.\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 488/5000\n",
      "\n",
      "Epoch 00488: LearningRateScheduler reducing learning rate to 7.077155389805108e-24.\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 489/5000\n",
      "\n",
      "Epoch 00489: LearningRateScheduler reducing learning rate to 6.40367500995052e-24.\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 490/5000\n",
      "\n",
      "Epoch 00490: LearningRateScheduler reducing learning rate to 5.7942847619450174e-24.\n",
      "11/11 [==============================] - 0s 20ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 491/5000\n",
      "\n",
      "Epoch 00491: LearningRateScheduler reducing learning rate to 5.242885663363464e-24.\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 492/5000\n",
      "\n",
      "Epoch 00492: LearningRateScheduler reducing learning rate to 4.743959126695539e-24.\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 493/5000\n",
      "\n",
      "Epoch 00493: LearningRateScheduler reducing learning rate to 4.292511727467311e-24.\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 494/5000\n",
      "\n",
      "Epoch 00494: LearningRateScheduler reducing learning rate to 3.8840252283705925e-24.\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 495/5000\n",
      "\n",
      "Epoch 00495: LearningRateScheduler reducing learning rate to 3.51441135922537e-24.\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 496/5000\n",
      "\n",
      "Epoch 00496: LearningRateScheduler reducing learning rate to 3.1799709001977496e-24.\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 497/5000\n",
      "\n",
      "Epoch 00497: LearningRateScheduler reducing learning rate to 2.8773566587644137e-24.\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 498/5000\n",
      "\n",
      "Epoch 00498: LearningRateScheduler reducing learning rate to 2.603539969884964e-24.\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 499/5000\n",
      "\n",
      "Epoch 00499: LearningRateScheduler reducing learning rate to 2.3557803841041277e-24.\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 500/5000\n",
      "\n",
      "Epoch 00500: LearningRateScheduler reducing learning rate to 2.1315982402125367e-24.\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 501/5000\n",
      "\n",
      "Epoch 00501: LearningRateScheduler reducing learning rate to 1.9287498479639177e-24.\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 502/5000\n",
      "\n",
      "Epoch 00502: LearningRateScheduler reducing learning rate to 1.7452050324689182e-24.\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 503/5000\n",
      "\n",
      "Epoch 00503: LearningRateScheduler reducing learning rate to 1.579126815522537e-24.\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 504/5000\n",
      "\n",
      "Epoch 00504: LearningRateScheduler reducing learning rate to 1.4288530305087574e-24.\n",
      "11/11 [==============================] - 0s 31ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 505/5000\n",
      "\n",
      "Epoch 00505: LearningRateScheduler reducing learning rate to 1.2928796868783984e-24.\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 506/5000\n",
      "\n",
      "Epoch 00506: LearningRateScheduler reducing learning rate to 1.1698459177061965e-24.\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 507/5000\n",
      "\n",
      "Epoch 00507: LearningRateScheduler reducing learning rate to 1.058520359677181e-24.\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 508/5000\n",
      "\n",
      "Epoch 00508: LearningRateScheduler reducing learning rate to 9.577888291887944e-25.\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 509/5000\n",
      "\n",
      "Epoch 00509: LearningRateScheduler reducing learning rate to 8.66643171226872e-25.\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 510/5000\n",
      "\n",
      "Epoch 00510: LearningRateScheduler reducing learning rate to 7.841711694114179e-25.\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 511/5000\n",
      "\n",
      "Epoch 00511: LearningRateScheduler reducing learning rate to 7.095474162284704e-25.\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 512/5000\n",
      "\n",
      "Epoch 00512: LearningRateScheduler reducing learning rate to 6.420250520742546e-25.\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 513/5000\n",
      "\n",
      "Epoch 00513: LearningRateScheduler reducing learning rate to 5.809282904332702e-25.\n",
      "11/11 [==============================] - 0s 16ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 514/5000\n",
      "\n",
      "Epoch 00514: LearningRateScheduler reducing learning rate to 5.256456543796835e-25.\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 515/5000\n",
      "\n",
      "Epoch 00515: LearningRateScheduler reducing learning rate to 4.756238567107345e-25.\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 516/5000\n",
      "\n",
      "Epoch 00516: LearningRateScheduler reducing learning rate to 4.303622624624487e-25.\n",
      "11/11 [==============================] - 0s 17ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 517/5000\n",
      "\n",
      "Epoch 00517: LearningRateScheduler reducing learning rate to 3.8940787838663545e-25.\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 518/5000\n",
      "\n",
      "Epoch 00518: LearningRateScheduler reducing learning rate to 3.523508192422237e-25.\n",
      "11/11 [==============================] - 0s 20ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 519/5000\n",
      "\n",
      "Epoch 00519: LearningRateScheduler reducing learning rate to 3.188202055259883e-25.\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 520/5000\n",
      "\n",
      "Epoch 00520: LearningRateScheduler reducing learning rate to 2.884804515858288e-25.\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 521/5000\n",
      "\n",
      "Epoch 00521: LearningRateScheduler reducing learning rate to 2.610279069667705e-25.\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 522/5000\n",
      "\n",
      "Epoch 00522: LearningRateScheduler reducing learning rate to 2.361878173751429e-25.\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 523/5000\n",
      "\n",
      "Epoch 00523: LearningRateScheduler reducing learning rate to 2.137115748452728e-25.\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 524/5000\n",
      "\n",
      "Epoch 00524: LearningRateScheduler reducing learning rate to 1.9337422958739505e-25.\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 525/5000\n",
      "\n",
      "Epoch 00525: LearningRateScheduler reducing learning rate to 1.7497223861455118e-25.\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 526/5000\n",
      "\n",
      "Epoch 00526: LearningRateScheduler reducing learning rate to 1.583214286159632e-25.\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 527/5000\n",
      "\n",
      "Epoch 00527: LearningRateScheduler reducing learning rate to 1.4325515268863242e-25.\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 528/5000\n",
      "\n",
      "Epoch 00528: LearningRateScheduler reducing learning rate to 1.2962262247912913e-25.\n",
      "11/11 [==============================] - 0s 16ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 529/5000\n",
      "\n",
      "Epoch 00529: LearningRateScheduler reducing learning rate to 1.1728739904306496e-25.\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 530/5000\n",
      "\n",
      "Epoch 00530: LearningRateScheduler reducing learning rate to 1.0612602731828004e-25.\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 531/5000\n",
      "\n",
      "Epoch 00531: LearningRateScheduler reducing learning rate to 9.602680054508675e-26.\n",
      "11/11 [==============================] - 0s 17ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 532/5000\n",
      "\n",
      "Epoch 00532: LearningRateScheduler reducing learning rate to 8.688864226747026e-26.\n",
      "11/11 [==============================] - 0s 15ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 533/5000\n",
      "\n",
      "Epoch 00533: LearningRateScheduler reducing learning rate to 7.862009472594782e-26.\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 534/5000\n",
      "\n",
      "Epoch 00534: LearningRateScheduler reducing learning rate to 7.11384035175691e-26.\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 535/5000\n",
      "\n",
      "Epoch 00535: LearningRateScheduler reducing learning rate to 6.436868936203734e-26.\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 536/5000\n",
      "\n",
      "Epoch 00536: LearningRateScheduler reducing learning rate to 5.824319868470494e-26.\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 537/5000\n",
      "\n",
      "Epoch 00537: LearningRateScheduler reducing learning rate to 5.270062551602374e-26.\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 538/5000\n",
      "\n",
      "Epoch 00538: LearningRateScheduler reducing learning rate to 4.7685497920798865e-26.\n",
      "11/11 [==============================] - 0s 25ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 539/5000\n",
      "\n",
      "Epoch 00539: LearningRateScheduler reducing learning rate to 4.31476228164147e-26.\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 540/5000\n",
      "\n",
      "Epoch 00540: LearningRateScheduler reducing learning rate to 3.904158362359408e-26.\n",
      "11/11 [==============================] - 0s 15ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 541/5000\n",
      "\n",
      "Epoch 00541: LearningRateScheduler reducing learning rate to 3.532628572200807e-26.\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 542/5000\n",
      "\n",
      "Epoch 00542: LearningRateScheduler reducing learning rate to 3.1964545161502324e-26.\n",
      "11/11 [==============================] - 0s 15ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 543/5000\n",
      "\n",
      "Epoch 00543: LearningRateScheduler reducing learning rate to 2.892271651262754e-26.\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 544/5000\n",
      "\n",
      "Epoch 00544: LearningRateScheduler reducing learning rate to 2.617035613187188e-26.\n",
      "11/11 [==============================] - 0s 16ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 545/5000\n",
      "\n",
      "Epoch 00545: LearningRateScheduler reducing learning rate to 2.3679917471444464e-26.\n",
      "11/11 [==============================] - 0s 26ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 546/5000\n",
      "\n",
      "Epoch 00546: LearningRateScheduler reducing learning rate to 2.142647538416654e-26.\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 547/5000\n",
      "\n",
      "Epoch 00547: LearningRateScheduler reducing learning rate to 1.9387476664220267e-26.\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 548/5000\n",
      "\n",
      "Epoch 00548: LearningRateScheduler reducing learning rate to 1.7542514327085462e-26.\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 549/5000\n",
      "\n",
      "Epoch 00549: LearningRateScheduler reducing learning rate to 1.5873123369578816e-26.\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 550/5000\n",
      "\n",
      "Epoch 00550: LearningRateScheduler reducing learning rate to 1.4362595965895924e-26.\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 551/5000\n",
      "\n",
      "Epoch 00551: LearningRateScheduler reducing learning rate to 1.299581425007503e-26.\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 552/5000\n",
      "\n",
      "Epoch 00552: LearningRateScheduler reducing learning rate to 1.1759099011312803e-26.\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 553/5000\n",
      "\n",
      "Epoch 00553: LearningRateScheduler reducing learning rate to 1.0640072787825469e-26.\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 554/5000\n",
      "\n",
      "Epoch 00554: LearningRateScheduler reducing learning rate to 9.627535989050657e-27.\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 555/5000\n",
      "\n",
      "Epoch 00555: LearningRateScheduler reducing learning rate to 8.711354806380864e-27.\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 556/5000\n",
      "\n",
      "Epoch 00556: LearningRateScheduler reducing learning rate to 7.88235979060085e-27.\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 557/5000\n",
      "\n",
      "Epoch 00557: LearningRateScheduler reducing learning rate to 7.132254080957731e-27.\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 558/5000\n",
      "\n",
      "Epoch 00558: LearningRateScheduler reducing learning rate to 6.4535303673902196e-27.\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 559/5000\n",
      "\n",
      "Epoch 00559: LearningRateScheduler reducing learning rate to 5.8393957548460155e-27.\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 560/5000\n",
      "\n",
      "Epoch 00560: LearningRateScheduler reducing learning rate to 5.283703777705005e-27.\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 561/5000\n",
      "\n",
      "Epoch 00561: LearningRateScheduler reducing learning rate to 4.780892883885469e-27.\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 562/5000\n",
      "\n",
      "Epoch 00562: LearningRateScheduler reducing learning rate to 4.325930772961415e-27.\n",
      "11/11 [==============================] - 0s 18ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 563/5000\n",
      "\n",
      "Epoch 00563: LearningRateScheduler reducing learning rate to 3.914264031208703e-27.\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 564/5000\n",
      "\n",
      "Epoch 00564: LearningRateScheduler reducing learning rate to 3.541772559509905e-27.\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 565/5000\n",
      "\n",
      "Epoch 00565: LearningRateScheduler reducing learning rate to 3.2047283380175497e-27.\n",
      "11/11 [==============================] - 0s 18ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 566/5000\n",
      "\n",
      "Epoch 00566: LearningRateScheduler reducing learning rate to 2.899758114878488e-27.\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 567/5000\n",
      "\n",
      "Epoch 00567: LearningRateScheduler reducing learning rate to 2.623809645595469e-27.\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 568/5000\n",
      "\n",
      "Epoch 00568: LearningRateScheduler reducing learning rate to 2.3741211451384467e-27.\n",
      "11/11 [==============================] - 0s 17ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 569/5000\n",
      "\n",
      "Epoch 00569: LearningRateScheduler reducing learning rate to 2.148193647071645e-27.\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 570/5000\n",
      "\n",
      "Epoch 00570: LearningRateScheduler reducing learning rate to 1.943765993057556e-27.\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 571/5000\n",
      "\n",
      "Epoch 00571: LearningRateScheduler reducing learning rate to 1.7587922024243115e-27.\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 572/5000\n",
      "\n",
      "Epoch 00572: LearningRateScheduler reducing learning rate to 1.5914209953033905e-27.\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 573/5000\n",
      "\n",
      "Epoch 00573: LearningRateScheduler reducing learning rate to 1.439977264398535e-27.\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 574/5000\n",
      "\n",
      "Epoch 00574: LearningRateScheduler reducing learning rate to 1.3029453099488527e-27.\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 575/5000\n",
      "\n",
      "Epoch 00575: LearningRateScheduler reducing learning rate to 1.1789536700961813e-27.\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 576/5000\n",
      "\n",
      "Epoch 00576: LearningRateScheduler reducing learning rate to 1.0667613948338533e-27.\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 577/5000\n",
      "\n",
      "Epoch 00577: LearningRateScheduler reducing learning rate to 9.652456261619012e-28.\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 578/5000\n",
      "\n",
      "Epoch 00578: LearningRateScheduler reducing learning rate to 8.733903601468365e-28.\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 579/5000\n",
      "\n",
      "Epoch 00579: LearningRateScheduler reducing learning rate to 7.902762784127594e-28.\n",
      "11/11 [==============================] - 0s 24ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 580/5000\n",
      "\n",
      "Epoch 00580: LearningRateScheduler reducing learning rate to 7.150715472940673e-28.\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 581/5000\n",
      "\n",
      "Epoch 00581: LearningRateScheduler reducing learning rate to 6.47023492564546e-28.\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 582/5000\n",
      "\n",
      "Epoch 00582: LearningRateScheduler reducing learning rate to 5.854510664207118e-28.\n",
      "11/11 [==============================] - 0s 24ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 583/5000\n",
      "\n",
      "Epoch 00583: LearningRateScheduler reducing learning rate to 5.297380313265153e-28.\n",
      "11/11 [==============================] - 0s 17ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 584/5000\n",
      "\n",
      "Epoch 00584: LearningRateScheduler reducing learning rate to 4.793267925009357e-28.\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 585/5000\n",
      "\n",
      "Epoch 00585: LearningRateScheduler reducing learning rate to 4.337128173220042e-28.\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 586/5000\n",
      "\n",
      "Epoch 00586: LearningRateScheduler reducing learning rate to 3.924395857947463e-28.\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 587/5000\n",
      "\n",
      "Epoch 00587: LearningRateScheduler reducing learning rate to 3.5509402154561915e-28.\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 588/5000\n",
      "\n",
      "Epoch 00588: LearningRateScheduler reducing learning rate to 3.2130235761534297e-28.\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 589/5000\n",
      "\n",
      "Epoch 00589: LearningRateScheduler reducing learning rate to 2.9072639567353304e-28.\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 590/5000\n",
      "\n",
      "Epoch 00590: LearningRateScheduler reducing learning rate to 2.6306012121614005e-28.\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 591/5000\n",
      "\n",
      "Epoch 00591: LearningRateScheduler reducing learning rate to 2.380266408694401e-28.\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 592/5000\n",
      "\n",
      "Epoch 00592: LearningRateScheduler reducing learning rate to 2.1537541114807646e-28.\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 593/5000\n",
      "\n",
      "Epoch 00593: LearningRateScheduler reducing learning rate to 1.9487973093165845e-28.\n",
      "11/11 [==============================] - 0s 16ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 594/5000\n",
      "\n",
      "Epoch 00594: LearningRateScheduler reducing learning rate to 1.763344725637441e-28.\n",
      "11/11 [==============================] - 0s 23ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 595/5000\n",
      "\n",
      "Epoch 00595: LearningRateScheduler reducing learning rate to 1.5955402886531074e-28.\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 596/5000\n",
      "\n",
      "Epoch 00596: LearningRateScheduler reducing learning rate to 1.4437045551572356e-28.\n",
      "11/11 [==============================] - 0s 20ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 597/5000\n",
      "\n",
      "Epoch 00597: LearningRateScheduler reducing learning rate to 1.3063179020952248e-28.\n",
      "11/11 [==============================] - 0s 18ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 598/5000\n",
      "\n",
      "Epoch 00598: LearningRateScheduler reducing learning rate to 1.182005317665993e-28.\n",
      "11/11 [==============================] - 0s 17ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 599/5000\n",
      "\n",
      "Epoch 00599: LearningRateScheduler reducing learning rate to 1.0695226397416698e-28.\n",
      "11/11 [==============================] - 0s 15ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 600/5000\n",
      "\n",
      "Epoch 00600: LearningRateScheduler reducing learning rate to 9.677441038748547e-29.\n",
      "11/11 [==============================] - 0s 24ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 601/5000\n",
      "\n",
      "Epoch 00601: LearningRateScheduler reducing learning rate to 8.75651076269652e-29.\n",
      "11/11 [==============================] - 0s 16ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 602/5000\n",
      "\n",
      "Epoch 00602: LearningRateScheduler reducing learning rate to 7.923218589522399e-29.\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 603/5000\n",
      "\n",
      "Epoch 00603: LearningRateScheduler reducing learning rate to 7.169224651077954e-29.\n",
      "11/11 [==============================] - 0s 15ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 604/5000\n",
      "\n",
      "Epoch 00604: LearningRateScheduler reducing learning rate to 6.486982722601121e-29.\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 605/5000\n",
      "\n",
      "Epoch 00605: LearningRateScheduler reducing learning rate to 5.869664697562269e-29.\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 606/5000\n",
      "\n",
      "Epoch 00606: LearningRateScheduler reducing learning rate to 5.311092249679096e-29.\n",
      "11/11 [==============================] - ETA: 0s - loss: 113.8599 - mae: 7.7975 - mse: 113.859 - 0s 18ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 607/5000\n",
      "\n",
      "Epoch 00607: LearningRateScheduler reducing learning rate to 4.805674998150422e-29.\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 608/5000\n",
      "\n",
      "Epoch 00608: LearningRateScheduler reducing learning rate to 4.348354557246386e-29.\n",
      "11/11 [==============================] - 0s 17ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 609/5000\n",
      "\n",
      "Epoch 00609: LearningRateScheduler reducing learning rate to 3.934553910283713e-29.\n",
      "11/11 [==============================] - 0s 37ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 610/5000\n",
      "\n",
      "Epoch 00610: LearningRateScheduler reducing learning rate to 3.560131601304398e-29.\n",
      "11/11 [==============================] - 0s 24ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 611/5000\n",
      "\n",
      "Epoch 00611: LearningRateScheduler reducing learning rate to 3.221340285992516e-29.\n",
      "11/11 [==============================] - 0s 20ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 612/5000\n",
      "\n",
      "Epoch 00612: LearningRateScheduler reducing learning rate to 2.914789226992684e-29.\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 613/5000\n",
      "\n",
      "Epoch 00613: LearningRateScheduler reducing learning rate to 2.6374103582710865e-29.\n",
      "11/11 [==============================] - 0s 18ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 614/5000\n",
      "\n",
      "Epoch 00614: LearningRateScheduler reducing learning rate to 2.3864275788793018e-29.\n",
      "11/11 [==============================] - 0s 16ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 615/5000\n",
      "\n",
      "Epoch 00615: LearningRateScheduler reducing learning rate to 2.1593289688029506e-29.\n",
      "11/11 [==============================] - 0s 25ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 616/5000\n",
      "\n",
      "Epoch 00616: LearningRateScheduler reducing learning rate to 1.9538416488219242e-29.\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 617/5000\n",
      "\n",
      "Epoch 00617: LearningRateScheduler reducing learning rate to 1.7679090327711495e-29.\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 618/5000\n",
      "\n",
      "Epoch 00618: LearningRateScheduler reducing learning rate to 1.5996702445350952e-29.\n",
      "11/11 [==============================] - 0s 18ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 619/5000\n",
      "\n",
      "Epoch 00619: LearningRateScheduler reducing learning rate to 1.4474414937740856e-29.\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 620/5000\n",
      "\n",
      "Epoch 00620: LearningRateScheduler reducing learning rate to 1.309699223984654e-29.\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 621/5000\n",
      "\n",
      "Epoch 00621: LearningRateScheduler reducing learning rate to 1.185064864233981e-29.\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 622/5000\n",
      "\n",
      "Epoch 00622: LearningRateScheduler reducing learning rate to 1.0722910319586089e-29.\n",
      "11/11 [==============================] - 0s 31ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 623/5000\n",
      "\n",
      "Epoch 00623: LearningRateScheduler reducing learning rate to 9.702490487405409e-30.\n",
      "11/11 [==============================] - 0s 23ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 624/5000\n",
      "\n",
      "Epoch 00624: LearningRateScheduler reducing learning rate to 8.779176441142356e-30.\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 625/5000\n",
      "\n",
      "Epoch 00625: LearningRateScheduler reducing learning rate to 7.943727343485363e-30.\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 626/5000\n",
      "\n",
      "Epoch 00626: LearningRateScheduler reducing learning rate to 7.187781739060989e-30.\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 627/5000\n",
      "\n",
      "Epoch 00627: LearningRateScheduler reducing learning rate to 6.503773870177955e-30.\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 628/5000\n",
      "\n",
      "Epoch 00628: LearningRateScheduler reducing learning rate to 5.884857956181553e-30.\n",
      "11/11 [==============================] - 0s 21ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 629/5000\n",
      "\n",
      "Epoch 00629: LearningRateScheduler reducing learning rate to 5.3248396785796826e-30.\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 630/5000\n",
      "\n",
      "Epoch 00630: LearningRateScheduler reducing learning rate to 4.818114186221462e-30.\n",
      "11/11 [==============================] - 0s 38ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 631/5000\n",
      "\n",
      "Epoch 00631: LearningRateScheduler reducing learning rate to 4.359610000063081e-30.\n",
      "11/11 [==============================] - 0s 27ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 632/5000\n",
      "\n",
      "Epoch 00632: LearningRateScheduler reducing learning rate to 3.944738256100822e-30.\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 633/5000\n",
      "\n",
      "Epoch 00633: LearningRateScheduler reducing learning rate to 3.569346778477937e-30.\n",
      "11/11 [==============================] - 0s 17ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 634/5000\n",
      "\n",
      "Epoch 00634: LearningRateScheduler reducing learning rate to 3.2296785231129416e-30.\n",
      "11/11 [==============================] - 0s 21ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 635/5000\n",
      "\n",
      "Epoch 00635: LearningRateScheduler reducing learning rate to 2.9223339759397015e-30.\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 636/5000\n",
      "\n",
      "Epoch 00636: LearningRateScheduler reducing learning rate to 2.6442371294280542e-30.\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 637/5000\n",
      "\n",
      "Epoch 00637: LearningRateScheduler reducing learning rate to 2.392604696866495e-30.\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 638/5000\n",
      "\n",
      "Epoch 00638: LearningRateScheduler reducing learning rate to 2.1649182562933856e-30.\n",
      "11/11 [==============================] - 0s 20ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 639/5000\n",
      "\n",
      "Epoch 00639: LearningRateScheduler reducing learning rate to 1.9588990452834163e-30.\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 640/5000\n",
      "\n",
      "Epoch 00640: LearningRateScheduler reducing learning rate to 1.77248515432735e-30.\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 641/5000\n",
      "\n",
      "Epoch 00641: LearningRateScheduler reducing learning rate to 1.603810890548638e-30.\n",
      "11/11 [==============================] - 0s 18ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 642/5000\n",
      "\n",
      "Epoch 00642: LearningRateScheduler reducing learning rate to 1.45118810522197e-30.\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 643/5000\n",
      "\n",
      "Epoch 00643: LearningRateScheduler reducing learning rate to 1.3130892982135513e-30.\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 644/5000\n",
      "\n",
      "Epoch 00644: LearningRateScheduler reducing learning rate to 1.1881323302462067e-30.\n",
      "11/11 [==============================] - 0s 22ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 645/5000\n",
      "\n",
      "Epoch 00645: LearningRateScheduler reducing learning rate to 1.0750665899850165e-30.\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 646/5000\n",
      "\n",
      "Epoch 00646: LearningRateScheduler reducing learning rate to 9.727604774987714e-31.\n",
      "11/11 [==============================] - 0s 21ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 647/5000\n",
      "\n",
      "Epoch 00647: LearningRateScheduler reducing learning rate to 8.80190078827408e-31.\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 648/5000\n",
      "\n",
      "Epoch 00648: LearningRateScheduler reducing learning rate to 7.964289183070642e-31.\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 649/5000\n",
      "\n",
      "Epoch 00649: LearningRateScheduler reducing learning rate to 7.206386860901401e-31.\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 650/5000\n",
      "\n",
      "Epoch 00650: LearningRateScheduler reducing learning rate to 6.520608480586232e-31.\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 651/5000\n",
      "\n",
      "Epoch 00651: LearningRateScheduler reducing learning rate to 5.900090541597061e-31.\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 652/5000\n",
      "\n",
      "Epoch 00652: LearningRateScheduler reducing learning rate to 5.338622691837025e-31.\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 653/5000\n",
      "\n",
      "Epoch 00653: LearningRateScheduler reducing learning rate to 4.830585572350026e-31.\n",
      "11/11 [==============================] - 0s 17ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 654/5000\n",
      "\n",
      "Epoch 00654: LearningRateScheduler reducing learning rate to 4.370894576886981e-31.\n",
      "11/11 [==============================] - 0s 22ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 655/5000\n",
      "\n",
      "Epoch 00655: LearningRateScheduler reducing learning rate to 3.95494896345776e-31.\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 656/5000\n",
      "\n",
      "Epoch 00656: LearningRateScheduler reducing learning rate to 3.5785858085591343e-31.\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 657/5000\n",
      "\n",
      "Epoch 00657: LearningRateScheduler reducing learning rate to 3.238038343236746e-31.\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 658/5000\n",
      "\n",
      "Epoch 00658: LearningRateScheduler reducing learning rate to 2.92989825399579e-31.\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 659/5000\n",
      "\n",
      "Epoch 00659: LearningRateScheduler reducing learning rate to 2.651081571253632e-31.\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 660/5000\n",
      "\n",
      "Epoch 00660: LearningRateScheduler reducing learning rate to 2.398797803935831e-31.\n",
      "11/11 [==============================] - 0s 18ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 661/5000\n",
      "\n",
      "Epoch 00661: LearningRateScheduler reducing learning rate to 2.1705220113036394e-31.\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 662/5000\n",
      "\n",
      "Epoch 00662: LearningRateScheduler reducing learning rate to 1.963969532498186e-31.\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 663/5000\n",
      "\n",
      "Epoch 00663: LearningRateScheduler reducing learning rate to 1.7770731208869597e-31.\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 664/5000\n",
      "\n",
      "Epoch 00664: LearningRateScheduler reducing learning rate to 1.6079622543644703e-31.\n",
      "11/11 [==============================] - 0s 23ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 665/5000\n",
      "\n",
      "Epoch 00665: LearningRateScheduler reducing learning rate to 1.4549444145384157e-31.\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 666/5000\n",
      "\n",
      "Epoch 00666: LearningRateScheduler reducing learning rate to 1.3164881474367883e-31.\n",
      "11/11 [==============================] - 0s 20ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 667/5000\n",
      "\n",
      "Epoch 00667: LearningRateScheduler reducing learning rate to 1.1912077362016373e-31.\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 668/5000\n",
      "\n",
      "Epoch 00668: LearningRateScheduler reducing learning rate to 1.077849332369156e-31.\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 669/5000\n",
      "\n",
      "Epoch 00669: LearningRateScheduler reducing learning rate to 9.752784069326955e-32.\n",
      "11/11 [==============================] - 0s 15ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 670/5000\n",
      "\n",
      "Epoch 00670: LearningRateScheduler reducing learning rate to 8.824683955951964e-32.\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 671/5000\n",
      "\n",
      "Epoch 00671: LearningRateScheduler reducing learning rate to 7.984904245686979e-32.\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 672/5000\n",
      "\n",
      "Epoch 00672: LearningRateScheduler reducing learning rate to 7.225040140931716e-32.\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 673/5000\n",
      "\n",
      "Epoch 00673: LearningRateScheduler reducing learning rate to 6.537486666326857e-32.\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 674/5000\n",
      "\n",
      "Epoch 00674: LearningRateScheduler reducing learning rate to 5.915362555603739e-32.\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 675/5000\n",
      "\n",
      "Epoch 00675: LearningRateScheduler reducing learning rate to 5.352441381559037e-32.\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 676/5000\n",
      "\n",
      "Epoch 00676: LearningRateScheduler reducing learning rate to 4.8430892398787314e-32.\n",
      "11/11 [==============================] - 0s 16ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 677/5000\n",
      "\n",
      "Epoch 00677: LearningRateScheduler reducing learning rate to 4.3822083631295716e-32.\n",
      "11/11 [==============================] - 0s 16ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 678/5000\n",
      "\n",
      "Epoch 00678: LearningRateScheduler reducing learning rate to 3.965186100589773e-32.\n",
      "11/11 [==============================] - 0s 23ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 679/5000\n",
      "\n",
      "Epoch 00679: LearningRateScheduler reducing learning rate to 3.587848753289745e-32.\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 680/5000\n",
      "\n",
      "Epoch 00680: LearningRateScheduler reducing learning rate to 3.2464198022302015e-32.\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 681/5000\n",
      "\n",
      "Epoch 00681: LearningRateScheduler reducing learning rate to 2.937482111710803e-32.\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 682/5000\n",
      "\n",
      "Epoch 00682: LearningRateScheduler reducing learning rate to 2.6579437294871986e-32.\n",
      "11/11 [==============================] - 0s 16ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 683/5000\n",
      "\n",
      "Epoch 00683: LearningRateScheduler reducing learning rate to 2.4050069414740794e-32.\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 684/5000\n",
      "\n",
      "Epoch 00684: LearningRateScheduler reducing learning rate to 2.1761402712819785e-32.\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 685/5000\n",
      "\n",
      "Epoch 00685: LearningRateScheduler reducing learning rate to 1.9690531443508412e-32.\n",
      "11/11 [==============================] - 0s 16ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 686/5000\n",
      "\n",
      "Epoch 00686: LearningRateScheduler reducing learning rate to 1.781672963110013e-32.\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 687/5000\n",
      "\n",
      "Epoch 00687: LearningRateScheduler reducing learning rate to 1.612124363724928e-32.\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 688/5000\n",
      "\n",
      "Epoch 00688: LearningRateScheduler reducing learning rate to 1.4587104468257362e-32.\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 689/5000\n",
      "\n",
      "Epoch 00689: LearningRateScheduler reducing learning rate to 1.3198957943678875e-32.\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 690/5000\n",
      "\n",
      "Epoch 00690: LearningRateScheduler reducing learning rate to 1.194291102652351e-32.\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 691/5000\n",
      "\n",
      "Epoch 00691: LearningRateScheduler reducing learning rate to 1.0806392777072785e-32.\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 692/5000\n",
      "\n",
      "Epoch 00692: LearningRateScheduler reducing learning rate to 9.778028538688898e-33.\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 693/5000\n",
      "\n",
      "Epoch 00693: LearningRateScheduler reducing learning rate to 8.84752609642924e-33.\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 694/5000\n",
      "\n",
      "Epoch 00694: LearningRateScheduler reducing learning rate to 8.005572669098851e-33.\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 695/5000\n",
      "\n",
      "Epoch 00695: LearningRateScheduler reducing learning rate to 7.243741703806587e-33.\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 696/5000\n",
      "\n",
      "Epoch 00696: LearningRateScheduler reducing learning rate to 6.554408540191793e-33.\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 697/5000\n",
      "\n",
      "Epoch 00697: LearningRateScheduler reducing learning rate to 5.930674100259935e-33.\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 698/5000\n",
      "\n",
      "Epoch 00698: LearningRateScheduler reducing learning rate to 5.366295840091968e-33.\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 699/5000\n",
      "\n",
      "Epoch 00699: LearningRateScheduler reducing learning rate to 4.855625272365953e-33.\n",
      "11/11 [==============================] - 0s 16ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 700/5000\n",
      "\n",
      "Epoch 00700: LearningRateScheduler reducing learning rate to 4.3935514343977256e-33.\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 701/5000\n",
      "\n",
      "Epoch 00701: LearningRateScheduler reducing learning rate to 3.975449735908647e-33.\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 702/5000\n",
      "\n",
      "Epoch 00702: LearningRateScheduler reducing learning rate to 3.597135674571287e-33.\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 703/5000\n",
      "\n",
      "Epoch 00703: LearningRateScheduler reducing learning rate to 3.254822956104141e-33.\n",
      "11/11 [==============================] - 0s 18ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 704/5000\n",
      "\n",
      "Epoch 00704: LearningRateScheduler reducing learning rate to 2.9450855997654572e-33.\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 705/5000\n",
      "\n",
      "Epoch 00705: LearningRateScheduler reducing learning rate to 2.664823649986639e-33.\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 706/5000\n",
      "\n",
      "Epoch 00706: LearningRateScheduler reducing learning rate to 2.411232150975086e-33.\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 707/5000\n",
      "\n",
      "Epoch 00707: LearningRateScheduler reducing learning rate to 2.181773073773571e-33.\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 708/5000\n",
      "\n",
      "Epoch 00708: LearningRateScheduler reducing learning rate to 1.9741499148136685e-33.\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 709/5000\n",
      "\n",
      "Epoch 00709: LearningRateScheduler reducing learning rate to 1.7862847117359196e-33.\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 710/5000\n",
      "\n",
      "Epoch 00710: LearningRateScheduler reducing learning rate to 1.6162972464442238e-33.\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 711/5000\n",
      "\n",
      "Epoch 00711: LearningRateScheduler reducing learning rate to 1.462486227251231e-33.\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 712/5000\n",
      "\n",
      "Epoch 00712: LearningRateScheduler reducing learning rate to 1.3233122617791443e-33.\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 713/5000\n",
      "\n",
      "Epoch 00713: LearningRateScheduler reducing learning rate to 1.1973824502035735e-33.\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 714/5000\n",
      "\n",
      "Epoch 00714: LearningRateScheduler reducing learning rate to 1.0834364446437785e-33.\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 715/5000\n",
      "\n",
      "Epoch 00715: LearningRateScheduler reducing learning rate to 9.80333835177528e-34.\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 716/5000\n",
      "\n",
      "Epoch 00716: LearningRateScheduler reducing learning rate to 8.870427362353295e-34.\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 717/5000\n",
      "\n",
      "Epoch 00717: LearningRateScheduler reducing learning rate to 8.026294591427213e-34.\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 718/5000\n",
      "\n",
      "Epoch 00718: LearningRateScheduler reducing learning rate to 7.262491674503029e-34.\n",
      "11/11 [==============================] - 0s 17ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 719/5000\n",
      "\n",
      "Epoch 00719: LearningRateScheduler reducing learning rate to 6.57137421526501e-34.\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 720/5000\n",
      "\n",
      "Epoch 00720: LearningRateScheduler reducing learning rate to 5.946025277888421e-34.\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 721/5000\n",
      "\n",
      "Epoch 00721: LearningRateScheduler reducing learning rate to 5.380186160021139e-34.\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 722/5000\n",
      "\n",
      "Epoch 00722: LearningRateScheduler reducing learning rate to 4.8681937535862895e-34.\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 723/5000\n",
      "\n",
      "Epoch 00723: LearningRateScheduler reducing learning rate to 4.40492386649383e-34.\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 724/5000\n",
      "\n",
      "Epoch 00724: LearningRateScheduler reducing learning rate to 3.985739938003276e-34.\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 725/5000\n",
      "\n",
      "Epoch 00725: LearningRateScheduler reducing learning rate to 3.6064466344656586e-34.\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 726/5000\n",
      "\n",
      "Epoch 00726: LearningRateScheduler reducing learning rate to 3.263247861014401e-34.\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 727/5000\n",
      "\n",
      "Epoch 00727: LearningRateScheduler reducing learning rate to 2.9527087689716132e-34.\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 728/5000\n",
      "\n",
      "Epoch 00728: LearningRateScheduler reducing learning rate to 2.6717213787284268e-34.\n",
      "11/11 [==============================] - 0s 17ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 729/5000\n",
      "\n",
      "Epoch 00729: LearningRateScheduler reducing learning rate to 2.4174734740401173e-34.\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 730/5000\n",
      "\n",
      "Epoch 00730: LearningRateScheduler reducing learning rate to 2.1874204564208623e-34.\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 731/5000\n",
      "\n",
      "Epoch 00731: LearningRateScheduler reducing learning rate to 1.9792598779469043e-34.\n",
      "11/11 [==============================] - 0s 15ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 732/5000\n",
      "\n",
      "Epoch 00732: LearningRateScheduler reducing learning rate to 1.7909083975836303e-34.\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 733/5000\n",
      "\n",
      "Epoch 00733: LearningRateScheduler reducing learning rate to 1.6204809304084992e-34.\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 734/5000\n",
      "\n",
      "Epoch 00734: LearningRateScheduler reducing learning rate to 1.466271781047344e-34.\n",
      "11/11 [==============================] - 0s 20ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 735/5000\n",
      "\n",
      "Epoch 00735: LearningRateScheduler reducing learning rate to 1.3267375725018555e-34.\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 736/5000\n",
      "\n",
      "Epoch 00736: LearningRateScheduler reducing learning rate to 1.2004817995138824e-34.\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 737/5000\n",
      "\n",
      "Epoch 00737: LearningRateScheduler reducing learning rate to 1.0862408518712944e-34.\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 738/5000\n",
      "\n",
      "Epoch 00738: LearningRateScheduler reducing learning rate to 9.82871367772409e-35.\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 739/5000\n",
      "\n",
      "Epoch 00739: LearningRateScheduler reducing learning rate to 8.893387906766636e-35.\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 740/5000\n",
      "\n",
      "Epoch 00740: LearningRateScheduler reducing learning rate to 8.047070151150882e-35.\n",
      "11/11 [==============================] - 0s 15ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 741/5000\n",
      "\n",
      "Epoch 00741: LearningRateScheduler reducing learning rate to 7.281290178321644e-35.\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 742/5000\n",
      "\n",
      "Epoch 00742: LearningRateScheduler reducing learning rate to 6.588383804923091e-35.\n",
      "11/11 [==============================] - 0s 21ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 743/5000\n",
      "\n",
      "Epoch 00743: LearningRateScheduler reducing learning rate to 5.961416191076576e-35.\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 744/5000\n",
      "\n",
      "Epoch 00744: LearningRateScheduler reducing learning rate to 5.394112434171524e-35.\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 745/5000\n",
      "\n",
      "Epoch 00745: LearningRateScheduler reducing learning rate to 4.880794767531385e-35.\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 746/5000\n",
      "\n",
      "Epoch 00746: LearningRateScheduler reducing learning rate to 4.416325735416545e-35.\n",
      "11/11 [==============================] - 0s 15ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 747/5000\n",
      "\n",
      "Epoch 00747: LearningRateScheduler reducing learning rate to 3.9960567756400326e-35.\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 748/5000\n",
      "\n",
      "Epoch 00748: LearningRateScheduler reducing learning rate to 3.6157816951952495e-35.\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 749/5000\n",
      "\n",
      "Epoch 00749: LearningRateScheduler reducing learning rate to 3.2716945732621734e-35.\n",
      "11/11 [==============================] - 0s 16ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 750/5000\n",
      "\n",
      "Epoch 00750: LearningRateScheduler reducing learning rate to 2.96035167027278e-35.\n",
      "11/11 [==============================] - 0s 20ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 751/5000\n",
      "\n",
      "Epoch 00751: LearningRateScheduler reducing learning rate to 2.678636961808078e-35.\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 752/5000\n",
      "\n",
      "Epoch 00752: LearningRateScheduler reducing learning rate to 2.423730952378088e-35.\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 753/5000\n",
      "\n",
      "Epoch 00753: LearningRateScheduler reducing learning rate to 2.1930824569636388e-35.\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 754/5000\n",
      "\n",
      "Epoch 00754: LearningRateScheduler reducing learning rate to 1.9843830678989485e-35.\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 755/5000\n",
      "\n",
      "Epoch 00755: LearningRateScheduler reducing learning rate to 1.7955440515519456e-35.\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 756/5000\n",
      "\n",
      "Epoch 00756: LearningRateScheduler reducing learning rate to 1.6246754435760977e-35.\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 757/5000\n",
      "\n",
      "Epoch 00757: LearningRateScheduler reducing learning rate to 1.470067133511811e-35.\n",
      "11/11 [==============================] - 0s 16ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 758/5000\n",
      "\n",
      "Epoch 00758: LearningRateScheduler reducing learning rate to 1.330171749426359e-35.\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 759/5000\n",
      "\n",
      "Epoch 00759: LearningRateScheduler reducing learning rate to 1.2035891712953288e-35.\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 760/5000\n",
      "\n",
      "Epoch 00760: LearningRateScheduler reducing learning rate to 1.0890525181308963e-35.\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 761/5000\n",
      "\n",
      "Epoch 00761: LearningRateScheduler reducing learning rate to 9.854154686111258e-36.\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 762/5000\n",
      "\n",
      "Epoch 00762: LearningRateScheduler reducing learning rate to 8.916407883107787e-36.\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 763/5000\n",
      "\n",
      "Epoch 00763: LearningRateScheduler reducing learning rate to 8.067899487106771e-36.\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 764/5000\n",
      "\n",
      "Epoch 00764: LearningRateScheduler reducing learning rate to 7.300137340887376e-36.\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 765/5000\n",
      "\n",
      "Epoch 00765: LearningRateScheduler reducing learning rate to 6.605437422836372e-36.\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 766/5000\n",
      "\n",
      "Epoch 00766: LearningRateScheduler reducing learning rate to 5.976846942677399e-36.\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 767/5000\n",
      "\n",
      "Epoch 00767: LearningRateScheduler reducing learning rate to 5.408074755608291e-36.\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 768/5000\n",
      "\n",
      "Epoch 00768: LearningRateScheduler reducing learning rate to 4.893428398410087e-36.\n",
      "11/11 [==============================] - 0s 15ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 769/5000\n",
      "\n",
      "Epoch 00769: LearningRateScheduler reducing learning rate to 4.427757117361186e-36.\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 770/5000\n",
      "\n",
      "Epoch 00770: LearningRateScheduler reducing learning rate to 4.0064003177634614e-36.\n",
      "11/11 [==============================] - 0s 17ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 771/5000\n",
      "\n",
      "Epoch 00771: LearningRateScheduler reducing learning rate to 3.6251409191435594e-36.\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 772/5000\n",
      "\n",
      "Epoch 00772: LearningRateScheduler reducing learning rate to 3.280163149294336e-36.\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 773/5000\n",
      "\n",
      "Epoch 00773: LearningRateScheduler reducing learning rate to 2.9680143547442052e-36.\n",
      "11/11 [==============================] - 0s 24ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 774/5000\n",
      "\n",
      "Epoch 00774: LearningRateScheduler reducing learning rate to 2.6855704454403884e-36.\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 775/5000\n",
      "\n",
      "Epoch 00775: LearningRateScheduler reducing learning rate to 2.4300046278059768e-36.\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 776/5000\n",
      "\n",
      "Epoch 00776: LearningRateScheduler reducing learning rate to 2.1987591132394055e-36.\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 777/5000\n",
      "\n",
      "Epoch 00777: LearningRateScheduler reducing learning rate to 1.9895195189065626e-36.\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 778/5000\n",
      "\n",
      "Epoch 00778: LearningRateScheduler reducing learning rate to 1.8001917046195687e-36.\n",
      "11/11 [==============================] - 0s 15ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 779/5000\n",
      "\n",
      "Epoch 00779: LearningRateScheduler reducing learning rate to 1.6288808139777096e-36.\n",
      "11/11 [==============================] - 0s 20ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 780/5000\n",
      "\n",
      "Epoch 00780: LearningRateScheduler reducing learning rate to 1.4738723100079113e-36.\n",
      "11/11 [==============================] - 0s 18ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 781/5000\n",
      "\n",
      "Epoch 00781: LearningRateScheduler reducing learning rate to 1.3336148155022613e-36.\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 782/5000\n",
      "\n",
      "Epoch 00782: LearningRateScheduler reducing learning rate to 1.2067045863135585e-36.\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 783/5000\n",
      "\n",
      "Epoch 00783: LearningRateScheduler reducing learning rate to 1.0918714622121171e-36.\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 784/5000\n",
      "\n",
      "Epoch 00784: LearningRateScheduler reducing learning rate to 9.879661546951515e-37.\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 785/5000\n",
      "\n",
      "Epoch 00785: LearningRateScheduler reducing learning rate to 8.939487445212812e-37.\n",
      "11/11 [==============================] - 0s 15ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 786/5000\n",
      "\n",
      "Epoch 00786: LearningRateScheduler reducing learning rate to 8.088782738491285e-37.\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 787/5000\n",
      "\n",
      "Epoch 00787: LearningRateScheduler reducing learning rate to 7.31903328815023e-37.\n",
      "11/11 [==============================] - 0s 22ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 788/5000\n",
      "\n",
      "Epoch 00788: LearningRateScheduler reducing learning rate to 6.622535182969131e-37.\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 789/5000\n",
      "\n",
      "Epoch 00789: LearningRateScheduler reducing learning rate to 5.992317635810039e-37.\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 790/5000\n",
      "\n",
      "Epoch 00790: LearningRateScheduler reducing learning rate to 5.422073217637733e-37.\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 791/5000\n",
      "\n",
      "Epoch 00791: LearningRateScheduler reducing learning rate to 4.906094730649281e-37.\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 792/5000\n",
      "\n",
      "Epoch 00792: LearningRateScheduler reducing learning rate to 4.439218088720484e-37.\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 793/5000\n",
      "\n",
      "Epoch 00793: LearningRateScheduler reducing learning rate to 4.016770633496393e-37.\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 794/5000\n",
      "\n",
      "Epoch 00794: LearningRateScheduler reducing learning rate to 3.6345243688555106e-37.\n",
      "11/11 [==============================] - 0s 17ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 795/5000\n",
      "\n",
      "Epoch 00795: LearningRateScheduler reducing learning rate to 3.2886536457040146e-37.\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 796/5000\n",
      "\n",
      "Epoch 00796: LearningRateScheduler reducing learning rate to 2.9756968735933827e-37.\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 797/5000\n",
      "\n",
      "Epoch 00797: LearningRateScheduler reducing learning rate to 2.6925218759598907e-37.\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 798/5000\n",
      "\n",
      "Epoch 00798: LearningRateScheduler reducing learning rate to 2.4362945422488996e-37.\n",
      "11/11 [==============================] - 0s 17ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 799/5000\n",
      "\n",
      "Epoch 00799: LearningRateScheduler reducing learning rate to 2.2044504631835755e-37.\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 800/5000\n",
      "\n",
      "Epoch 00800: LearningRateScheduler reducing learning rate to 1.994669265295213e-37.\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 801/5000\n",
      "\n",
      "Epoch 00801: LearningRateScheduler reducing learning rate to 1.8048513878454152e-37.\n",
      "11/11 [==============================] - 0s 17ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 802/5000\n",
      "\n",
      "Epoch 00802: LearningRateScheduler reducing learning rate to 1.63309706971665e-37.\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 803/5000\n",
      "\n",
      "Epoch 00803: LearningRateScheduler reducing learning rate to 1.4776873359645133e-37.\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 804/5000\n",
      "\n",
      "Epoch 00804: LearningRateScheduler reducing learning rate to 1.3370667937385543e-37.\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 805/5000\n",
      "\n",
      "Epoch 00805: LearningRateScheduler reducing learning rate to 1.2098280653880193e-37.\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 806/5000\n",
      "\n",
      "Epoch 00806: LearningRateScheduler reducing learning rate to 1.0946977029531418e-37.\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 807/5000\n",
      "\n",
      "Epoch 00807: LearningRateScheduler reducing learning rate to 9.905234430700081e-38.\n",
      "11/11 [==============================] - 0s 17ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 808/5000\n",
      "\n",
      "Epoch 00808: LearningRateScheduler reducing learning rate to 8.9626267473156e-38.\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 809/5000\n",
      "\n",
      "Epoch 00809: LearningRateScheduler reducing learning rate to 8.10972004486101e-38.\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 810/5000\n",
      "\n",
      "Epoch 00810: LearningRateScheduler reducing learning rate to 7.337978146386544e-38.\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 811/5000\n",
      "\n",
      "Epoch 00811: LearningRateScheduler reducing learning rate to 6.639677199580735e-38.\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 812/5000\n",
      "\n",
      "Epoch 00812: LearningRateScheduler reducing learning rate to 6.007828373860811e-38.\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 813/5000\n",
      "\n",
      "Epoch 00813: LearningRateScheduler reducing learning rate to 5.436107913807425e-38.\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 814/5000\n",
      "\n",
      "Epoch 00814: LearningRateScheduler reducing learning rate to 4.918793848894315e-38.\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 815/5000\n",
      "\n",
      "Epoch 00815: LearningRateScheduler reducing learning rate to 4.450708726084717e-38.\n",
      "11/11 [==============================] - 0s 15ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 816/5000\n",
      "\n",
      "Epoch 00816: LearningRateScheduler reducing learning rate to 4.027167792140633e-38.\n",
      "11/11 [==============================] - 0s 23ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 817/5000\n",
      "\n",
      "Epoch 00817: LearningRateScheduler reducing learning rate to 3.6439321070380755e-38.\n",
      "11/11 [==============================] - 0s 15ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 818/5000\n",
      "\n",
      "Epoch 00818: LearningRateScheduler reducing learning rate to 3.2971661192306847e-38.\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 819/5000\n",
      "\n",
      "Epoch 00819: LearningRateScheduler reducing learning rate to 2.9833992781603124e-38.\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 820/5000\n",
      "\n",
      "Epoch 00820: LearningRateScheduler reducing learning rate to 2.699491299820938e-38.\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 821/5000\n",
      "\n",
      "Epoch 00821: LearningRateScheduler reducing learning rate to 2.442600737740528e-38.\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 822/5000\n",
      "\n",
      "Epoch 00822: LearningRateScheduler reducing learning rate to 2.2101565448298503e-38.\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 823/5000\n",
      "\n",
      "Epoch 00823: LearningRateScheduler reducing learning rate to 1.9998323414791309e-38.\n",
      "11/11 [==============================] - 0s 15ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 824/5000\n",
      "\n",
      "Epoch 00824: LearningRateScheduler reducing learning rate to 1.8095231323687687e-38.\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 825/5000\n",
      "\n",
      "Epoch 00825: LearningRateScheduler reducing learning rate to 1.637324238968908e-38.\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 826/5000\n",
      "\n",
      "Epoch 00826: LearningRateScheduler reducing learning rate to 1.4815122368763275e-38.\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 827/5000\n",
      "\n",
      "Epoch 00827: LearningRateScheduler reducing learning rate to 1.3405277072038437e-38.\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 828/5000\n",
      "\n",
      "Epoch 00828: LearningRateScheduler reducing learning rate to 1.2129596293919974e-38.\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 829/5000\n",
      "\n",
      "Epoch 00829: LearningRateScheduler reducing learning rate to 1.0975312592409e-38.\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 830/5000\n",
      "\n",
      "Epoch 00830: LearningRateScheduler reducing learning rate to 9.930873508252971e-39.\n",
      "11/11 [==============================] - 0s 25ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 831/5000\n",
      "\n",
      "Epoch 00831: LearningRateScheduler reducing learning rate to 8.985825944049381e-39.\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 832/5000\n",
      "\n",
      "Epoch 00832: LearningRateScheduler reducing learning rate to 8.130711546134111e-39.\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 833/5000\n",
      "\n",
      "Epoch 00833: LearningRateScheduler reducing learning rate to 7.356972042199196e-39.\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 834/5000\n",
      "\n",
      "Epoch 00834: LearningRateScheduler reducing learning rate to 6.656863587226205e-39.\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 835/5000\n",
      "\n",
      "Epoch 00835: LearningRateScheduler reducing learning rate to 6.023379260483388e-39.\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 836/5000\n",
      "\n",
      "Epoch 00836: LearningRateScheduler reducing learning rate to 5.450178937907168e-39.\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 837/5000\n",
      "\n",
      "Epoch 00837: LearningRateScheduler reducing learning rate to 4.931525838009848e-39.\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 838/5000\n",
      "\n",
      "Epoch 00838: LearningRateScheduler reducing learning rate to 4.462229106242478e-39.\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 839/5000\n",
      "\n",
      "Epoch 00839: LearningRateScheduler reducing learning rate to 4.037591863177317e-39.\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 840/5000\n",
      "\n",
      "Epoch 00840: LearningRateScheduler reducing learning rate to 3.653364196560384e-39.\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 841/5000\n",
      "\n",
      "Epoch 00841: LearningRateScheduler reducing learning rate to 3.305700626760734e-39.\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 842/5000\n",
      "\n",
      "Epoch 00842: LearningRateScheduler reducing learning rate to 2.9911216199180104e-39.\n",
      "11/11 [==============================] - 0s 18ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 843/5000\n",
      "\n",
      "Epoch 00843: LearningRateScheduler reducing learning rate to 2.706478763598165e-39.\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 844/5000\n",
      "\n",
      "Epoch 00844: LearningRateScheduler reducing learning rate to 2.4489232564232987e-39.\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 845/5000\n",
      "\n",
      "Epoch 00845: LearningRateScheduler reducing learning rate to 2.2158773963102848e-39.\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 846/5000\n",
      "\n",
      "Epoch 00846: LearningRateScheduler reducing learning rate to 2.005008781961654e-39.\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 847/5000\n",
      "\n",
      "Epoch 00847: LearningRateScheduler reducing learning rate to 1.8142069694095918e-39.\n",
      "11/11 [==============================] - 0s 25ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 848/5000\n",
      "\n",
      "Epoch 00848: LearningRateScheduler reducing learning rate to 1.6415623499834273e-39.\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 849/5000\n",
      "\n",
      "Epoch 00849: LearningRateScheduler reducing learning rate to 1.4853470383040342e-39.\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 850/5000\n",
      "\n",
      "Epoch 00850: LearningRateScheduler reducing learning rate to 1.3439975790263895e-39.\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 851/5000\n",
      "\n",
      "Epoch 00851: LearningRateScheduler reducing learning rate to 1.2160992992528257e-39.\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 852/5000\n",
      "\n",
      "Epoch 00852: LearningRateScheduler reducing learning rate to 1.1003721500112571e-39.\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 853/5000\n",
      "\n",
      "Epoch 00853: LearningRateScheduler reducing learning rate to 9.95657895094869e-40.\n",
      "11/11 [==============================] - 0s 16ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 854/5000\n",
      "\n",
      "Epoch 00854: LearningRateScheduler reducing learning rate to 9.009085190447519e-40.\n",
      "11/11 [==============================] - 0s 22ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 855/5000\n",
      "\n",
      "Epoch 00855: LearningRateScheduler reducing learning rate to 8.151757382590581e-40.\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 856/5000\n",
      "\n",
      "Epoch 00856: LearningRateScheduler reducing learning rate to 7.376015102518876e-40.\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 857/5000\n",
      "\n",
      "Epoch 00857: LearningRateScheduler reducing learning rate to 6.674094460757366e-40.\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 858/5000\n",
      "\n",
      "Epoch 00858: LearningRateScheduler reducing learning rate to 6.0389703995998295e-40.\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 859/5000\n",
      "\n",
      "Epoch 00859: LearningRateScheduler reducing learning rate to 5.4642863839694504e-40.\n",
      "11/11 [==============================] - 0s 30ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 860/5000\n",
      "\n",
      "Epoch 00860: LearningRateScheduler reducing learning rate to 4.944290783079995e-40.\n",
      "11/11 [==============================] - 0s 25ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 861/5000\n",
      "\n",
      "Epoch 00861: LearningRateScheduler reducing learning rate to 4.473779306181121e-40.\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 862/5000\n",
      "\n",
      "Epoch 00862: LearningRateScheduler reducing learning rate to 4.048042916267597e-40.\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 863/5000\n",
      "\n",
      "Epoch 00863: LearningRateScheduler reducing learning rate to 3.6628207004543497e-40.\n",
      "11/11 [==============================] - 0s 18ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 864/5000\n",
      "\n",
      "Epoch 00864: LearningRateScheduler reducing learning rate to 3.3142572253277506e-40.\n",
      "11/11 [==============================] - 0s 18ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 865/5000\n",
      "\n",
      "Epoch 00865: LearningRateScheduler reducing learning rate to 2.998863950472602e-40.\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 866/5000\n",
      "\n",
      "Epoch 00866: LearningRateScheduler reducing learning rate to 2.7134843139867623e-40.\n",
      "11/11 [==============================] - 0s 15ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 867/5000\n",
      "\n",
      "Epoch 00867: LearningRateScheduler reducing learning rate to 2.4552621405488382e-40.\n",
      "11/11 [==============================] - 0s 17ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 868/5000\n",
      "\n",
      "Epoch 00868: LearningRateScheduler reducing learning rate to 2.221613055855667e-40.\n",
      "11/11 [==============================] - 0s 20ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 869/5000\n",
      "\n",
      "Epoch 00869: LearningRateScheduler reducing learning rate to 2.0101986213354026e-40.\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 870/5000\n",
      "\n",
      "Epoch 00870: LearningRateScheduler reducing learning rate to 1.8189029302685815e-40.\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 871/5000\n",
      "\n",
      "Epoch 00871: LearningRateScheduler reducing learning rate to 1.6458114310822737e-40.\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 872/5000\n",
      "\n",
      "Epoch 00872: LearningRateScheduler reducing learning rate to 1.4891917658745394e-40.\n",
      "11/11 [==============================] - 0s 23ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 873/5000\n",
      "\n",
      "Epoch 00873: LearningRateScheduler reducing learning rate to 1.3474764323943372e-40.\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 874/5000\n",
      "\n",
      "Epoch 00874: LearningRateScheduler reducing learning rate to 1.2192470959519879e-40.\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 875/5000\n",
      "\n",
      "Epoch 00875: LearningRateScheduler reducing learning rate to 1.1032203942490448e-40.\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 876/5000\n",
      "\n",
      "Epoch 00876: LearningRateScheduler reducing learning rate to 9.982350930569248e-41.\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 877/5000\n",
      "\n",
      "Epoch 00877: LearningRateScheduler reducing learning rate to 9.032404641945059e-41.\n",
      "11/11 [==============================] - 0s 20ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 878/5000\n",
      "\n",
      "Epoch 00878: LearningRateScheduler reducing learning rate to 8.17285769487363e-41.\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 879/5000\n",
      "\n",
      "Epoch 00879: LearningRateScheduler reducing learning rate to 7.395107454604717e-41.\n",
      "11/11 [==============================] - 0s 16ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 880/5000\n",
      "\n",
      "Epoch 00880: LearningRateScheduler reducing learning rate to 6.691369935323047e-41.\n",
      "11/11 [==============================] - 0s 15ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 881/5000\n",
      "\n",
      "Epoch 00881: LearningRateScheduler reducing learning rate to 6.054601895401186e-41.\n",
      "11/11 [==============================] - 0s 15ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 882/5000\n",
      "\n",
      "Epoch 00882: LearningRateScheduler reducing learning rate to 5.478430346270389e-41.\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 883/5000\n",
      "\n",
      "Epoch 00883: LearningRateScheduler reducing learning rate to 4.957088769409175e-41.\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 884/5000\n",
      "\n",
      "Epoch 00884: LearningRateScheduler reducing learning rate to 4.485359403087212e-41.\n",
      "11/11 [==============================] - 0s 17ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 885/5000\n",
      "\n",
      "Epoch 00885: LearningRateScheduler reducing learning rate to 4.058521021252769e-41.\n",
      "11/11 [==============================] - 0s 33ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 886/5000\n",
      "\n",
      "Epoch 00886: LearningRateScheduler reducing learning rate to 3.6723016819150424e-41.\n",
      "11/11 [==============================] - 0s 25ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 887/5000\n",
      "\n",
      "Epoch 00887: LearningRateScheduler reducing learning rate to 3.32283597211309e-41.\n",
      "11/11 [==============================] - 0s 17ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 888/5000\n",
      "\n",
      "Epoch 00888: LearningRateScheduler reducing learning rate to 3.006626321563833e-41.\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 889/5000\n",
      "\n",
      "Epoch 00889: LearningRateScheduler reducing learning rate to 2.7205079978027504e-41.\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 890/5000\n",
      "\n",
      "Epoch 00890: LearningRateScheduler reducing learning rate to 2.4616174324780327e-41.\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 891/5000\n",
      "\n",
      "Epoch 00891: LearningRateScheduler reducing learning rate to 2.2273635617957438e-41.\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 892/5000\n",
      "\n",
      "Epoch 00892: LearningRateScheduler reducing learning rate to 2.0154018942826221e-41.\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 893/5000\n",
      "\n",
      "Epoch 00893: LearningRateScheduler reducing learning rate to 1.82361104632748e-41.\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 894/5000\n",
      "\n",
      "Epoch 00894: LearningRateScheduler reducing learning rate to 1.6500715106607978e-41.\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 895/5000\n",
      "\n",
      "Epoch 00895: LearningRateScheduler reducing learning rate to 1.4930464452810201e-41.\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 896/5000\n",
      "\n",
      "Epoch 00896: LearningRateScheduler reducing learning rate to 1.3509642905558532e-41.\n",
      "11/11 [==============================] - 0s 17ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 897/5000\n",
      "\n",
      "Epoch 00897: LearningRateScheduler reducing learning rate to 1.22240304052533e-41.\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 898/5000\n",
      "\n",
      "Epoch 00898: LearningRateScheduler reducing learning rate to 1.1060760109882523e-41.\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 899/5000\n",
      "\n",
      "Epoch 00899: LearningRateScheduler reducing learning rate to 1.0008189619341152e-41.\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 900/5000\n",
      "\n",
      "Epoch 00900: LearningRateScheduler reducing learning rate to 9.055784454378993e-42.\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 901/5000\n",
      "\n",
      "Epoch 00901: LearningRateScheduler reducing learning rate to 8.194012623990515e-42.\n",
      "11/11 [==============================] - 0s 15ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 902/5000\n",
      "\n",
      "Epoch 00902: LearningRateScheduler reducing learning rate to 7.414249226045572e-42.\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 903/5000\n",
      "\n",
      "Epoch 00903: LearningRateScheduler reducing learning rate to 6.708690126370227e-42.\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 904/5000\n",
      "\n",
      "Epoch 00904: LearningRateScheduler reducing learning rate to 6.070273852348119e-42.\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 905/5000\n",
      "\n",
      "Epoch 00905: LearningRateScheduler reducing learning rate to 5.4926109193299014e-42.\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 906/5000\n",
      "\n",
      "Epoch 00906: LearningRateScheduler reducing learning rate to 4.969919882522614e-42.\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 907/5000\n",
      "\n",
      "Epoch 00907: LearningRateScheduler reducing learning rate to 4.4969694743473034e-42.\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 908/5000\n",
      "\n",
      "Epoch 00908: LearningRateScheduler reducing learning rate to 4.0690262481549635e-42.\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 909/5000\n",
      "\n",
      "Epoch 00909: LearningRateScheduler reducing learning rate to 3.681807204301053e-42.\n",
      "11/11 [==============================] - 0s 32ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 910/5000\n",
      "\n",
      "Epoch 00910: LearningRateScheduler reducing learning rate to 3.331436924445979e-42.\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 911/5000\n",
      "\n",
      "Epoch 00911: LearningRateScheduler reducing learning rate to 3.014408785065375e-42.\n",
      "11/11 [==============================] - 0s 47ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 912/5000\n",
      "\n",
      "Epoch 00912: LearningRateScheduler reducing learning rate to 2.7275498619834443e-42.\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 913/5000\n",
      "\n",
      "Epoch 00913: LearningRateScheduler reducing learning rate to 2.4679891746814513e-42.\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 914/5000\n",
      "\n",
      "Epoch 00914: LearningRateScheduler reducing learning rate to 2.2331289525594442e-42.\n",
      "11/11 [==============================] - 0s 16ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 915/5000\n",
      "\n",
      "Epoch 00915: LearningRateScheduler reducing learning rate to 2.0206186355752457e-42.\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 916/5000\n",
      "\n",
      "Epoch 00916: LearningRateScheduler reducing learning rate to 1.8283313490492595e-42.\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 917/5000\n",
      "\n",
      "Epoch 00917: LearningRateScheduler reducing learning rate to 1.6543426171879206e-42.\n",
      "11/11 [==============================] - 0s 16ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 918/5000\n",
      "\n",
      "Epoch 00918: LearningRateScheduler reducing learning rate to 1.4969111022831783e-42.\n",
      "11/11 [==============================] - 0s 18ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 919/5000\n",
      "\n",
      "Epoch 00919: LearningRateScheduler reducing learning rate to 1.3544611768192618e-42.\n",
      "11/11 [==============================] - 0s 25ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 920/5000\n",
      "\n",
      "Epoch 00920: LearningRateScheduler reducing learning rate to 1.2255671540630953e-42.\n",
      "11/11 [==============================] - 0s 17ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 921/5000\n",
      "\n",
      "Epoch 00921: LearningRateScheduler reducing learning rate to 1.1089390193121365e-42.\n",
      "11/11 [==============================] - 0s 15ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 922/5000\n",
      "\n",
      "Epoch 00922: LearningRateScheduler reducing learning rate to 1.003409518993714e-42.\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 923/5000\n",
      "\n",
      "Epoch 00923: LearningRateScheduler reducing learning rate to 9.079224783989815e-43.\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 924/5000\n",
      "\n",
      "Epoch 00924: LearningRateScheduler reducing learning rate to 8.215222311313366e-43.\n",
      "11/11 [==============================] - 0s 16ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 925/5000\n",
      "\n",
      "Epoch 00925: LearningRateScheduler reducing learning rate to 7.433440544760237e-43.\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 926/5000\n",
      "\n",
      "Epoch 00926: LearningRateScheduler reducing learning rate to 6.7260551496447084e-43.\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 927/5000\n",
      "\n",
      "Epoch 00927: LearningRateScheduler reducing learning rate to 6.085986375171935e-43.\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 928/5000\n",
      "\n",
      "Epoch 00928: LearningRateScheduler reducing learning rate to 5.506828197912634e-43.\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 929/5000\n",
      "\n",
      "Epoch 00929: LearningRateScheduler reducing learning rate to 4.982784208166842e-43.\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 930/5000\n",
      "\n",
      "Epoch 00930: LearningRateScheduler reducing learning rate to 4.508609597548064e-43.\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 931/5000\n",
      "\n",
      "Epoch 00931: LearningRateScheduler reducing learning rate to 4.0795586671775606e-43.\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 932/5000\n",
      "\n",
      "Epoch 00932: LearningRateScheduler reducing learning rate to 3.691337331135133e-43.\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 933/5000\n",
      "\n",
      "Epoch 00933: LearningRateScheduler reducing learning rate to 3.340060139804082e-43.\n",
      "11/11 [==============================] - 0s 23ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 934/5000\n",
      "\n",
      "Epoch 00934: LearningRateScheduler reducing learning rate to 3.022211392985126e-43.\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 935/5000\n",
      "\n",
      "Epoch 00935: LearningRateScheduler reducing learning rate to 2.734609953587538e-43.\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 936/5000\n",
      "\n",
      "Epoch 00936: LearningRateScheduler reducing learning rate to 2.474377409739597e-43.\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 937/5000\n",
      "\n",
      "Epoch 00937: LearningRateScheduler reducing learning rate to 2.2389092666752635e-43.\n",
      "11/11 [==============================] - 0s 32ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 938/5000\n",
      "\n",
      "Epoch 00938: LearningRateScheduler reducing learning rate to 2.0258488800752408e-43.\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 939/5000\n",
      "\n",
      "Epoch 00939: LearningRateScheduler reducing learning rate to 1.8330638699783057e-43.\n",
      "11/11 [==============================] - 0s 15ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 940/5000\n",
      "\n",
      "Epoch 00940: LearningRateScheduler reducing learning rate to 1.6586247792061834e-43.\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 941/5000\n",
      "\n",
      "Epoch 00941: LearningRateScheduler reducing learning rate to 1.5007857627073948e-43.\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 942/5000\n",
      "\n",
      "Epoch 00942: LearningRateScheduler reducing learning rate to 1.3579671145532758e-43.\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 943/5000\n",
      "\n",
      "Epoch 00943: LearningRateScheduler reducing learning rate to 1.2287394577101352e-43.\n",
      "11/11 [==============================] - 0s 20ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 944/5000\n",
      "\n",
      "Epoch 00944: LearningRateScheduler reducing learning rate to 1.1118094383533344e-43.\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 945/5000\n",
      "\n",
      "Epoch 00945: LearningRateScheduler reducing learning rate to 1.0060067815476473e-43.\n",
      "11/11 [==============================] - 0s 18ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 946/5000\n",
      "\n",
      "Epoch 00946: LearningRateScheduler reducing learning rate to 9.10272578742244e-44.\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 947/5000\n",
      "\n",
      "Epoch 00947: LearningRateScheduler reducing learning rate to 8.236486898580598e-44.\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 948/5000\n",
      "\n",
      "Epoch 00948: LearningRateScheduler reducing learning rate to 7.452681538998718e-44.\n",
      "11/11 [==============================] - 0s 15ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 949/5000\n",
      "\n",
      "Epoch 00949: LearningRateScheduler reducing learning rate to 6.743465121191804e-44.\n",
      "11/11 [==============================] - 0s 23ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 950/5000\n",
      "\n",
      "Epoch 00950: LearningRateScheduler reducing learning rate to 6.101739568874776e-44.\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 951/5000\n",
      "\n",
      "Epoch 00951: LearningRateScheduler reducing learning rate to 5.521082277028533e-44.\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 952/5000\n",
      "\n",
      "Epoch 00952: LearningRateScheduler reducing learning rate to 4.995681832310551e-44.\n",
      "11/11 [==============================] - 0s 39ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 953/5000\n",
      "\n",
      "Epoch 00953: LearningRateScheduler reducing learning rate to 4.520279850477056e-44.\n",
      "11/11 [==============================] - 0s 15ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 954/5000\n",
      "\n",
      "Epoch 00954: LearningRateScheduler reducing learning rate to 4.090118348705598e-44.\n",
      "11/11 [==============================] - 0s 20ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 955/5000\n",
      "\n",
      "Epoch 00955: LearningRateScheduler reducing learning rate to 3.7008921261042974e-44.\n",
      "11/11 [==============================] - 0s 26ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 956/5000\n",
      "\n",
      "Epoch 00956: LearningRateScheduler reducing learning rate to 3.3487056758138443e-44.\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 957/5000\n",
      "\n",
      "Epoch 00957: LearningRateScheduler reducing learning rate to 3.030034197465736e-44.\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 958/5000\n",
      "\n",
      "Epoch 00958: LearningRateScheduler reducing learning rate to 2.7416883197955735e-44.\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 959/5000\n",
      "\n",
      "Epoch 00959: LearningRateScheduler reducing learning rate to 2.4807821803431535e-44.\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 960/5000\n",
      "\n",
      "Epoch 00960: LearningRateScheduler reducing learning rate to 2.24470454277133e-44.\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 961/5000\n",
      "\n",
      "Epoch 00961: LearningRateScheduler reducing learning rate to 2.031092662734811e-44.\n",
      "11/11 [==============================] - 0s 18ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 962/5000\n",
      "\n",
      "Epoch 00962: LearningRateScheduler reducing learning rate to 1.8378086407407328e-44.\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 963/5000\n",
      "\n",
      "Epoch 00963: LearningRateScheduler reducing learning rate to 1.6629180253320303e-44.\n",
      "11/11 [==============================] - 0s 18ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 964/5000\n",
      "\n",
      "Epoch 00964: LearningRateScheduler reducing learning rate to 1.5046704524468781e-44.\n",
      "11/11 [==============================] - 0s 23ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 965/5000\n",
      "\n",
      "Epoch 00965: LearningRateScheduler reducing learning rate to 1.36148212718704e-44.\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 966/5000\n",
      "\n",
      "Epoch 00966: LearningRateScheduler reducing learning rate to 1.231919972666034e-44.\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 967/5000\n",
      "\n",
      "Epoch 00967: LearningRateScheduler reducing learning rate to 1.1146872872940547e-44.\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 968/5000\n",
      "\n",
      "Epoch 00968: LearningRateScheduler reducing learning rate to 1.008610766952666e-44.\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 969/5000\n",
      "\n",
      "Epoch 00969: LearningRateScheduler reducing learning rate to 9.126287621727115e-45.\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 970/5000\n",
      "\n",
      "Epoch 00970: LearningRateScheduler reducing learning rate to 8.257806527897148e-45.\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 971/5000\n",
      "\n",
      "Epoch 00971: LearningRateScheduler reducing learning rate to 7.471972337342991e-45.\n",
      "11/11 [==============================] - 0s 21ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 972/5000\n",
      "\n",
      "Epoch 00972: LearningRateScheduler reducing learning rate to 6.760920157357488e-45.\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 973/5000\n",
      "\n",
      "Epoch 00973: LearningRateScheduler reducing learning rate to 6.117533538730657e-45.\n",
      "11/11 [==============================] - 0s 22ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 974/5000\n",
      "\n",
      "Epoch 00974: LearningRateScheduler reducing learning rate to 5.5353732519333876e-45.\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 975/5000\n",
      "\n",
      "Epoch 00975: LearningRateScheduler reducing learning rate to 5.008612841144748e-45.\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 976/5000\n",
      "\n",
      "Epoch 00976: LearningRateScheduler reducing learning rate to 4.531980311123192e-45.\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 977/5000\n",
      "\n",
      "Epoch 00977: LearningRateScheduler reducing learning rate to 4.1007053633064787e-45.\n",
      "11/11 [==============================] - 0s 18ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 978/5000\n",
      "\n",
      "Epoch 00978: LearningRateScheduler reducing learning rate to 3.7104716530604667e-45.\n",
      "11/11 [==============================] - 0s 21ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 979/5000\n",
      "\n",
      "Epoch 00979: LearningRateScheduler reducing learning rate to 3.357373590250823e-45.\n",
      "11/11 [==============================] - 0s 15ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 980/5000\n",
      "\n",
      "Epoch 00980: LearningRateScheduler reducing learning rate to 3.0378772507846913e-45.\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 981/5000\n",
      "\n",
      "Epoch 00981: LearningRateScheduler reducing learning rate to 2.7487850079102147e-45.\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 982/5000\n",
      "\n",
      "Epoch 00982: LearningRateScheduler reducing learning rate to 2.4872035292934122e-45.\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 983/5000\n",
      "\n",
      "Epoch 00983: LearningRateScheduler reducing learning rate to 2.25051481957579e-45.\n",
      "11/11 [==============================] - 0s 15ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 984/5000\n",
      "\n",
      "Epoch 00984: LearningRateScheduler reducing learning rate to 2.036350018596604e-45.\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 985/5000\n",
      "\n",
      "Epoch 00985: LearningRateScheduler reducing learning rate to 1.8425656930444397e-45.\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 986/5000\n",
      "\n",
      "Epoch 00986: LearningRateScheduler reducing learning rate to 1.667222384255979e-45.\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 987/5000\n",
      "\n",
      "Epoch 00987: LearningRateScheduler reducing learning rate to 1.5085651974619236e-45.\n",
      "11/11 [==============================] - 0s 15ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 988/5000\n",
      "\n",
      "Epoch 00988: LearningRateScheduler reducing learning rate to 1.3650062382103623e-45.\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 989/5000\n",
      "\n",
      "Epoch 00989: LearningRateScheduler reducing learning rate to 1.2351087201852317e-45.\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 990/5000\n",
      "\n",
      "Epoch 00990: LearningRateScheduler reducing learning rate to 1.1175725853661098e-45.\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 991/5000\n",
      "\n",
      "Epoch 00991: LearningRateScheduler reducing learning rate to 1.0112214926104486e-45.\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 992/5000\n",
      "\n",
      "Epoch 00992: LearningRateScheduler reducing learning rate to 9.149910444360996e-46.\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 993/5000\n",
      "\n",
      "Epoch 00993: LearningRateScheduler reducing learning rate to 8.279181341735911e-46.\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 994/5000\n",
      "\n",
      "Epoch 00994: LearningRateScheduler reducing learning rate to 7.491313068707749e-46.\n",
      "11/11 [==============================] - 0s 26ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 995/5000\n",
      "\n",
      "Epoch 00995: LearningRateScheduler reducing learning rate to 6.778420374788599e-46.\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 996/5000\n",
      "\n",
      "Epoch 00996: LearningRateScheduler reducing learning rate to 6.1333683902860926e-46.\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 997/5000\n",
      "\n",
      "Epoch 00997: LearningRateScheduler reducing learning rate to 5.54970121812979e-46.\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 998/5000\n",
      "\n",
      "Epoch 00998: LearningRateScheduler reducing learning rate to 5.021577321083608e-46.\n",
      "11/11 [==============================] - 0s 21ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 999/5000\n",
      "\n",
      "Epoch 00999: LearningRateScheduler reducing learning rate to 4.5437110576771834e-46.\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1000/5000\n",
      "\n",
      "Epoch 01000: LearningRateScheduler reducing learning rate to 4.111319781730085e-46.\n",
      "11/11 [==============================] - 0s 21ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1001/5000\n",
      "\n",
      "Epoch 01001: LearningRateScheduler reducing learning rate to 3.7200759760208366e-46.\n",
      "11/11 [==============================] - 0s 20ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1002/5000\n",
      "\n",
      "Epoch 01002: LearningRateScheduler reducing learning rate to 3.366063941040267e-46.\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1003/5000\n",
      "\n",
      "Epoch 01003: LearningRateScheduler reducing learning rate to 3.0457406053548386e-46.\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1004/5000\n",
      "\n",
      "Epoch 01004: LearningRateScheduler reducing learning rate to 2.7559000653565295e-46.\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1005/5000\n",
      "\n",
      "Epoch 01005: LearningRateScheduler reducing learning rate to 2.4936414995023486e-46.\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1006/5000\n",
      "\n",
      "Epoch 01006: LearningRateScheduler reducing learning rate to 2.256340135917036e-46.\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1007/5000\n",
      "\n",
      "Epoch 01007: LearningRateScheduler reducing learning rate to 2.04162098279406e-46.\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1008/5000\n",
      "\n",
      "Epoch 01008: LearningRateScheduler reducing learning rate to 1.847335058679426e-46.\n",
      "11/11 [==============================] - 0s 18ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1009/5000\n",
      "\n",
      "Epoch 01009: LearningRateScheduler reducing learning rate to 1.6715378847427853e-46.\n",
      "11/11 [==============================] - 0s 21ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1010/5000\n",
      "\n",
      "Epoch 01010: LearningRateScheduler reducing learning rate to 1.5124700237799598e-46.\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1011/5000\n",
      "\n",
      "Epoch 01011: LearningRateScheduler reducing learning rate to 1.368539471173853e-46.\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1012/5000\n",
      "\n",
      "Epoch 01012: LearningRateScheduler reducing learning rate to 1.2383057215772361e-46.\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1013/5000\n",
      "\n",
      "Epoch 01013: LearningRateScheduler reducing learning rate to 1.1204653518511086e-46.\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1014/5000\n",
      "\n",
      "Epoch 01014: LearningRateScheduler reducing learning rate to 1.0138389759677013e-46.\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1015/5000\n",
      "\n",
      "Epoch 01015: LearningRateScheduler reducing learning rate to 9.173594413188415e-47.\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1016/5000\n",
      "\n",
      "Epoch 01016: LearningRateScheduler reducing learning rate to 8.300611482938556e-47.\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1017/5000\n",
      "\n",
      "Epoch 01017: LearningRateScheduler reducing learning rate to 7.510703862341697e-47.\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1018/5000\n",
      "\n",
      "Epoch 01018: LearningRateScheduler reducing learning rate to 6.795965890434008e-47.\n",
      "11/11 [==============================] - 0s 16ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1019/5000\n",
      "\n",
      "Epoch 01019: LearningRateScheduler reducing learning rate to 6.149244229360705e-47.\n",
      "11/11 [==============================] - 0s 16ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1020/5000\n",
      "\n",
      "Epoch 01020: LearningRateScheduler reducing learning rate to 5.564066271367297e-47.\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1021/5000\n",
      "\n",
      "Epoch 01021: LearningRateScheduler reducing learning rate to 5.034575358764982e-47.\n",
      "11/11 [==============================] - 0s 21ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1022/5000\n",
      "\n",
      "Epoch 01022: LearningRateScheduler reducing learning rate to 4.555472168532333e-47.\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1023/5000\n",
      "\n",
      "Epoch 01023: LearningRateScheduler reducing learning rate to 4.1219616749094936e-47.\n",
      "11/11 [==============================] - 0s 17ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1024/5000\n",
      "\n",
      "Epoch 01024: LearningRateScheduler reducing learning rate to 3.7297051591682534e-47.\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1025/5000\n",
      "\n",
      "Epoch 01025: LearningRateScheduler reducing learning rate to 3.3747767862572193e-47.\n",
      "11/11 [==============================] - 0s 21ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1026/5000\n",
      "\n",
      "Epoch 01026: LearningRateScheduler reducing learning rate to 3.053624313724693e-47.\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1027/5000\n",
      "\n",
      "Epoch 01027: LearningRateScheduler reducing learning rate to 2.7630335396824565e-47.\n",
      "11/11 [==============================] - 0s 22ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1028/5000\n",
      "\n",
      "Epoch 01028: LearningRateScheduler reducing learning rate to 2.5000961339930464e-47.\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1029/5000\n",
      "\n",
      "Epoch 01029: LearningRateScheduler reducing learning rate to 2.262180530723933e-47.\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1030/5000\n",
      "\n",
      "Epoch 01030: LearningRateScheduler reducing learning rate to 2.0469055905514721e-47.\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1031/5000\n",
      "\n",
      "Epoch 01031: LearningRateScheduler reducing learning rate to 1.8521167695179755e-47.\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1032/5000\n",
      "\n",
      "Epoch 01032: LearningRateScheduler reducing learning rate to 1.6758645556317332e-47.\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1033/5000\n",
      "\n",
      "Epoch 01033: LearningRateScheduler reducing learning rate to 1.5163849574958067e-47.\n",
      "11/11 [==============================] - 0s 15ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1034/5000\n",
      "\n",
      "Epoch 01034: LearningRateScheduler reducing learning rate to 1.3720818496890623e-47.\n",
      "11/11 [==============================] - 0s 25ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1035/5000\n",
      "\n",
      "Epoch 01035: LearningRateScheduler reducing learning rate to 1.241510998206662e-47.\n",
      "11/11 [==============================] - 0s 40ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1036/5000\n",
      "\n",
      "Epoch 01036: LearningRateScheduler reducing learning rate to 1.123365606080569e-47.\n",
      "11/11 [==============================] - 0s 17ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1037/5000\n",
      "\n",
      "Epoch 01037: LearningRateScheduler reducing learning rate to 1.0164632345163344e-47.\n",
      "11/11 [==============================] - 0s 22ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1038/5000\n",
      "\n",
      "Epoch 01038: LearningRateScheduler reducing learning rate to 9.197339686482453e-48.\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1039/5000\n",
      "\n",
      "Epoch 01039: LearningRateScheduler reducing learning rate to 8.322097094716373e-48.\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1040/5000\n",
      "\n",
      "Epoch 01040: LearningRateScheduler reducing learning rate to 7.530144847827767e-48.\n",
      "11/11 [==============================] - 0s 15ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1041/5000\n",
      "\n",
      "Epoch 01041: LearningRateScheduler reducing learning rate to 6.813556821545298e-48.\n",
      "11/11 [==============================] - 0s 20ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1042/5000\n",
      "\n",
      "Epoch 01042: LearningRateScheduler reducing learning rate to 6.165161162048295e-48.\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1043/5000\n",
      "\n",
      "Epoch 01043: LearningRateScheduler reducing learning rate to 5.578468507643387e-48.\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1044/5000\n",
      "\n",
      "Epoch 01044: LearningRateScheduler reducing learning rate to 5.047607041050912e-48.\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1045/5000\n",
      "\n",
      "Epoch 01045: LearningRateScheduler reducing learning rate to 4.567263722284663e-48.\n",
      "11/11 [==============================] - 0s 27ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1046/5000\n",
      "\n",
      "Epoch 01046: LearningRateScheduler reducing learning rate to 4.132631113961384e-48.\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1047/5000\n",
      "\n",
      "Epoch 01047: LearningRateScheduler reducing learning rate to 3.739359266851858e-48.\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1048/5000\n",
      "\n",
      "Epoch 01048: LearningRateScheduler reducing learning rate to 3.383512184127093e-48.\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1049/5000\n",
      "\n",
      "Epoch 01049: LearningRateScheduler reducing learning rate to 3.061528428578743e-48.\n",
      "11/11 [==============================] - 0s 17ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1050/5000\n",
      "\n",
      "Epoch 01050: LearningRateScheduler reducing learning rate to 2.770185478558894e-48.\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1051/5000\n",
      "\n",
      "Epoch 01051: LearningRateScheduler reducing learning rate to 2.506567475899953e-48.\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1052/5000\n",
      "\n",
      "Epoch 01052: LearningRateScheduler reducing learning rate to 2.268036043026207e-48.\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1053/5000\n",
      "\n",
      "Epoch 01053: LearningRateScheduler reducing learning rate to 2.052203877184339e-48.\n",
      "11/11 [==============================] - 0s 35ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1054/5000\n",
      "\n",
      "Epoch 01054: LearningRateScheduler reducing learning rate to 1.8569108575148467e-48.\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1055/5000\n",
      "\n",
      "Epoch 01055: LearningRateScheduler reducing learning rate to 1.6802024258366833e-48.\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1056/5000\n",
      "\n",
      "Epoch 01056: LearningRateScheduler reducing learning rate to 1.520310024771829e-48.\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1057/5000\n",
      "\n",
      "Epoch 01057: LearningRateScheduler reducing learning rate to 1.3756333974287157e-48.\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1058/5000\n",
      "\n",
      "Epoch 01058: LearningRateScheduler reducing learning rate to 1.2447245714934413e-48.\n",
      "11/11 [==============================] - 0s 20ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1059/5000\n",
      "\n",
      "Epoch 01059: LearningRateScheduler reducing learning rate to 1.1262733674360321e-48.\n",
      "11/11 [==============================] - 0s 16ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1060/5000\n",
      "\n",
      "Epoch 01060: LearningRateScheduler reducing learning rate to 1.0190942857934905e-48.\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1061/5000\n",
      "\n",
      "Epoch 01061: LearningRateScheduler reducing learning rate to 9.221146422925875e-49.\n",
      "11/11 [==============================] - 0s 18ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1062/5000\n",
      "\n",
      "Epoch 01062: LearningRateScheduler reducing learning rate to 8.343638320651702e-49.\n",
      "11/11 [==============================] - 0s 15ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1063/5000\n",
      "\n",
      "Epoch 01063: LearningRateScheduler reducing learning rate to 7.549636155084418e-49.\n",
      "11/11 [==============================] - 0s 25ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1064/5000\n",
      "\n",
      "Epoch 01064: LearningRateScheduler reducing learning rate to 6.831193285677457e-49.\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1065/5000\n",
      "\n",
      "Epoch 01065: LearningRateScheduler reducing learning rate to 6.181119294717008e-49.\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1066/5000\n",
      "\n",
      "Epoch 01066: LearningRateScheduler reducing learning rate to 5.592908023204021e-49.\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1067/5000\n",
      "\n",
      "Epoch 01067: LearningRateScheduler reducing learning rate to 5.060672455028485e-49.\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1068/5000\n",
      "\n",
      "Epoch 01068: LearningRateScheduler reducing learning rate to 4.5790857977337016e-49.\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1069/5000\n",
      "\n",
      "Epoch 01069: LearningRateScheduler reducing learning rate to 4.1433281701864594e-49.\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1070/5000\n",
      "\n",
      "Epoch 01070: LearningRateScheduler reducing learning rate to 3.7490383635871944e-49.\n",
      "11/11 [==============================] - 0s 15ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1071/5000\n",
      "\n",
      "Epoch 01071: LearningRateScheduler reducing learning rate to 3.3922701930260154e-49.\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1072/5000\n",
      "\n",
      "Epoch 01072: LearningRateScheduler reducing learning rate to 3.0694530027379795e-49.\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1073/5000\n",
      "\n",
      "Epoch 01073: LearningRateScheduler reducing learning rate to 2.777355929780173e-49.\n",
      "11/11 [==============================] - 0s 20ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1074/5000\n",
      "\n",
      "Epoch 01074: LearningRateScheduler reducing learning rate to 2.513055568469132e-49.\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1075/5000\n",
      "\n",
      "Epoch 01075: LearningRateScheduler reducing learning rate to 2.2739067119545126e-49.\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1076/5000\n",
      "\n",
      "Epoch 01076: LearningRateScheduler reducing learning rate to 2.0575158780995712e-49.\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1077/5000\n",
      "\n",
      "Epoch 01077: LearningRateScheduler reducing learning rate to 1.8617173547075906e-49.\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1078/5000\n",
      "\n",
      "Epoch 01078: LearningRateScheduler reducing learning rate to 1.6845515243463625e-49.\n",
      "11/11 [==============================] - 0s 20ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1079/5000\n",
      "\n",
      "Epoch 01079: LearningRateScheduler reducing learning rate to 1.5242452518380895e-49.\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1080/5000\n",
      "\n",
      "Epoch 01080: LearningRateScheduler reducing learning rate to 1.3791941381267557e-49.\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1081/5000\n",
      "\n",
      "Epoch 01081: LearningRateScheduler reducing learning rate to 1.2479464629129514e-49.\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1082/5000\n",
      "\n",
      "Epoch 01082: LearningRateScheduler reducing learning rate to 1.1291886553492535e-49.\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1083/5000\n",
      "\n",
      "Epoch 01083: LearningRateScheduler reducing learning rate to 1.0217321473817214e-49.\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1084/5000\n",
      "\n",
      "Epoch 01084: LearningRateScheduler reducing learning rate to 9.245014781612054e-50.\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1085/5000\n",
      "\n",
      "Epoch 01085: LearningRateScheduler reducing learning rate to 8.365235304698179e-50.\n",
      "11/11 [==============================] - 0s 16ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1086/5000\n",
      "\n",
      "Epoch 01086: LearningRateScheduler reducing learning rate to 7.569177914366397e-50.\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1087/5000\n",
      "\n",
      "Epoch 01087: LearningRateScheduler reducing learning rate to 6.848875400690042e-50.\n",
      "11/11 [==============================] - 0s 18ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1088/5000\n",
      "\n",
      "Epoch 01088: LearningRateScheduler reducing learning rate to 6.197118734010411e-50.\n",
      "11/11 [==============================] - 0s 16ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1089/5000\n",
      "\n",
      "Epoch 01089: LearningRateScheduler reducing learning rate to 5.607384914544207e-50.\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1090/5000\n",
      "\n",
      "Epoch 01090: LearningRateScheduler reducing learning rate to 5.073771688009999e-50.\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1091/5000\n",
      "\n",
      "Epoch 01091: LearningRateScheduler reducing learning rate to 4.590938473882946e-50.\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1092/5000\n",
      "\n",
      "Epoch 01092: LearningRateScheduler reducing learning rate to 4.154052915070158e-50.\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1093/5000\n",
      "\n",
      "Epoch 01093: LearningRateScheduler reducing learning rate to 3.7587425140568543e-50.\n",
      "11/11 [==============================] - 0s 16ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1094/5000\n",
      "\n",
      "Epoch 01094: LearningRateScheduler reducing learning rate to 3.4010508714811664e-50.\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1095/5000\n",
      "\n",
      "Epoch 01095: LearningRateScheduler reducing learning rate to 3.0773980891599865e-50.\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1096/5000\n",
      "\n",
      "Epoch 01096: LearningRateScheduler reducing learning rate to 2.7845449412643337e-50.\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1097/5000\n",
      "\n",
      "Epoch 01097: LearningRateScheduler reducing learning rate to 2.519560455058691e-50.\n",
      "11/11 [==============================] - 0s 15ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1098/5000\n",
      "\n",
      "Epoch 01098: LearningRateScheduler reducing learning rate to 2.279792576740826e-50.\n",
      "11/11 [==============================] - 0s 22ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1099/5000\n",
      "\n",
      "Epoch 01099: LearningRateScheduler reducing learning rate to 2.062841628795699e-50.\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1100/5000\n",
      "\n",
      "Epoch 01100: LearningRateScheduler reducing learning rate to 1.8665362932166042e-50.\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1101/5000\n",
      "\n",
      "Epoch 01101: LearningRateScheduler reducing learning rate to 1.6889118802245324e-50.\n",
      "11/11 [==============================] - 0s 30ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1102/5000\n",
      "\n",
      "Epoch 01102: LearningRateScheduler reducing learning rate to 1.5281906649926106e-50.\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1103/5000\n",
      "\n",
      "Epoch 01103: LearningRateScheduler reducing learning rate to 1.3827640955785778e-50.\n",
      "11/11 [==============================] - 0s 20ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1104/5000\n",
      "\n",
      "Epoch 01104: LearningRateScheduler reducing learning rate to 1.2511766939961384e-50.\n",
      "11/11 [==============================] - ETA: 0s - loss: 110.0405 - mae: 7.9526 - mse: 110.040 - 0s 17ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1105/5000\n",
      "\n",
      "Epoch 01105: LearningRateScheduler reducing learning rate to 1.1321114893022402e-50.\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1106/5000\n",
      "\n",
      "Epoch 01106: LearningRateScheduler reducing learning rate to 1.0243768369090898e-50.\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1107/5000\n",
      "\n",
      "Epoch 01107: LearningRateScheduler reducing learning rate to 9.268944922046561e-51.\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1108/5000\n",
      "\n",
      "Epoch 01108: LearningRateScheduler reducing learning rate to 8.386888191182176e-51.\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1109/5000\n",
      "\n",
      "Epoch 01109: LearningRateScheduler reducing learning rate to 7.588770256265496e-51.\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1110/5000\n",
      "\n",
      "Epoch 01110: LearningRateScheduler reducing learning rate to 6.866603284747397e-51.\n",
      "11/11 [==============================] - 0s 27ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1111/5000\n",
      "\n",
      "Epoch 01111: LearningRateScheduler reducing learning rate to 6.21315958684811e-51.\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1112/5000\n",
      "\n",
      "Epoch 01112: LearningRateScheduler reducing learning rate to 5.621899278408964e-51.\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1113/5000\n",
      "\n",
      "Epoch 01113: LearningRateScheduler reducing learning rate to 5.0869048275338206e-51.\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1114/5000\n",
      "\n",
      "Epoch 01114: LearningRateScheduler reducing learning rate to 4.602821829940321e-51.\n",
      "11/11 [==============================] - ETA: 0s - loss: 67.5158 - mae: 6.4446 - mse: 67.515 - 0s 17ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1115/5000\n",
      "\n",
      "Epoch 01115: LearningRateScheduler reducing learning rate to 4.1648054202827744e-51.\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1116/5000\n",
      "\n",
      "Epoch 01116: LearningRateScheduler reducing learning rate to 3.7684717831108566e-51.\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1117/5000\n",
      "\n",
      "Epoch 01117: LearningRateScheduler reducing learning rate to 3.4098542781713667e-51.\n",
      "11/11 [==============================] - 0s 15ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1118/5000\n",
      "\n",
      "Epoch 01118: LearningRateScheduler reducing learning rate to 3.0853637409394677e-51.\n",
      "11/11 [==============================] - 0s 17ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1119/5000\n",
      "\n",
      "Epoch 01119: LearningRateScheduler reducing learning rate to 2.791752561053414e-51.\n",
      "11/11 [==============================] - 0s 30ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1120/5000\n",
      "\n",
      "Epoch 01120: LearningRateScheduler reducing learning rate to 2.5260821791388627e-51.\n",
      "11/11 [==============================] - 0s 23ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1121/5000\n",
      "\n",
      "Epoch 01121: LearningRateScheduler reducing learning rate to 2.2856936767186715e-51.\n",
      "11/11 [==============================] - 0s 30ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1122/5000\n",
      "\n",
      "Epoch 01122: LearningRateScheduler reducing learning rate to 2.0681811648632248e-51.\n",
      "11/11 [==============================] - 0s 30ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1123/5000\n",
      "\n",
      "Epoch 01123: LearningRateScheduler reducing learning rate to 1.871367705245454e-51.\n",
      "11/11 [==============================] - 0s 22ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1124/5000\n",
      "\n",
      "Epoch 01124: LearningRateScheduler reducing learning rate to 1.693283522610161e-51.\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1125/5000\n",
      "\n",
      "Epoch 01125: LearningRateScheduler reducing learning rate to 1.532146290601421e-51.\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1126/5000\n",
      "\n",
      "Epoch 01126: LearningRateScheduler reducing learning rate to 1.3863432936411707e-51.\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1127/5000\n",
      "\n",
      "Epoch 01127: LearningRateScheduler reducing learning rate to 1.2544152863297343e-51.\n",
      "11/11 [==============================] - 0s 33ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1128/5000\n",
      "\n",
      "Epoch 01128: LearningRateScheduler reducing learning rate to 1.1350418888274421e-51.\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1129/5000\n",
      "\n",
      "Epoch 01129: LearningRateScheduler reducing learning rate to 1.0270283720492727e-51.\n",
      "11/11 [==============================] - 0s 15ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1130/5000\n",
      "\n",
      "Epoch 01130: LearningRateScheduler reducing learning rate to 9.29293700414744e-52.\n",
      "11/11 [==============================] - 0s 18ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1131/5000\n",
      "\n",
      "Epoch 01131: LearningRateScheduler reducing learning rate to 8.408597124803643e-52.\n",
      "11/11 [==============================] - 0s 18ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1132/5000\n",
      "\n",
      "Epoch 01132: LearningRateScheduler reducing learning rate to 7.608413311711857e-52.\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1133/5000\n",
      "\n",
      "Epoch 01133: LearningRateScheduler reducing learning rate to 6.884377056319819e-52.\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1134/5000\n",
      "\n",
      "Epoch 01134: LearningRateScheduler reducing learning rate to 6.2292419604263725e-52.\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1135/5000\n",
      "\n",
      "Epoch 01135: LearningRateScheduler reducing learning rate to 5.63645121179349e-52.\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1136/5000\n",
      "\n",
      "Epoch 01136: LearningRateScheduler reducing learning rate to 5.100071961364906e-52.\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1137/5000\n",
      "\n",
      "Epoch 01137: LearningRateScheduler reducing learning rate to 4.614735945318974e-52.\n",
      "11/11 [==============================] - 0s 15ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1138/5000\n",
      "\n",
      "Epoch 01138: LearningRateScheduler reducing learning rate to 4.175585757680178e-52.\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1139/5000\n",
      "\n",
      "Epoch 01139: LearningRateScheduler reducing learning rate to 3.778226235767026e-52.\n",
      "11/11 [==============================] - 0s 15ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1140/5000\n",
      "\n",
      "Epoch 01140: LearningRateScheduler reducing learning rate to 3.418680471927177e-52.\n",
      "11/11 [==============================] - 0s 17ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1141/5000\n",
      "\n",
      "Epoch 01141: LearningRateScheduler reducing learning rate to 3.0933500113085613e-52.\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1142/5000\n",
      "\n",
      "Epoch 01142: LearningRateScheduler reducing learning rate to 2.7989788373139207e-52.\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1143/5000\n",
      "\n",
      "Epoch 01143: LearningRateScheduler reducing learning rate to 2.5326207842924345e-52.\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1144/5000\n",
      "\n",
      "Epoch 01144: LearningRateScheduler reducing learning rate to 2.291610051323354e-52.\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1145/5000\n",
      "\n",
      "Epoch 01145: LearningRateScheduler reducing learning rate to 2.0735345219846882e-52.\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1146/5000\n",
      "\n",
      "Epoch 01146: LearningRateScheduler reducing learning rate to 1.8762116230810635e-52.\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1147/5000\n",
      "\n",
      "Epoch 01147: LearningRateScheduler reducing learning rate to 1.697666480717712e-52.\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1148/5000\n",
      "\n",
      "Epoch 01148: LearningRateScheduler reducing learning rate to 1.5361121550988173e-52.\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1149/5000\n",
      "\n",
      "Epoch 01149: LearningRateScheduler reducing learning rate to 1.3899317562332554e-52.\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1150/5000\n",
      "\n",
      "Epoch 01150: LearningRateScheduler reducing learning rate to 1.2576622615562929e-52.\n",
      "11/11 [==============================] - 0s 16ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1151/5000\n",
      "\n",
      "Epoch 01151: LearningRateScheduler reducing learning rate to 1.137979873507868e-52.\n",
      "11/11 [==============================] - 0s 17ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1152/5000\n",
      "\n",
      "Epoch 01152: LearningRateScheduler reducing learning rate to 1.0296867705217385e-52.\n",
      "11/11 [==============================] - 0s 15ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1153/5000\n",
      "\n",
      "Epoch 01153: LearningRateScheduler reducing learning rate to 9.316991188246808e-53.\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1154/5000\n",
      "\n",
      "Epoch 01154: LearningRateScheduler reducing learning rate to 8.430362250636957e-53.\n",
      "11/11 [==============================] - 0s 21ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1155/5000\n",
      "\n",
      "Epoch 01155: LearningRateScheduler reducing learning rate to 7.628107211974208e-53.\n",
      "11/11 [==============================] - 0s 22ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1156/5000\n",
      "\n",
      "Epoch 01156: LearningRateScheduler reducing learning rate to 6.902196834184264e-53.\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1157/5000\n",
      "\n",
      "Epoch 01157: LearningRateScheduler reducing learning rate to 6.24536596221921e-53.\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1158/5000\n",
      "\n",
      "Epoch 01158: LearningRateScheduler reducing learning rate to 5.651040811944128e-53.\n",
      "11/11 [==============================] - 0s 18ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1159/5000\n",
      "\n",
      "Epoch 01159: LearningRateScheduler reducing learning rate to 5.113273177495315e-53.\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1160/5000\n",
      "\n",
      "Epoch 01160: LearningRateScheduler reducing learning rate to 4.626680899637413e-53.\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1161/5000\n",
      "\n",
      "Epoch 01161: LearningRateScheduler reducing learning rate to 4.186393999304231e-53.\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1162/5000\n",
      "\n",
      "Epoch 01162: LearningRateScheduler reducing learning rate to 3.788005937211643e-53.\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1163/5000\n",
      "\n",
      "Epoch 01163: LearningRateScheduler reducing learning rate to 3.4275295117314884e-53.\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1164/5000\n",
      "\n",
      "Epoch 01164: LearningRateScheduler reducing learning rate to 3.101356953637146e-53.\n",
      "11/11 [==============================] - 0s 20ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1165/5000\n",
      "\n",
      "Epoch 01165: LearningRateScheduler reducing learning rate to 2.8062238183369204e-53.\n",
      "11/11 [==============================] - 0s 15ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1166/5000\n",
      "\n",
      "Epoch 01166: LearningRateScheduler reducing learning rate to 2.5391763142150057e-53.\n",
      "11/11 [==============================] - 0s 25ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1167/5000\n",
      "\n",
      "Epoch 01167: LearningRateScheduler reducing learning rate to 2.29754174009235e-53.\n",
      "11/11 [==============================] - 0s 15ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1168/5000\n",
      "\n",
      "Epoch 01168: LearningRateScheduler reducing learning rate to 2.0789017359350199e-53.\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1169/5000\n",
      "\n",
      "Epoch 01169: LearningRateScheduler reducing learning rate to 1.8810680790939016e-53.\n",
      "11/11 [==============================] - 0s 21ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1170/5000\n",
      "\n",
      "Epoch 01170: LearningRateScheduler reducing learning rate to 1.7020607838371977e-53.\n",
      "11/11 [==============================] - 0s 18ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1171/5000\n",
      "\n",
      "Epoch 01171: LearningRateScheduler reducing learning rate to 1.5400882849875203e-53.\n",
      "11/11 [==============================] - 0s 17ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1172/5000\n",
      "\n",
      "Epoch 01172: LearningRateScheduler reducing learning rate to 1.3935295073355249e-53.\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1173/5000\n",
      "\n",
      "Epoch 01173: LearningRateScheduler reducing learning rate to 1.2609176413744065e-53.\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1174/5000\n",
      "\n",
      "Epoch 01174: LearningRateScheduler reducing learning rate to 1.1409254629772002e-53.\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1175/5000\n",
      "\n",
      "Epoch 01175: LearningRateScheduler reducing learning rate to 1.0323520500917774e-53.\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1176/5000\n",
      "\n",
      "Epoch 01176: LearningRateScheduler reducing learning rate to 9.341107635091788e-54.\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1177/5000\n",
      "\n",
      "Epoch 01177: LearningRateScheduler reducing learning rate to 8.452183714132371e-54.\n",
      "11/11 [==============================] - 0s 15ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1178/5000\n",
      "\n",
      "Epoch 01178: LearningRateScheduler reducing learning rate to 7.647852088661163e-54.\n",
      "11/11 [==============================] - 0s 15ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1179/5000\n",
      "\n",
      "Epoch 01179: LearningRateScheduler reducing learning rate to 6.920062737425029e-54.\n",
      "11/11 [==============================] - 0s 25ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1180/5000\n",
      "\n",
      "Epoch 01180: LearningRateScheduler reducing learning rate to 6.261531699978553e-54.\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1181/5000\n",
      "\n",
      "Epoch 01181: LearningRateScheduler reducing learning rate to 5.665668176358939e-54.\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1182/5000\n",
      "\n",
      "Epoch 01182: LearningRateScheduler reducing learning rate to 5.126508564145083e-54.\n",
      "11/11 [==============================] - 0s 17ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1183/5000\n",
      "\n",
      "Epoch 01183: LearningRateScheduler reducing learning rate to 4.6386567727202975e-54.\n",
      "11/11 [==============================] - 0s 36ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1184/5000\n",
      "\n",
      "Epoch 01184: LearningRateScheduler reducing learning rate to 4.197230217383215e-54.\n",
      "11/11 [==============================] - 0s 18ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1185/5000\n",
      "\n",
      "Epoch 01185: LearningRateScheduler reducing learning rate to 3.79781095279956e-54.\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1186/5000\n",
      "\n",
      "Epoch 01186: LearningRateScheduler reducing learning rate to 3.4364014567198604e-54.\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1187/5000\n",
      "\n",
      "Epoch 01187: LearningRateScheduler reducing learning rate to 3.109384621433382e-54.\n",
      "11/11 [==============================] - 0s 18ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1188/5000\n",
      "\n",
      "Epoch 01188: LearningRateScheduler reducing learning rate to 2.813487552538517e-54.\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1189/5000\n",
      "\n",
      "Epoch 01189: LearningRateScheduler reducing learning rate to 2.5457488127152413e-54.\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1190/5000\n",
      "\n",
      "Epoch 01190: LearningRateScheduler reducing learning rate to 2.3034887826653815e-54.\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1191/5000\n",
      "\n",
      "Epoch 01191: LearningRateScheduler reducing learning rate to 2.0842828425817513e-54.\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1192/5000\n",
      "\n",
      "Epoch 01192: LearningRateScheduler reducing learning rate to 1.8859371057383062e-54.\n",
      "11/11 [==============================] - 0s 20ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1193/5000\n",
      "\n",
      "Epoch 01193: LearningRateScheduler reducing learning rate to 1.7064664613344692e-54.\n",
      "11/11 [==============================] - 0s 33ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1194/5000\n",
      "\n",
      "Epoch 01194: LearningRateScheduler reducing learning rate to 1.5440747068388286e-54.\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1195/5000\n",
      "\n",
      "Epoch 01195: LearningRateScheduler reducing learning rate to 1.3971365709906848e-54.\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1196/5000\n",
      "\n",
      "Epoch 01196: LearningRateScheduler reducing learning rate to 1.2641814475388326e-54.\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1197/5000\n",
      "\n",
      "Epoch 01197: LearningRateScheduler reducing learning rate to 1.1438786769199893e-54.\n",
      "11/11 [==============================] - 0s 23ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1198/5000\n",
      "\n",
      "Epoch 01198: LearningRateScheduler reducing learning rate to 1.0350242285706787e-54.\n",
      "11/11 [==============================] - 0s 18ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1199/5000\n",
      "\n",
      "Epoch 01199: LearningRateScheduler reducing learning rate to 9.365286505845457e-55.\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1200/5000\n",
      "\n",
      "Epoch 01200: LearningRateScheduler reducing learning rate to 8.474061661116265e-55.\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1201/5000\n",
      "\n",
      "Epoch 01201: LearningRateScheduler reducing learning rate to 7.667648073722e-55.\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1202/5000\n",
      "\n",
      "Epoch 01202: LearningRateScheduler reducing learning rate to 6.937974885434953e-55.\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1203/5000\n",
      "\n",
      "Epoch 01203: LearningRateScheduler reducing learning rate to 6.277739281735332e-55.\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1204/5000\n",
      "\n",
      "Epoch 01204: LearningRateScheduler reducing learning rate to 5.6803334027882686e-55.\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1205/5000\n",
      "\n",
      "Epoch 01205: LearningRateScheduler reducing learning rate to 5.139778209762383e-55.\n",
      "11/11 [==============================] - 0s 18ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1206/5000\n",
      "\n",
      "Epoch 01206: LearningRateScheduler reducing learning rate to 4.650663644598907e-55.\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1207/5000\n",
      "\n",
      "Epoch 01207: LearningRateScheduler reducing learning rate to 4.2080944843325444e-55.\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1208/5000\n",
      "\n",
      "Epoch 01208: LearningRateScheduler reducing learning rate to 3.8076413480548444e-55.\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1209/5000\n",
      "\n",
      "Epoch 01209: LearningRateScheduler reducing learning rate to 3.445296366180877e-55.\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1210/5000\n",
      "\n",
      "Epoch 01210: LearningRateScheduler reducing learning rate to 3.117433068343796e-55.\n",
      "11/11 [==============================] - 0s 15ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1211/5000\n",
      "\n",
      "Epoch 01211: LearningRateScheduler reducing learning rate to 2.8207700884601354e-55.\n",
      "11/11 [==============================] - 0s 18ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1212/5000\n",
      "\n",
      "Epoch 01212: LearningRateScheduler reducing learning rate to 2.5523383237153126e-55.\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1213/5000\n",
      "\n",
      "Epoch 01213: LearningRateScheduler reducing learning rate to 2.3094512187848057e-55.\n",
      "11/11 [==============================] - 0s 15ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1214/5000\n",
      "\n",
      "Epoch 01214: LearningRateScheduler reducing learning rate to 2.0896778778852258e-55.\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1215/5000\n",
      "\n",
      "Epoch 01215: LearningRateScheduler reducing learning rate to 1.8908187355525417e-55.\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1216/5000\n",
      "\n",
      "Epoch 01216: LearningRateScheduler reducing learning rate to 1.710883542651389e-55.\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1217/5000\n",
      "\n",
      "Epoch 01217: LearningRateScheduler reducing learning rate to 1.5480714472928854e-55.\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1218/5000\n",
      "\n",
      "Epoch 01218: LearningRateScheduler reducing learning rate to 1.4007529713036936e-55.\n",
      "11/11 [==============================] - 0s 26ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1219/5000\n",
      "\n",
      "Epoch 01219: LearningRateScheduler reducing learning rate to 1.267453701860622e-55.\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1220/5000\n",
      "\n",
      "Epoch 01220: LearningRateScheduler reducing learning rate to 1.1468395350716903e-55.\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1221/5000\n",
      "\n",
      "Epoch 01221: LearningRateScheduler reducing learning rate to 1.0377033238158345e-55.\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1222/5000\n",
      "\n",
      "Epoch 01222: LearningRateScheduler reducing learning rate to 9.38952796208845e-56.\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1223/5000\n",
      "\n",
      "Epoch 01223: LearningRateScheduler reducing learning rate to 8.495996237792607e-56.\n",
      "11/11 [==============================] - 0s 15ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1224/5000\n",
      "\n",
      "Epoch 01224: LearningRateScheduler reducing learning rate to 7.687495299447423e-56.\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1225/5000\n",
      "\n",
      "Epoch 01225: LearningRateScheduler reducing learning rate to 6.955933397915622e-56.\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1226/5000\n",
      "\n",
      "Epoch 01226: LearningRateScheduler reducing learning rate to 6.293988815800106e-56.\n",
      "11/11 [==============================] - 0s 15ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1227/5000\n",
      "\n",
      "Epoch 01227: LearningRateScheduler reducing learning rate to 5.695036589235726e-56.\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1228/5000\n",
      "\n",
      "Epoch 01228: LearningRateScheduler reducing learning rate to 5.153082203024401e-56.\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1229/5000\n",
      "\n",
      "Epoch 01229: LearningRateScheduler reducing learning rate to 4.662701595511615e-56.\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1230/5000\n",
      "\n",
      "Epoch 01230: LearningRateScheduler reducing learning rate to 4.218986872754902e-56.\n",
      "11/11 [==============================] - 0s 30ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1231/5000\n",
      "\n",
      "Epoch 01231: LearningRateScheduler reducing learning rate to 3.8174971886711748e-56.\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1232/5000\n",
      "\n",
      "Epoch 01232: LearningRateScheduler reducing learning rate to 3.454214299556731e-56.\n",
      "11/11 [==============================] - 0s 24ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1233/5000\n",
      "\n",
      "Epoch 01233: LearningRateScheduler reducing learning rate to 3.1255023481538205e-56.\n",
      "11/11 [==============================] - 0s 15ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1234/5000\n",
      "\n",
      "Epoch 01234: LearningRateScheduler reducing learning rate to 2.828071474768808e-56.\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1235/5000\n",
      "\n",
      "Epoch 01235: LearningRateScheduler reducing learning rate to 2.558944891250971e-56.\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1236/5000\n",
      "\n",
      "Epoch 01236: LearningRateScheduler reducing learning rate to 2.315429088295851e-56.\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1237/5000\n",
      "\n",
      "Epoch 01237: LearningRateScheduler reducing learning rate to 2.0950868778989557e-56.\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1238/5000\n",
      "\n",
      "Epoch 01238: LearningRateScheduler reducing learning rate to 1.8957130011591217e-56.\n",
      "11/11 [==============================] - 0s 20ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1239/5000\n",
      "\n",
      "Epoch 01239: LearningRateScheduler reducing learning rate to 1.715312057306005e-56.\n",
      "11/11 [==============================] - 0s 24ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1240/5000\n",
      "\n",
      "Epoch 01240: LearningRateScheduler reducing learning rate to 1.5520785330587243e-56.\n",
      "11/11 [==============================] - 0s 15ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1241/5000\n",
      "\n",
      "Epoch 01241: LearningRateScheduler reducing learning rate to 1.4043787324419038e-56.\n",
      "11/11 [==============================] - 0s 20ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1242/5000\n",
      "\n",
      "Epoch 01242: LearningRateScheduler reducing learning rate to 1.270734426207335e-56.\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1243/5000\n",
      "\n",
      "Epoch 01243: LearningRateScheduler reducing learning rate to 1.1498080572188583e-56.\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1244/5000\n",
      "\n",
      "Epoch 01244: LearningRateScheduler reducing learning rate to 1.0403893537308456e-56.\n",
      "11/11 [==============================] - 0s 18ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1245/5000\n",
      "\n",
      "Epoch 01245: LearningRateScheduler reducing learning rate to 9.413832165819244e-57.\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1246/5000\n",
      "\n",
      "Epoch 01246: LearningRateScheduler reducing learning rate to 8.5179875907438e-57.\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1247/5000\n",
      "\n",
      "Epoch 01247: LearningRateScheduler reducing learning rate to 7.707393898470896e-57.\n",
      "11/11 [==============================] - 0s 17ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1248/5000\n",
      "\n",
      "Epoch 01248: LearningRateScheduler reducing learning rate to 6.973938394878554e-57.\n",
      "11/11 [==============================] - 0s 16ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1249/5000\n",
      "\n",
      "Epoch 01249: LearningRateScheduler reducing learning rate to 6.310280410763702e-57.\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1250/5000\n",
      "\n",
      "Epoch 01250: LearningRateScheduler reducing learning rate to 5.709777833958355e-57.\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1251/5000\n",
      "\n",
      "Epoch 01251: LearningRateScheduler reducing learning rate to 5.166420632837861e-57.\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1252/5000\n",
      "\n",
      "Epoch 01252: LearningRateScheduler reducing learning rate to 4.6747707059046785e-57.\n",
      "11/11 [==============================] - 0s 16ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1253/5000\n",
      "\n",
      "Epoch 01253: LearningRateScheduler reducing learning rate to 4.2299074554409534e-57.\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1254/5000\n",
      "\n",
      "Epoch 01254: LearningRateScheduler reducing learning rate to 3.827378540512216e-57.\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1255/5000\n",
      "\n",
      "Epoch 01255: LearningRateScheduler reducing learning rate to 3.463155316443332e-57.\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1256/5000\n",
      "\n",
      "Epoch 01256: LearningRateScheduler reducing learning rate to 3.133592514788109e-57.\n",
      "11/11 [==============================] - 0s 22ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1257/5000\n",
      "\n",
      "Epoch 01257: LearningRateScheduler reducing learning rate to 2.835391760257658e-57.\n",
      "11/11 [==============================] - 0s 16ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1258/5000\n",
      "\n",
      "Epoch 01258: LearningRateScheduler reducing learning rate to 2.565568559471988e-57.\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1259/5000\n",
      "\n",
      "Epoch 01259: LearningRateScheduler reducing learning rate to 2.3214224311468503e-57.\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1260/5000\n",
      "\n",
      "Epoch 01260: LearningRateScheduler reducing learning rate to 2.100509878769688e-57.\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1261/5000\n",
      "\n",
      "Epoch 01261: LearningRateScheduler reducing learning rate to 1.9006199352650016e-57.\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1262/5000\n",
      "\n",
      "Epoch 01262: LearningRateScheduler reducing learning rate to 1.719752034892842e-57.\n",
      "11/11 [==============================] - 0s 16ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1263/5000\n",
      "\n",
      "Epoch 01263: LearningRateScheduler reducing learning rate to 1.5560959909145355e-57.\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1264/5000\n",
      "\n",
      "Epoch 01264: LearningRateScheduler reducing learning rate to 1.4080138786352043e-57.\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1265/5000\n",
      "\n",
      "Epoch 01265: LearningRateScheduler reducing learning rate to 1.2740236425030824e-57.\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1266/5000\n",
      "\n",
      "Epoch 01266: LearningRateScheduler reducing learning rate to 1.1527842631992641e-57.\n",
      "11/11 [==============================] - 0s 17ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1267/5000\n",
      "\n",
      "Epoch 01267: LearningRateScheduler reducing learning rate to 1.0430823362656993e-57.\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1268/5000\n",
      "\n",
      "Epoch 01268: LearningRateScheduler reducing learning rate to 9.438199279455773e-58.\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1269/5000\n",
      "\n",
      "Epoch 01269: LearningRateScheduler reducing learning rate to 8.540035866931544e-58.\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1270/5000\n",
      "\n",
      "Epoch 01270: LearningRateScheduler reducing learning rate to 7.727344003768869e-58.\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1271/5000\n",
      "\n",
      "Epoch 01271: LearningRateScheduler reducing learning rate to 6.991989996645918e-58.\n",
      "11/11 [==============================] - 0s 21ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1272/5000\n",
      "\n",
      "Epoch 01272: LearningRateScheduler reducing learning rate to 6.3266141754982954e-58.\n",
      "11/11 [==============================] - 0s 17ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1273/5000\n",
      "\n",
      "Epoch 01273: LearningRateScheduler reducing learning rate to 5.724557235467611e-58.\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1274/5000\n",
      "\n",
      "Epoch 01274: LearningRateScheduler reducing learning rate to 5.17979358833954e-58.\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1275/5000\n",
      "\n",
      "Epoch 01275: LearningRateScheduler reducing learning rate to 4.686871056432394e-58.\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1276/5000\n",
      "\n",
      "Epoch 01276: LearningRateScheduler reducing learning rate to 4.240856305369782e-58.\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1277/5000\n",
      "\n",
      "Epoch 01277: LearningRateScheduler reducing learning rate to 3.8372854696122794e-58.\n",
      "11/11 [==============================] - 0s 23ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1278/5000\n",
      "\n",
      "Epoch 01278: LearningRateScheduler reducing learning rate to 3.472119476590899e-58.\n",
      "11/11 [==============================] - 0s 17ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1279/5000\n",
      "\n",
      "Epoch 01279: LearningRateScheduler reducing learning rate to 3.14170362231085e-58.\n",
      "11/11 [==============================] - 0s 17ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1280/5000\n",
      "\n",
      "Epoch 01280: LearningRateScheduler reducing learning rate to 2.8427309938459872e-58.\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1281/5000\n",
      "\n",
      "Epoch 01281: LearningRateScheduler reducing learning rate to 2.5722093726424148e-58.\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1282/5000\n",
      "\n",
      "Epoch 01282: LearningRateScheduler reducing learning rate to 2.3274312873896712e-58.\n",
      "11/11 [==============================] - 0s 25ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1283/5000\n",
      "\n",
      "Epoch 01283: LearningRateScheduler reducing learning rate to 2.1059469167377315e-58.\n",
      "11/11 [==============================] - 0s 32ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1284/5000\n",
      "\n",
      "Epoch 01284: LearningRateScheduler reducing learning rate to 1.9055395706617698e-58.\n",
      "11/11 [==============================] - 0s 21ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1285/5000\n",
      "\n",
      "Epoch 01285: LearningRateScheduler reducing learning rate to 1.724203505082957e-58.\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1286/5000\n",
      "\n",
      "Epoch 01286: LearningRateScheduler reducing learning rate to 1.5601238477078229e-58.\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1287/5000\n",
      "\n",
      "Epoch 01287: LearningRateScheduler reducing learning rate to 1.411658434176281e-58.\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1288/5000\n",
      "\n",
      "Epoch 01288: LearningRateScheduler reducing learning rate to 1.2773213727287227e-58.\n",
      "11/11 [==============================] - 0s 16ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1289/5000\n",
      "\n",
      "Epoch 01289: LearningRateScheduler reducing learning rate to 1.1557681729020116e-58.\n",
      "11/11 [==============================] - 0s 30ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1290/5000\n",
      "\n",
      "Epoch 01290: LearningRateScheduler reducing learning rate to 1.0457822894168005e-58.\n",
      "11/11 [==============================] - 0s 18ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1291/5000\n",
      "\n",
      "Epoch 01291: LearningRateScheduler reducing learning rate to 9.462629465836379e-59.\n",
      "11/11 [==============================] - 0s 16ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1292/5000\n",
      "\n",
      "Epoch 01292: LearningRateScheduler reducing learning rate to 8.56214121369843e-59.\n",
      "11/11 [==============================] - 0s 16ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1293/5000\n",
      "\n",
      "Epoch 01293: LearningRateScheduler reducing learning rate to 7.747345748661987e-59.\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1294/5000\n",
      "\n",
      "Epoch 01294: LearningRateScheduler reducing learning rate to 7.01008832385122e-59.\n",
      "11/11 [==============================] - 0s 18ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1295/5000\n",
      "\n",
      "Epoch 01295: LearningRateScheduler reducing learning rate to 6.342990219157603e-59.\n",
      "11/11 [==============================] - 0s 15ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1296/5000\n",
      "\n",
      "Epoch 01296: LearningRateScheduler reducing learning rate to 5.739374892529943e-59.\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1297/5000\n",
      "\n",
      "Epoch 01297: LearningRateScheduler reducing learning rate to 5.1932011588972355e-59.\n",
      "11/11 [==============================] - 0s 20ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1298/5000\n",
      "\n",
      "Epoch 01298: LearningRateScheduler reducing learning rate to 4.699002727957821e-59.\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1299/5000\n",
      "\n",
      "Epoch 01299: LearningRateScheduler reducing learning rate to 4.25183349570931e-59.\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1300/5000\n",
      "\n",
      "Epoch 01300: LearningRateScheduler reducing learning rate to 3.8472180421764416e-59.\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1301/5000\n",
      "\n",
      "Epoch 01301: LearningRateScheduler reducing learning rate to 3.4811068399043106e-59.\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1302/5000\n",
      "\n",
      "Epoch 01302: LearningRateScheduler reducing learning rate to 3.149835724926353e-59.\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1303/5000\n",
      "\n",
      "Epoch 01303: LearningRateScheduler reducing learning rate to 2.8500892245797213e-59.\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1304/5000\n",
      "\n",
      "Epoch 01304: LearningRateScheduler reducing learning rate to 2.5788673751408396e-59.\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1305/5000\n",
      "\n",
      "Epoch 01305: LearningRateScheduler reducing learning rate to 2.333455697179623e-59.\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1306/5000\n",
      "\n",
      "Epoch 01306: LearningRateScheduler reducing learning rate to 2.111398028137322e-59.\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1307/5000\n",
      "\n",
      "Epoch 01307: LearningRateScheduler reducing learning rate to 1.9104719402260018e-59.\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1308/5000\n",
      "\n",
      "Epoch 01308: LearningRateScheduler reducing learning rate to 1.7286664976242062e-59.\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1309/5000\n",
      "\n",
      "Epoch 01309: LearningRateScheduler reducing learning rate to 1.5641621303555607e-59.\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1310/5000\n",
      "\n",
      "Epoch 01310: LearningRateScheduler reducing learning rate to 1.4153124234205599e-59.\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1311/5000\n",
      "\n",
      "Epoch 01311: LearningRateScheduler reducing learning rate to 1.2806276389220833e-59.\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1312/5000\n",
      "\n",
      "Epoch 01312: LearningRateScheduler reducing learning rate to 1.1587598062677515e-59.\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1313/5000\n",
      "\n",
      "Epoch 01313: LearningRateScheduler reducing learning rate to 1.0484892312271372e-59.\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1314/5000\n",
      "\n",
      "Epoch 01314: LearningRateScheduler reducing learning rate to 9.487122888220764e-60.\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1315/5000\n",
      "\n",
      "Epoch 01315: LearningRateScheduler reducing learning rate to 8.58430377876758e-60.\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1316/5000\n",
      "\n",
      "Epoch 01316: LearningRateScheduler reducing learning rate to 7.767399266816434e-60.\n",
      "11/11 [==============================] - 0s 16ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1317/5000\n",
      "\n",
      "Epoch 01317: LearningRateScheduler reducing learning rate to 7.028233497440627e-60.\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1318/5000\n",
      "\n",
      "Epoch 01318: LearningRateScheduler reducing learning rate to 6.359408651177874e-60.\n",
      "11/11 [==============================] - 0s 16ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1319/5000\n",
      "\n",
      "Epoch 01319: LearningRateScheduler reducing learning rate to 5.754230904167364e-60.\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1320/5000\n",
      "\n",
      "Epoch 01320: LearningRateScheduler reducing learning rate to 5.206643434109552e-60.\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1321/5000\n",
      "\n",
      "Epoch 01321: LearningRateScheduler reducing learning rate to 4.7111658015535966e-60.\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1322/5000\n",
      "\n",
      "Epoch 01322: LearningRateScheduler reducing learning rate to 4.262839099817092e-60.\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1323/5000\n",
      "\n",
      "Epoch 01323: LearningRateScheduler reducing learning rate to 3.857176324581144e-60.\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1324/5000\n",
      "\n",
      "Epoch 01324: LearningRateScheduler reducing learning rate to 3.4901174664434544e-60.\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1325/5000\n",
      "\n",
      "Epoch 01325: LearningRateScheduler reducing learning rate to 3.157988876978918e-60.\n",
      "11/11 [==============================] - 0s 26ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1326/5000\n",
      "\n",
      "Epoch 01326: LearningRateScheduler reducing learning rate to 2.8574665016319005e-60.\n",
      "11/11 [==============================] - 0s 16ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1327/5000\n",
      "\n",
      "Epoch 01327: LearningRateScheduler reducing learning rate to 2.5855426114608697e-60.\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1328/5000\n",
      "\n",
      "Epoch 01328: LearningRateScheduler reducing learning rate to 2.3394957007761518e-60.\n",
      "11/11 [==============================] - 0s 16ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1329/5000\n",
      "\n",
      "Epoch 01329: LearningRateScheduler reducing learning rate to 2.1168632493965332e-60.\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1330/5000\n",
      "\n",
      "Epoch 01330: LearningRateScheduler reducing learning rate to 1.9154170769191816e-60.\n",
      "11/11 [==============================] - 0s 16ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1331/5000\n",
      "\n",
      "Epoch 01331: LearningRateScheduler reducing learning rate to 1.733141042341547e-60.\n",
      "11/11 [==============================] - 0s 17ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1332/5000\n",
      "\n",
      "Epoch 01332: LearningRateScheduler reducing learning rate to 1.568210865844486e-60.\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1333/5000\n",
      "\n",
      "Epoch 01333: LearningRateScheduler reducing learning rate to 1.418975870786629e-60.\n",
      "11/11 [==============================] - 0s 22ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1334/5000\n",
      "\n",
      "Epoch 01334: LearningRateScheduler reducing learning rate to 1.2839424631779082e-60.\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1335/5000\n",
      "\n",
      "Epoch 01335: LearningRateScheduler reducing learning rate to 1.1617591832886352e-60.\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1336/5000\n",
      "\n",
      "Epoch 01336: LearningRateScheduler reducing learning rate to 1.0512031797864598e-60.\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1337/5000\n",
      "\n",
      "Epoch 01337: LearningRateScheduler reducing learning rate to 9.511679710291762e-61.\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1338/5000\n",
      "\n",
      "Epoch 01338: LearningRateScheduler reducing learning rate to 8.606523710245225e-61.\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1339/5000\n",
      "\n",
      "Epoch 01339: LearningRateScheduler reducing learning rate to 7.787504692243602e-61.\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1340/5000\n",
      "\n",
      "Epoch 01340: LearningRateScheduler reducing learning rate to 7.04642563867266e-61.\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1341/5000\n",
      "\n",
      "Epoch 01341: LearningRateScheduler reducing learning rate to 6.375869581278993e-61.\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1342/5000\n",
      "\n",
      "Epoch 01342: LearningRateScheduler reducing learning rate to 5.769125369658532e-61.\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1343/5000\n",
      "\n",
      "Epoch 01343: LearningRateScheduler reducing learning rate to 5.2201205038074584e-61.\n",
      "11/11 [==============================] - 0s 18ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1344/5000\n",
      "\n",
      "Epoch 01344: LearningRateScheduler reducing learning rate to 4.72336035850174e-61.\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1345/5000\n",
      "\n",
      "Epoch 01345: LearningRateScheduler reducing learning rate to 4.273873191240143e-61.\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1346/5000\n",
      "\n",
      "Epoch 01346: LearningRateScheduler reducing learning rate to 3.86716038337486e-61.\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1347/5000\n",
      "\n",
      "Epoch 01347: LearningRateScheduler reducing learning rate to 3.49915141642388e-61.\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1348/5000\n",
      "\n",
      "Epoch 01348: LearningRateScheduler reducing learning rate to 3.166163132953782e-61.\n",
      "11/11 [==============================] - 0s 21ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1349/5000\n",
      "\n",
      "Epoch 01349: LearningRateScheduler reducing learning rate to 2.864862874302561e-61.\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1350/5000\n",
      "\n",
      "Epoch 01350: LearningRateScheduler reducing learning rate to 2.592235126211022e-61.\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1351/5000\n",
      "\n",
      "Epoch 01351: LearningRateScheduler reducing learning rate to 2.3455513385429144e-61.\n",
      "11/11 [==============================] - 0s 16ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1352/5000\n",
      "\n",
      "Epoch 01352: LearningRateScheduler reducing learning rate to 2.1223426170379716e-61.\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1353/5000\n",
      "\n",
      "Epoch 01353: LearningRateScheduler reducing learning rate to 1.920375013788276e-61.\n",
      "11/11 [==============================] - 0s 16ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1354/5000\n",
      "\n",
      "Epoch 01354: LearningRateScheduler reducing learning rate to 1.7376271691369635e-61.\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1355/5000\n",
      "\n",
      "Epoch 01355: LearningRateScheduler reducing learning rate to 1.572270081231033e-61.\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1356/5000\n",
      "\n",
      "Epoch 01356: LearningRateScheduler reducing learning rate to 1.4226488007562843e-61.\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1357/5000\n",
      "\n",
      "Epoch 01357: LearningRateScheduler reducing learning rate to 1.2872658676482778e-61.\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1358/5000\n",
      "\n",
      "Epoch 01358: LearningRateScheduler reducing learning rate to 1.1647663240086604e-61.\n",
      "11/11 [==============================] - 0s 16ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1359/5000\n",
      "\n",
      "Epoch 01359: LearningRateScheduler reducing learning rate to 1.0539241532312382e-61.\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1360/5000\n",
      "\n",
      "Epoch 01360: LearningRateScheduler reducing learning rate to 9.53630009615494e-62.\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1361/5000\n",
      "\n",
      "Epoch 01361: LearningRateScheduler reducing learning rate to 8.628801156620958e-62.\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1362/5000\n",
      "\n",
      "Epoch 01362: LearningRateScheduler reducing learning rate to 7.807662159302654e-62.\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1363/5000\n",
      "\n",
      "Epoch 01363: LearningRateScheduler reducing learning rate to 7.064664869120317e-62.\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1364/5000\n",
      "\n",
      "Epoch 01364: LearningRateScheduler reducing learning rate to 6.392373119464214e-62.\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1365/5000\n",
      "\n",
      "Epoch 01365: LearningRateScheduler reducing learning rate to 5.784058388538506e-62.\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1366/5000\n",
      "\n",
      "Epoch 01366: LearningRateScheduler reducing learning rate to 5.233632458054444e-62.\n",
      "11/11 [==============================] - 0s 16ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1367/5000\n",
      "\n",
      "Epoch 01367: LearningRateScheduler reducing learning rate to 4.735586480295203e-62.\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1368/5000\n",
      "\n",
      "Epoch 01368: LearningRateScheduler reducing learning rate to 4.284935843716211e-62.\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1369/5000\n",
      "\n",
      "Epoch 01369: LearningRateScheduler reducing learning rate to 3.8771702852779347e-62.\n",
      "11/11 [==============================] - 0s 31ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1370/5000\n",
      "\n",
      "Epoch 01370: LearningRateScheduler reducing learning rate to 3.508208750216651e-62.\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1371/5000\n",
      "\n",
      "Epoch 01371: LearningRateScheduler reducing learning rate to 3.1743585474772133e-62.\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1372/5000\n",
      "\n",
      "Epoch 01372: LearningRateScheduler reducing learning rate to 2.872278392019677e-62.\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1373/5000\n",
      "\n",
      "Epoch 01373: LearningRateScheduler reducing learning rate to 2.5989449641155033e-62.\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1374/5000\n",
      "\n",
      "Epoch 01374: LearningRateScheduler reducing learning rate to 2.3516226509478446e-62.\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1375/5000\n",
      "\n",
      "Epoch 01375: LearningRateScheduler reducing learning rate to 2.1278361676785386e-62.\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1376/5000\n",
      "\n",
      "Epoch 01376: LearningRateScheduler reducing learning rate to 1.925345783965791e-62.\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1377/5000\n",
      "\n",
      "Epoch 01377: LearningRateScheduler reducing learning rate to 1.7421249079900365e-62.\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1378/5000\n",
      "\n",
      "Epoch 01378: LearningRateScheduler reducing learning rate to 1.5763398036418026e-62.\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1379/5000\n",
      "\n",
      "Epoch 01379: LearningRateScheduler reducing learning rate to 1.4263312378745682e-62.\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1380/5000\n",
      "\n",
      "Epoch 01380: LearningRateScheduler reducing learning rate to 1.2905978745424657e-62.\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1381/5000\n",
      "\n",
      "Epoch 01381: LearningRateScheduler reducing learning rate to 1.1677812485237087e-62.\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1382/5000\n",
      "\n",
      "Epoch 01382: LearningRateScheduler reducing learning rate to 1.0566521697450078e-62.\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1383/5000\n",
      "\n",
      "Epoch 01383: LearningRateScheduler reducing learning rate to 9.560984210341456e-63.\n",
      "11/11 [==============================] - 0s 21ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1384/5000\n",
      "\n",
      "Epoch 01384: LearningRateScheduler reducing learning rate to 8.651136266767989e-63.\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1385/5000\n",
      "\n",
      "Epoch 01385: LearningRateScheduler reducing learning rate to 7.827871802699642e-63.\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1386/5000\n",
      "\n",
      "Epoch 01386: LearningRateScheduler reducing learning rate to 7.082951310671277e-63.\n",
      "11/11 [==============================] - 0s 18ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1387/5000\n",
      "\n",
      "Epoch 01387: LearningRateScheduler reducing learning rate to 6.40891937602225e-63.\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1388/5000\n",
      "\n",
      "Epoch 01388: LearningRateScheduler reducing learning rate to 5.799030060600474e-63.\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1389/5000\n",
      "\n",
      "Epoch 01389: LearningRateScheduler reducing learning rate to 5.247179387146677e-63.\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1390/5000\n",
      "\n",
      "Epoch 01390: LearningRateScheduler reducing learning rate to 4.7478442486373355e-63.\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1391/5000\n",
      "\n",
      "Epoch 01391: LearningRateScheduler reducing learning rate to 4.2960271311739116e-63.\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1392/5000\n",
      "\n",
      "Epoch 01392: LearningRateScheduler reducing learning rate to 3.8872060971838546e-63.\n",
      "11/11 [==============================] - 0s 23ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1393/5000\n",
      "\n",
      "Epoch 01393: LearningRateScheduler reducing learning rate to 3.517289528349398e-63.\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1394/5000\n",
      "\n",
      "Epoch 01394: LearningRateScheduler reducing learning rate to 3.1825751753166055e-63.\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1395/5000\n",
      "\n",
      "Epoch 01395: LearningRateScheduler reducing learning rate to 2.879713104338835e-63.\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1396/5000\n",
      "\n",
      "Epoch 01396: LearningRateScheduler reducing learning rate to 2.6056721700142846e-63.\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1397/5000\n",
      "\n",
      "Epoch 01397: LearningRateScheduler reducing learning rate to 2.3577096785638943e-63.\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1398/5000\n",
      "\n",
      "Epoch 01398: LearningRateScheduler reducing learning rate to 2.133343938030098e-63.\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1399/5000\n",
      "\n",
      "Epoch 01399: LearningRateScheduler reducing learning rate to 1.930329420669831e-63.\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1400/5000\n",
      "\n",
      "Epoch 01400: LearningRateScheduler reducing learning rate to 1.7466342889577493e-63.\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1401/5000\n",
      "\n",
      "Epoch 01401: LearningRateScheduler reducing learning rate to 1.580420060273613e-63.\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1402/5000\n",
      "\n",
      "Epoch 01402: LearningRateScheduler reducing learning rate to 1.4300232067502196e-63.\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1403/5000\n",
      "\n",
      "Epoch 01403: LearningRateScheduler reducing learning rate to 1.2939385061273427e-63.\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1404/5000\n",
      "\n",
      "Epoch 01404: LearningRateScheduler reducing learning rate to 1.170803976981578e-63.\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1405/5000\n",
      "\n",
      "Epoch 01405: LearningRateScheduler reducing learning rate to 1.0593872475582502e-63.\n",
      "11/11 [==============================] - 0s 15ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1406/5000\n",
      "\n",
      "Epoch 01406: LearningRateScheduler reducing learning rate to 9.585732217808344e-64.\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1407/5000\n",
      "\n",
      "Epoch 01407: LearningRateScheduler reducing learning rate to 8.673529189945864e-64.\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1408/5000\n",
      "\n",
      "Epoch 01408: LearningRateScheduler reducing learning rate to 7.848133757489964e-64.\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1409/5000\n",
      "\n",
      "Epoch 01409: LearningRateScheduler reducing learning rate to 7.101285085528114e-64.\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1410/5000\n",
      "\n",
      "Epoch 01410: LearningRateScheduler reducing learning rate to 6.425508461526563e-64.\n",
      "11/11 [==============================] - 0s 16ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1411/5000\n",
      "\n",
      "Epoch 01411: LearningRateScheduler reducing learning rate to 5.814040485895939e-64.\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1412/5000\n",
      "\n",
      "Epoch 01412: LearningRateScheduler reducing learning rate to 5.260761381614647e-64.\n",
      "11/11 [==============================] - 0s 16ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1413/5000\n",
      "\n",
      "Epoch 01413: LearningRateScheduler reducing learning rate to 4.760133745443377e-64.\n",
      "11/11 [==============================] - 0s 15ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1414/5000\n",
      "\n",
      "Epoch 01414: LearningRateScheduler reducing learning rate to 4.307147127732851e-64.\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1415/5000\n",
      "\n",
      "Epoch 01415: LearningRateScheduler reducing learning rate to 3.897267886158815e-64.\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1416/5000\n",
      "\n",
      "Epoch 01416: LearningRateScheduler reducing learning rate to 3.526393811506424e-64.\n",
      "11/11 [==============================] - 0s 15ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1417/5000\n",
      "\n",
      "Epoch 01417: LearningRateScheduler reducing learning rate to 3.190813071381477e-64.\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1418/5000\n",
      "\n",
      "Epoch 01418: LearningRateScheduler reducing learning rate to 2.88716706094414e-64.\n",
      "11/11 [==============================] - 0s 18ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1419/5000\n",
      "\n",
      "Epoch 01419: LearningRateScheduler reducing learning rate to 2.6124167888631803e-64.\n",
      "11/11 [==============================] - 0s 16ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1420/5000\n",
      "\n",
      "Epoch 01420: LearningRateScheduler reducing learning rate to 2.363812462068766e-64.\n",
      "11/11 [==============================] - 0s 16ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1421/5000\n",
      "\n",
      "Epoch 01421: LearningRateScheduler reducing learning rate to 2.1388659648995393e-64.\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1422/5000\n",
      "\n",
      "Epoch 01422: LearningRateScheduler reducing learning rate to 1.9353259572047014e-64.\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1423/5000\n",
      "\n",
      "Epoch 01423: LearningRateScheduler reducing learning rate to 1.751155342175034e-64.\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1424/5000\n",
      "\n",
      "Epoch 01424: LearningRateScheduler reducing learning rate to 1.5845108783935443e-64.\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1425/5000\n",
      "\n",
      "Epoch 01425: LearningRateScheduler reducing learning rate to 1.4337247320555129e-64.\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1426/5000\n",
      "\n",
      "Epoch 01426: LearningRateScheduler reducing learning rate to 1.2972877847274156e-64.\n",
      "11/11 [==============================] - 0s 21ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1427/5000\n",
      "\n",
      "Epoch 01427: LearningRateScheduler reducing learning rate to 1.1738345295823513e-64.\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1428/5000\n",
      "\n",
      "Epoch 01428: LearningRateScheduler reducing learning rate to 1.0621294049487258e-64.\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1429/5000\n",
      "\n",
      "Epoch 01429: LearningRateScheduler reducing learning rate to 9.610544283938808e-65.\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1430/5000\n",
      "\n",
      "Epoch 01430: LearningRateScheduler reducing learning rate to 8.69598007579949e-65.\n",
      "11/11 [==============================] - 0s 23ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1431/5000\n",
      "\n",
      "Epoch 01431: LearningRateScheduler reducing learning rate to 7.868448159078603e-65.\n",
      "11/11 [==============================] - 0s 21ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1432/5000\n",
      "\n",
      "Epoch 01432: LearningRateScheduler reducing learning rate to 7.119666316210522e-65.\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1433/5000\n",
      "\n",
      "Epoch 01433: LearningRateScheduler reducing learning rate to 6.442140486837375e-65.\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1434/5000\n",
      "\n",
      "Epoch 01434: LearningRateScheduler reducing learning rate to 5.829089764734883e-65.\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1435/5000\n",
      "\n",
      "Epoch 01435: LearningRateScheduler reducing learning rate to 5.27437853222258e-65.\n",
      "11/11 [==============================] - 2s 153ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1436/5000\n",
      "\n",
      "Epoch 01436: LearningRateScheduler reducing learning rate to 4.772455052840601e-65.\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1437/5000\n",
      "\n",
      "Epoch 01437: LearningRateScheduler reducing learning rate to 4.318295907704983e-65.\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1438/5000\n",
      "\n",
      "Epoch 01438: LearningRateScheduler reducing learning rate to 3.907355719442938e-65.\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1439/5000\n",
      "\n",
      "Epoch 01439: LearningRateScheduler reducing learning rate to 3.5355216605288075e-65.\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1440/5000\n",
      "\n",
      "Epoch 01440: LearningRateScheduler reducing learning rate to 3.1990722907231125e-65.\n",
      "11/11 [==============================] - 0s 17ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1441/5000\n",
      "\n",
      "Epoch 01441: LearningRateScheduler reducing learning rate to 2.8946403116483002e-65.\n",
      "11/11 [==============================] - 0s 17ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1442/5000\n",
      "\n",
      "Epoch 01442: LearningRateScheduler reducing learning rate to 2.619178865734668e-65.\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1443/5000\n",
      "\n",
      "Epoch 01443: LearningRateScheduler reducing learning rate to 2.369931042245657e-65.\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1444/5000\n",
      "\n",
      "Epoch 01444: LearningRateScheduler reducing learning rate to 2.144402285188843e-65.\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1445/5000\n",
      "\n",
      "Epoch 01445: LearningRateScheduler reducing learning rate to 1.940335426960695e-65.\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1446/5000\n",
      "\n",
      "Epoch 01446: LearningRateScheduler reducing learning rate to 1.7556880978548265e-65.\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1447/5000\n",
      "\n",
      "Epoch 01447: LearningRateScheduler reducing learning rate to 1.5886122853394354e-65.\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1448/5000\n",
      "\n",
      "Epoch 01448: LearningRateScheduler reducing learning rate to 1.437435838526707e-65.\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1449/5000\n",
      "\n",
      "Epoch 01449: LearningRateScheduler reducing learning rate to 1.3006457327248676e-65.\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1450/5000\n",
      "\n",
      "Epoch 01450: LearningRateScheduler reducing learning rate to 1.1768729265782646e-65.\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1451/5000\n",
      "\n",
      "Epoch 01451: LearningRateScheduler reducing learning rate to 1.0648786602415064e-65.\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1452/5000\n",
      "\n",
      "Epoch 01452: LearningRateScheduler reducing learning rate to 9.63542057454522e-66.\n",
      "11/11 [==============================] - 0s 21ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1453/5000\n",
      "\n",
      "Epoch 01453: LearningRateScheduler reducing learning rate to 8.71848907436186e-66.\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1454/5000\n",
      "\n",
      "Epoch 01454: LearningRateScheduler reducing learning rate to 7.888815143220354e-66.\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1455/5000\n",
      "\n",
      "Epoch 01455: LearningRateScheduler reducing learning rate to 7.138095125554525e-66.\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1456/5000\n",
      "\n",
      "Epoch 01456: LearningRateScheduler reducing learning rate to 6.458815563101861e-66.\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1457/5000\n",
      "\n",
      "Epoch 01457: LearningRateScheduler reducing learning rate to 5.844177997687594e-66.\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1458/5000\n",
      "\n",
      "Epoch 01458: LearningRateScheduler reducing learning rate to 5.288030929970086e-66.\n",
      "11/11 [==============================] - 0s 21ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1459/5000\n",
      "\n",
      "Epoch 01459: LearningRateScheduler reducing learning rate to 4.784808253168454e-66.\n",
      "11/11 [==============================] - 0s 15ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1460/5000\n",
      "\n",
      "Epoch 01460: LearningRateScheduler reducing learning rate to 4.3294735455941187e-66.\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1461/5000\n",
      "\n",
      "Epoch 01461: LearningRateScheduler reducing learning rate to 3.917469664450395e-66.\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1462/5000\n",
      "\n",
      "Epoch 01462: LearningRateScheduler reducing learning rate to 3.544673136415513e-66.\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1463/5000\n",
      "\n",
      "Epoch 01463: LearningRateScheduler reducing learning rate to 3.2073528885355667e-66.\n",
      "11/11 [==============================] - 0s 20ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1464/5000\n",
      "\n",
      "Epoch 01464: LearningRateScheduler reducing learning rate to 2.902132906392715e-66.\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1465/5000\n",
      "\n",
      "Epoch 01465: LearningRateScheduler reducing learning rate to 2.625958445817595e-66.\n",
      "11/11 [==============================] - 0s 16ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1466/5000\n",
      "\n",
      "Epoch 01466: LearningRateScheduler reducing learning rate to 2.376065459983327e-66.\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1467/5000\n",
      "\n",
      "Epoch 01467: LearningRateScheduler reducing learning rate to 2.1499529358957508e-66.\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1468/5000\n",
      "\n",
      "Epoch 01468: LearningRateScheduler reducing learning rate to 1.9453578634146975e-66.\n",
      "11/11 [==============================] - 0s 24ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1469/5000\n",
      "\n",
      "Epoch 01469: LearningRateScheduler reducing learning rate to 1.760232586288116e-66.\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1470/5000\n",
      "\n",
      "Epoch 01470: LearningRateScheduler reducing learning rate to 1.5927243085197072e-66.\n",
      "11/11 [==============================] - 0s 18ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1471/5000\n",
      "\n",
      "Epoch 01471: LearningRateScheduler reducing learning rate to 1.4411565509640891e-66.\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1472/5000\n",
      "\n",
      "Epoch 01472: LearningRateScheduler reducing learning rate to 1.3040123725599625e-66.\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1473/5000\n",
      "\n",
      "Epoch 01473: LearningRateScheduler reducing learning rate to 1.1799191882740756e-66.\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1474/5000\n",
      "\n",
      "Epoch 01474: LearningRateScheduler reducing learning rate to 1.0676350318090059e-66.\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1475/5000\n",
      "\n",
      "Epoch 01475: LearningRateScheduler reducing learning rate to 9.66036125586806e-67.\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1476/5000\n",
      "\n",
      "Epoch 01476: LearningRateScheduler reducing learning rate to 8.741056336054323e-67.\n",
      "11/11 [==============================] - 0s 18ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1477/5000\n",
      "\n",
      "Epoch 01477: LearningRateScheduler reducing learning rate to 7.909234846022304e-67.\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1478/5000\n",
      "\n",
      "Epoch 01478: LearningRateScheduler reducing learning rate to 7.1565716367147e-67.\n",
      "11/11 [==============================] - 0s 17ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1479/5000\n",
      "\n",
      "Epoch 01479: LearningRateScheduler reducing learning rate to 6.475533801754347e-67.\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1480/5000\n",
      "\n",
      "Epoch 01480: LearningRateScheduler reducing learning rate to 5.859305285584018e-67.\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1481/5000\n",
      "\n",
      "Epoch 01481: LearningRateScheduler reducing learning rate to 5.3017186660923235e-67.\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1482/5000\n",
      "\n",
      "Epoch 01482: LearningRateScheduler reducing learning rate to 4.7971934289800576e-67.\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1483/5000\n",
      "\n",
      "Epoch 01483: LearningRateScheduler reducing learning rate to 4.3406801160972874e-67.\n",
      "11/11 [==============================] - 0s 15ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1484/5000\n",
      "\n",
      "Epoch 01484: LearningRateScheduler reducing learning rate to 3.927609788769521e-67.\n",
      "11/11 [==============================] - 0s 17ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1485/5000\n",
      "\n",
      "Epoch 01485: LearningRateScheduler reducing learning rate to 3.553848300322995e-67.\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1486/5000\n",
      "\n",
      "Epoch 01486: LearningRateScheduler reducing learning rate to 3.21565492015576e-67.\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1487/5000\n",
      "\n",
      "Epoch 01487: LearningRateScheduler reducing learning rate to 2.9096448952483843e-67.\n",
      "11/11 [==============================] - 0s 17ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1488/5000\n",
      "\n",
      "Epoch 01488: LearningRateScheduler reducing learning rate to 2.632755574417998e-67.\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1489/5000\n",
      "\n",
      "Epoch 01489: LearningRateScheduler reducing learning rate to 2.3822157562761745e-67.\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1490/5000\n",
      "\n",
      "Epoch 01490: LearningRateScheduler reducing learning rate to 2.1555179541135267e-67.\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1491/5000\n",
      "\n",
      "Epoch 01491: LearningRateScheduler reducing learning rate to 1.9503933001302486e-67.\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1492/5000\n",
      "\n",
      "Epoch 01492: LearningRateScheduler reducing learning rate to 1.7647888378444985e-67.\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1493/5000\n",
      "\n",
      "Epoch 01493: LearningRateScheduler reducing learning rate to 1.5968469754138616e-67.\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1494/5000\n",
      "\n",
      "Epoch 01494: LearningRateScheduler reducing learning rate to 1.4448868942320181e-67.\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1495/5000\n",
      "\n",
      "Epoch 01495: LearningRateScheduler reducing learning rate to 1.3073877267309033e-67.\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1496/5000\n",
      "\n",
      "Epoch 01496: LearningRateScheduler reducing learning rate to 1.1829733350271e-67.\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1497/5000\n",
      "\n",
      "Epoch 01497: LearningRateScheduler reducing learning rate to 1.0703985380713154e-67.\n",
      "11/11 [==============================] - 0s 16ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1498/5000\n",
      "\n",
      "Epoch 01498: LearningRateScheduler reducing learning rate to 9.685366494578927e-68.\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1499/5000\n",
      "\n",
      "Epoch 01499: LearningRateScheduler reducing learning rate to 8.76368201168684e-68.\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1500/5000\n",
      "\n",
      "Epoch 01500: LearningRateScheduler reducing learning rate to 7.929707403942948e-68.\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1501/5000\n",
      "\n",
      "Epoch 01501: LearningRateScheduler reducing learning rate to 7.175095973164411e-68.\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1502/5000\n",
      "\n",
      "Epoch 01502: LearningRateScheduler reducing learning rate to 6.492295314518333e-68.\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1503/5000\n",
      "\n",
      "Epoch 01503: LearningRateScheduler reducing learning rate to 5.8744717295155935e-68.\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1504/5000\n",
      "\n",
      "Epoch 01504: LearningRateScheduler reducing learning rate to 5.315441832060158e-68.\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1505/5000\n",
      "\n",
      "Epoch 01505: LearningRateScheduler reducing learning rate to 4.80961066304167e-68.\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1506/5000\n",
      "\n",
      "Epoch 01506: LearningRateScheduler reducing learning rate to 4.35191569410487e-68.\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1507/5000\n",
      "\n",
      "Epoch 01507: LearningRateScheduler reducing learning rate to 3.937776160164043e-68.\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1508/5000\n",
      "\n",
      "Epoch 01508: LearningRateScheduler reducing learning rate to 3.5630472135663072e-68.\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1509/5000\n",
      "\n",
      "Epoch 01509: LearningRateScheduler reducing learning rate to 3.223978441063576e-68.\n",
      "11/11 [==============================] - ETA: 0s - loss: 103.9779 - mae: 7.2109 - mse: 103.977 - 0s 9ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1510/5000\n",
      "\n",
      "Epoch 01510: LearningRateScheduler reducing learning rate to 2.9171763284155804e-68.\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1511/5000\n",
      "\n",
      "Epoch 01511: LearningRateScheduler reducing learning rate to 2.6395702969591894e-68.\n",
      "11/11 [==============================] - 0s 21ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1512/5000\n",
      "\n",
      "Epoch 01512: LearningRateScheduler reducing learning rate to 2.388381972224978e-68.\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1513/5000\n",
      "\n",
      "Epoch 01513: LearningRateScheduler reducing learning rate to 2.1610973770316326e-68.\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1514/5000\n",
      "\n",
      "Epoch 01514: LearningRateScheduler reducing learning rate to 1.955441770757598e-68.\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1515/5000\n",
      "\n",
      "Epoch 01515: LearningRateScheduler reducing learning rate to 1.76935688297198e-68.\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1516/5000\n",
      "\n",
      "Epoch 01516: LearningRateScheduler reducing learning rate to 1.600980313572529e-68.\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1517/5000\n",
      "\n",
      "Epoch 01517: LearningRateScheduler reducing learning rate to 1.4486268932593763e-68.\n",
      "11/11 [==============================] - 0s 23ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1518/5000\n",
      "\n",
      "Epoch 01518: LearningRateScheduler reducing learning rate to 1.3107718177942378e-68.\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1519/5000\n",
      "\n",
      "Epoch 01519: LearningRateScheduler reducing learning rate to 1.1860353872472461e-68.\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1520/5000\n",
      "\n",
      "Epoch 01520: LearningRateScheduler reducing learning rate to 1.0731691974960837e-68.\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1521/5000\n",
      "\n",
      "Epoch 01521: LearningRateScheduler reducing learning rate to 9.710436457780846e-69.\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1522/5000\n",
      "\n",
      "Epoch 01522: LearningRateScheduler reducing learning rate to 8.786366252460721e-69.\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1523/5000\n",
      "\n",
      "Epoch 01523: LearningRateScheduler reducing learning rate to 7.950232953794667e-69.\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1524/5000\n",
      "\n",
      "Epoch 01524: LearningRateScheduler reducing learning rate to 7.193668258696008e-69.\n",
      "11/11 [==============================] - 0s 17ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1525/5000\n",
      "\n",
      "Epoch 01525: LearningRateScheduler reducing learning rate to 6.50910021340577e-69.\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1526/5000\n",
      "\n",
      "Epoch 01526: LearningRateScheduler reducing learning rate to 5.889677430835424e-69.\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1527/5000\n",
      "\n",
      "Epoch 01527: LearningRateScheduler reducing learning rate to 5.329200519581819e-69.\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1528/5000\n",
      "\n",
      "Epoch 01528: LearningRateScheduler reducing learning rate to 4.822060038334197e-69.\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1529/5000\n",
      "\n",
      "Epoch 01529: LearningRateScheduler reducing learning rate to 4.3631803547007203e-69.\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1530/5000\n",
      "\n",
      "Epoch 01530: LearningRateScheduler reducing learning rate to 3.9479688465726447e-69.\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1531/5000\n",
      "\n",
      "Epoch 01531: LearningRateScheduler reducing learning rate to 3.5722699376192177e-69.\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1532/5000\n",
      "\n",
      "Epoch 01532: LearningRateScheduler reducing learning rate to 3.2323235068828696e-69.\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1533/5000\n",
      "\n",
      "Epoch 01533: LearningRateScheduler reducing learning rate to 2.9247272562247674e-69.\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1534/5000\n",
      "\n",
      "Epoch 01534: LearningRateScheduler reducing learning rate to 2.64640265898183e-69.\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1535/5000\n",
      "\n",
      "Epoch 01535: LearningRateScheduler reducing learning rate to 2.3945641490366307e-69.\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1536/5000\n",
      "\n",
      "Epoch 01536: LearningRateScheduler reducing learning rate to 2.166691241935792e-69.\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1537/5000\n",
      "\n",
      "Epoch 01537: LearningRateScheduler reducing learning rate to 1.960503309034264e-69.\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1538/5000\n",
      "\n",
      "Epoch 01538: LearningRateScheduler reducing learning rate to 1.7739367521975286e-69.\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1539/5000\n",
      "\n",
      "Epoch 01539: LearningRateScheduler reducing learning rate to 1.6051243506175167e-69.\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1540/5000\n",
      "\n",
      "Epoch 01540: LearningRateScheduler reducing learning rate to 1.4523765730394083e-69.\n",
      "11/11 [==============================] - 0s 20ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1541/5000\n",
      "\n",
      "Epoch 01541: LearningRateScheduler reducing learning rate to 1.314164668364901e-69.\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1542/5000\n",
      "\n",
      "Epoch 01542: LearningRateScheduler reducing learning rate to 1.189105365397353e-69.\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1543/5000\n",
      "\n",
      "Epoch 01543: LearningRateScheduler reducing learning rate to 1.0759470285988533e-69.\n",
      "11/11 [==============================] - 0s 25ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1544/5000\n",
      "\n",
      "Epoch 01544: LearningRateScheduler reducing learning rate to 9.735571313008547e-70.\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1545/5000\n",
      "\n",
      "Epoch 01545: LearningRateScheduler reducing learning rate to 8.809109209967659e-70.\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1546/5000\n",
      "\n",
      "Epoch 01546: LearningRateScheduler reducing learning rate to 7.970811632743975e-70.\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1547/5000\n",
      "\n",
      "Epoch 01547: LearningRateScheduler reducing learning rate to 7.212288617422886e-70.\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1548/5000\n",
      "\n",
      "Epoch 01548: LearningRateScheduler reducing learning rate to 6.525948610719101e-70.\n",
      "11/11 [==============================] - 0s 18ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1549/5000\n",
      "\n",
      "Epoch 01549: LearningRateScheduler reducing learning rate to 5.904922491158462e-70.\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1550/5000\n",
      "\n",
      "Epoch 01550: LearningRateScheduler reducing learning rate to 5.34299482060232e-70.\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1551/5000\n",
      "\n",
      "Epoch 01551: LearningRateScheduler reducing learning rate to 4.8345416380533355e-70.\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1552/5000\n",
      "\n",
      "Epoch 01552: LearningRateScheduler reducing learning rate to 4.374474173163419e-70.\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1553/5000\n",
      "\n",
      "Epoch 01553: LearningRateScheduler reducing learning rate to 3.9581879161102e-70.\n",
      "11/11 [==============================] - 0s 15ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1554/5000\n",
      "\n",
      "Epoch 01554: LearningRateScheduler reducing learning rate to 3.581516534114309e-70.\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1555/5000\n",
      "\n",
      "Epoch 01555: LearningRateScheduler reducing learning rate to 3.2406901733811084e-70.\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1556/5000\n",
      "\n",
      "Epoch 01556: LearningRateScheduler reducing learning rate to 2.932297729136686e-70.\n",
      "11/11 [==============================] - 0s 18ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1557/5000\n",
      "\n",
      "Epoch 01557: LearningRateScheduler reducing learning rate to 2.6532527061446854e-70.\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1558/5000\n",
      "\n",
      "Epoch 01558: LearningRateScheduler reducing learning rate to 2.4007623280248936e-70.\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1559/5000\n",
      "\n",
      "Epoch 01559: LearningRateScheduler reducing learning rate to 2.1722995862080563e-70.\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1560/5000\n",
      "\n",
      "Epoch 01560: LearningRateScheduler reducing learning rate to 1.9655779487850924e-70.\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1561/5000\n",
      "\n",
      "Epoch 01561: LearningRateScheduler reducing learning rate to 1.7785284761271306e-70.\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1562/5000\n",
      "\n",
      "Epoch 01562: LearningRateScheduler reducing learning rate to 1.6092791142422658e-70.\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1563/5000\n",
      "\n",
      "Epoch 01563: LearningRateScheduler reducing learning rate to 1.456135958630176e-70.\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1564/5000\n",
      "\n",
      "Epoch 01564: LearningRateScheduler reducing learning rate to 1.3175663011162531e-70.\n",
      "11/11 [==============================] - 0s 18ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1565/5000\n",
      "\n",
      "Epoch 01565: LearningRateScheduler reducing learning rate to 1.1921832899932269e-70.\n",
      "11/11 [==============================] - 0s 18ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1566/5000\n",
      "\n",
      "Epoch 01566: LearningRateScheduler reducing learning rate to 1.078732049943093e-70.\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1567/5000\n",
      "\n",
      "Epoch 01567: LearningRateScheduler reducing learning rate to 9.76077122823124e-71.\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1568/5000\n",
      "\n",
      "Epoch 01568: LearningRateScheduler reducing learning rate to 8.831911036192487e-71.\n",
      "11/11 [==============================] - 0s 25ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1569/5000\n",
      "\n",
      "Epoch 01569: LearningRateScheduler reducing learning rate to 7.991443578311751e-71.\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1570/5000\n",
      "\n",
      "Epoch 01570: LearningRateScheduler reducing learning rate to 7.230957173779696e-71.\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1571/5000\n",
      "\n",
      "Epoch 01571: LearningRateScheduler reducing learning rate to 6.5428406190514575e-71.\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1572/5000\n",
      "\n",
      "Epoch 01572: LearningRateScheduler reducing learning rate to 5.9202070123631856e-71.\n",
      "11/11 [==============================] - 0s 16ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1573/5000\n",
      "\n",
      "Epoch 01573: LearningRateScheduler reducing learning rate to 5.356824827305117e-71.\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1574/5000\n",
      "\n",
      "Epoch 01574: LearningRateScheduler reducing learning rate to 4.847055545609715e-71.\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1575/5000\n",
      "\n",
      "Epoch 01575: LearningRateScheduler reducing learning rate to 4.3857972249663985e-71.\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1576/5000\n",
      "\n",
      "Epoch 01576: LearningRateScheduler reducing learning rate to 3.968433437067895e-71.\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1577/5000\n",
      "\n",
      "Epoch 01577: LearningRateScheduler reducing learning rate to 3.5907870648440014e-71.\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1578/5000\n",
      "\n",
      "Epoch 01578: LearningRateScheduler reducing learning rate to 3.2490784964703866e-71.\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1579/5000\n",
      "\n",
      "Epoch 01579: LearningRateScheduler reducing learning rate to 2.9398877977424386e-71.\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1580/5000\n",
      "\n",
      "Epoch 01580: LearningRateScheduler reducing learning rate to 2.660120484224707e-71.\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1581/5000\n",
      "\n",
      "Epoch 01581: LearningRateScheduler reducing learning rate to 2.4069765506104637e-71.\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1582/5000\n",
      "\n",
      "Epoch 01582: LearningRateScheduler reducing learning rate to 2.177922447327423e-71.\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1583/5000\n",
      "\n",
      "Epoch 01583: LearningRateScheduler reducing learning rate to 1.9706657239223146e-71.\n",
      "11/11 [==============================] - 0s 31ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1584/5000\n",
      "\n",
      "Epoch 01584: LearningRateScheduler reducing learning rate to 1.7831320854458423e-71.\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1585/5000\n",
      "\n",
      "Epoch 01585: LearningRateScheduler reducing learning rate to 1.6134446322119012e-71.\n",
      "11/11 [==============================] - 0s 15ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1586/5000\n",
      "\n",
      "Epoch 01586: LearningRateScheduler reducing learning rate to 1.4599050751546033e-71.\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1587/5000\n",
      "\n",
      "Epoch 01587: LearningRateScheduler reducing learning rate to 1.3209767387804548e-71.\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1588/5000\n",
      "\n",
      "Epoch 01588: LearningRateScheduler reducing learning rate to 1.1952691816036757e-71.\n",
      "11/11 [==============================] - 0s 16ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1589/5000\n",
      "\n",
      "Epoch 01589: LearningRateScheduler reducing learning rate to 1.0815242801402306e-71.\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1590/5000\n",
      "\n",
      "Epoch 01590: LearningRateScheduler reducing learning rate to 9.786036371852917e-72.\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1591/5000\n",
      "\n",
      "Epoch 01591: LearningRateScheduler reducing learning rate to 8.854771883513432e-72.\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1592/5000\n",
      "\n",
      "Epoch 01592: LearningRateScheduler reducing learning rate to 8.012128928375524e-72.\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1593/5000\n",
      "\n",
      "Epoch 01593: LearningRateScheduler reducing learning rate to 7.249674052522569e-72.\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1594/5000\n",
      "\n",
      "Epoch 01594: LearningRateScheduler reducing learning rate to 6.559776351286851e-72.\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1595/5000\n",
      "\n",
      "Epoch 01595: LearningRateScheduler reducing learning rate to 5.935531096591775e-72.\n",
      "11/11 [==============================] - 0s 16ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1596/5000\n",
      "\n",
      "Epoch 01596: LearningRateScheduler reducing learning rate to 5.37069063211228e-72.\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1597/5000\n",
      "\n",
      "Epoch 01597: LearningRateScheduler reducing learning rate to 4.85960184463028e-72.\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1598/5000\n",
      "\n",
      "Epoch 01598: LearningRateScheduler reducing learning rate to 4.3971495857780745e-72.\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1599/5000\n",
      "\n",
      "Epoch 01599: LearningRateScheduler reducing learning rate to 3.9787054779133443e-72.\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1600/5000\n",
      "\n",
      "Epoch 01600: LearningRateScheduler reducing learning rate to 3.6000815917606596e-72.\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1601/5000\n",
      "\n",
      "Epoch 01601: LearningRateScheduler reducing learning rate to 3.2574885322075213e-72.\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1602/5000\n",
      "\n",
      "Epoch 01602: LearningRateScheduler reducing learning rate to 2.9474975127643345e-72.\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1603/5000\n",
      "\n",
      "Epoch 01603: LearningRateScheduler reducing learning rate to 2.667006039117108e-72.\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1604/5000\n",
      "\n",
      "Epoch 01604: LearningRateScheduler reducing learning rate to 2.4132068583210494e-72.\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1605/5000\n",
      "\n",
      "Epoch 01605: LearningRateScheduler reducing learning rate to 2.1835598628699006e-72.\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1606/5000\n",
      "\n",
      "Epoch 01606: LearningRateScheduler reducing learning rate to 1.975766668446166e-72.\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1607/5000\n",
      "\n",
      "Epoch 01607: LearningRateScheduler reducing learning rate to 1.7877476109182978e-72.\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1608/5000\n",
      "\n",
      "Epoch 01608: LearningRateScheduler reducing learning rate to 1.617620932363277e-72.\n",
      "11/11 [==============================] - 0s 16ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1609/5000\n",
      "\n",
      "Epoch 01609: LearningRateScheduler reducing learning rate to 1.4636839478005176e-72.\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1610/5000\n",
      "\n",
      "Epoch 01610: LearningRateScheduler reducing learning rate to 1.324396004148508e-72.\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1611/5000\n",
      "\n",
      "Epoch 01611: LearningRateScheduler reducing learning rate to 1.1983630608508848e-72.\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1612/5000\n",
      "\n",
      "Epoch 01612: LearningRateScheduler reducing learning rate to 1.0843237378499594e-72.\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1613/5000\n",
      "\n",
      "Epoch 01613: LearningRateScheduler reducing learning rate to 9.811366912712636e-73.\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1614/5000\n",
      "\n",
      "Epoch 01614: LearningRateScheduler reducing learning rate to 8.877691904702396e-73.\n",
      "11/11 [==============================] - 0s 21ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1615/5000\n",
      "\n",
      "Epoch 01615: LearningRateScheduler reducing learning rate to 8.032867821169702e-73.\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1616/5000\n",
      "\n",
      "Epoch 01616: LearningRateScheduler reducing learning rate to 7.268439378731379e-73.\n",
      "11/11 [==============================] - 0s 25ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1617/5000\n",
      "\n",
      "Epoch 01617: LearningRateScheduler reducing learning rate to 6.576755920602045e-73.\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1618/5000\n",
      "\n",
      "Epoch 01618: LearningRateScheduler reducing learning rate to 5.950894846250298e-73.\n",
      "11/11 [==============================] - 0s 23ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1619/5000\n",
      "\n",
      "Epoch 01619: LearningRateScheduler reducing learning rate to 5.38459232768465e-73.\n",
      "11/11 [==============================] - 0s 23ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1620/5000\n",
      "\n",
      "Epoch 01620: LearningRateScheduler reducing learning rate to 4.872180618958443e-73.\n",
      "11/11 [==============================] - 0s 15ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1621/5000\n",
      "\n",
      "Epoch 01621: LearningRateScheduler reducing learning rate to 4.408531331463226e-73.\n",
      "11/11 [==============================] - 0s 17ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1622/5000\n",
      "\n",
      "Epoch 01622: LearningRateScheduler reducing learning rate to 3.989004107291726e-73.\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1623/5000\n",
      "\n",
      "Epoch 01623: LearningRateScheduler reducing learning rate to 3.6094001769767037e-73.\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1624/5000\n",
      "\n",
      "Epoch 01624: LearningRateScheduler reducing learning rate to 3.2659203367941546e-73.\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1625/5000\n",
      "\n",
      "Epoch 01625: LearningRateScheduler reducing learning rate to 2.955126925055971e-73.\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1626/5000\n",
      "\n",
      "Epoch 01626: LearningRateScheduler reducing learning rate to 2.6739094168362047e-73.\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1627/5000\n",
      "\n",
      "Epoch 01627: LearningRateScheduler reducing learning rate to 2.4194532927920552e-73.\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1628/5000\n",
      "\n",
      "Epoch 01628: LearningRateScheduler reducing learning rate to 2.189211870508576e-73.\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1629/5000\n",
      "\n",
      "Epoch 01629: LearningRateScheduler reducing learning rate to 1.980880816444665e-73.\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1630/5000\n",
      "\n",
      "Epoch 01630: LearningRateScheduler reducing learning rate to 1.7923750833887642e-73.\n",
      "11/11 [==============================] - 0s 30ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1631/5000\n",
      "\n",
      "Epoch 01631: LearningRateScheduler reducing learning rate to 1.6218080426054862e-73.\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1632/5000\n",
      "\n",
      "Epoch 01632: LearningRateScheduler reducing learning rate to 1.4674726018210684e-73.\n",
      "11/11 [==============================] - 0s 16ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1633/5000\n",
      "\n",
      "Epoch 01633: LearningRateScheduler reducing learning rate to 1.327824120070295e-73.\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1634/5000\n",
      "\n",
      "Epoch 01634: LearningRateScheduler reducing learning rate to 1.2014649484102824e-73.\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1635/5000\n",
      "\n",
      "Epoch 01635: LearningRateScheduler reducing learning rate to 1.0871304417802734e-73.\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1636/5000\n",
      "\n",
      "Epoch 01636: LearningRateScheduler reducing learning rate to 9.836763020087604e-74.\n",
      "11/11 [==============================] - 0s 16ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1637/5000\n",
      "\n",
      "Epoch 01637: LearningRateScheduler reducing learning rate to 8.900671252927473e-74.\n",
      "11/11 [==============================] - 0s 16ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1638/5000\n",
      "\n",
      "Epoch 01638: LearningRateScheduler reducing learning rate to 8.05366039528583e-74.\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1639/5000\n",
      "\n",
      "Epoch 01639: LearningRateScheduler reducing learning rate to 7.287253277808937e-74.\n",
      "11/11 [==============================] - 0s 16ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1640/5000\n",
      "\n",
      "Epoch 01640: LearningRateScheduler reducing learning rate to 6.593779440466759e-74.\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1641/5000\n",
      "\n",
      "Epoch 01641: LearningRateScheduler reducing learning rate to 5.96629836401057e-74.\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1642/5000\n",
      "\n",
      "Epoch 01642: LearningRateScheduler reducing learning rate to 5.398530006923371e-74.\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1643/5000\n",
      "\n",
      "Epoch 01643: LearningRateScheduler reducing learning rate to 4.884791952654222e-74.\n",
      "11/11 [==============================] - 0s 22ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1644/5000\n",
      "\n",
      "Epoch 01644: LearningRateScheduler reducing learning rate to 4.4199425380825044e-74.\n",
      "11/11 [==============================] - 0s 15ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1645/5000\n",
      "\n",
      "Epoch 01645: LearningRateScheduler reducing learning rate to 3.999329394025903e-74.\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1646/5000\n",
      "\n",
      "Epoch 01646: LearningRateScheduler reducing learning rate to 3.618742882765736e-74.\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1647/5000\n",
      "\n",
      "Epoch 01647: LearningRateScheduler reducing learning rate to 3.27437396657768e-74.\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1648/5000\n",
      "\n",
      "Epoch 01648: LearningRateScheduler reducing learning rate to 2.962776085602328e-74.\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1649/5000\n",
      "\n",
      "Epoch 01649: LearningRateScheduler reducing learning rate to 2.6808306635151132e-74.\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1650/5000\n",
      "\n",
      "Epoch 01650: LearningRateScheduler reducing learning rate to 2.425715895766657e-74.\n",
      "11/11 [==============================] - 0s 20ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1651/5000\n",
      "\n",
      "Epoch 01651: LearningRateScheduler reducing learning rate to 2.194878508014299e-74.\n",
      "11/11 [==============================] - 0s 17ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1652/5000\n",
      "\n",
      "Epoch 01652: LearningRateScheduler reducing learning rate to 1.9860082020942326e-74.\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1653/5000\n",
      "\n",
      "Epoch 01653: LearningRateScheduler reducing learning rate to 1.7970145337811937e-74.\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1654/5000\n",
      "\n",
      "Epoch 01654: LearningRateScheduler reducing learning rate to 1.6260059909196783e-74.\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1655/5000\n",
      "\n",
      "Epoch 01655: LearningRateScheduler reducing learning rate to 1.471271062534772e-74.\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1656/5000\n",
      "\n",
      "Epoch 01656: LearningRateScheduler reducing learning rate to 1.3312611094549934e-74.\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1657/5000\n",
      "\n",
      "Epoch 01657: LearningRateScheduler reducing learning rate to 1.2045748650109159e-74.\n",
      "11/11 [==============================] - 0s 17ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1658/5000\n",
      "\n",
      "Epoch 01658: LearningRateScheduler reducing learning rate to 1.0899444106874978e-74.\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1659/5000\n",
      "\n",
      "Epoch 01659: LearningRateScheduler reducing learning rate to 9.862224863692066e-75.\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1660/5000\n",
      "\n",
      "Epoch 01660: LearningRateScheduler reducing learning rate to 8.923710081753224e-75.\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1661/5000\n",
      "\n",
      "Epoch 01661: LearningRateScheduler reducing learning rate to 8.074506789675095e-75.\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1662/5000\n",
      "\n",
      "Epoch 01662: LearningRateScheduler reducing learning rate to 7.30611587548327e-75.\n",
      "11/11 [==============================] - 0s 15ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1663/5000\n",
      "\n",
      "Epoch 01663: LearningRateScheduler reducing learning rate to 6.610847024643855e-75.\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1664/5000\n",
      "\n",
      "Epoch 01664: LearningRateScheduler reducing learning rate to 5.981741752809485e-75.\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1665/5000\n",
      "\n",
      "Epoch 01665: LearningRateScheduler reducing learning rate to 5.412503762970061e-75.\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1666/5000\n",
      "\n",
      "Epoch 01666: LearningRateScheduler reducing learning rate to 4.897435929995772e-75.\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1667/5000\n",
      "\n",
      "Epoch 01667: LearningRateScheduler reducing learning rate to 4.431383281893813e-75.\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1668/5000\n",
      "\n",
      "Epoch 01668: LearningRateScheduler reducing learning rate to 4.009681407116537e-75.\n",
      "11/11 [==============================] - 0s 21ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1669/5000\n",
      "\n",
      "Epoch 01669: LearningRateScheduler reducing learning rate to 3.6281097715621414e-75.\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1670/5000\n",
      "\n",
      "Epoch 01670: LearningRateScheduler reducing learning rate to 3.2828494780513418e-75.\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1671/5000\n",
      "\n",
      "Epoch 01671: LearningRateScheduler reducing learning rate to 2.9704450455206906e-75.\n",
      "11/11 [==============================] - 0s 18ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1672/5000\n",
      "\n",
      "Epoch 01672: LearningRateScheduler reducing learning rate to 2.687769825406589e-75.\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1673/5000\n",
      "\n",
      "Epoch 01673: LearningRateScheduler reducing learning rate to 2.4319947090958734e-75.\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1674/5000\n",
      "\n",
      "Epoch 01674: LearningRateScheduler reducing learning rate to 2.2005598132554375e-75.\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1675/5000\n",
      "\n",
      "Epoch 01675: LearningRateScheduler reducing learning rate to 1.9911488596597544e-75.\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1676/5000\n",
      "\n",
      "Epoch 01676: LearningRateScheduler reducing learning rate to 1.8016659930997877e-75.\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1677/5000\n",
      "\n",
      "Epoch 01677: LearningRateScheduler reducing learning rate to 1.6302148053595679e-75.\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1678/5000\n",
      "\n",
      "Epoch 01678: LearningRateScheduler reducing learning rate to 1.4750793553255541e-75.\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1679/5000\n",
      "\n",
      "Epoch 01679: LearningRateScheduler reducing learning rate to 1.33470699527093e-75.\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1680/5000\n",
      "\n",
      "Epoch 01680: LearningRateScheduler reducing learning rate to 1.2076928314354889e-75.\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1681/5000\n",
      "\n",
      "Epoch 01681: LearningRateScheduler reducing learning rate to 1.0927656633766312e-75.\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1682/5000\n",
      "\n",
      "Epoch 01682: LearningRateScheduler reducing learning rate to 9.88775261368041e-76.\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1683/5000\n",
      "\n",
      "Epoch 01683: LearningRateScheduler reducing learning rate to 8.946808545140944e-76.\n",
      "11/11 [==============================] - 0s 26ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1684/5000\n",
      "\n",
      "Epoch 01684: LearningRateScheduler reducing learning rate to 8.095407143647437e-76.\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1685/5000\n",
      "\n",
      "Epoch 01685: LearningRateScheduler reducing learning rate to 7.325027297807852e-76.\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1686/5000\n",
      "\n",
      "Epoch 01686: LearningRateScheduler reducing learning rate to 6.627958787191416e-76.\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1687/5000\n",
      "\n",
      "Epoch 01687: LearningRateScheduler reducing learning rate to 5.997225115850895e-76.\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1688/5000\n",
      "\n",
      "Epoch 01688: LearningRateScheduler reducing learning rate to 5.426513689206963e-76.\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1689/5000\n",
      "\n",
      "Epoch 01689: LearningRateScheduler reducing learning rate to 4.910112635478846e-76.\n",
      "11/11 [==============================] - 0s 16ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1690/5000\n",
      "\n",
      "Epoch 01690: LearningRateScheduler reducing learning rate to 4.4428536393524444e-76.\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1691/5000\n",
      "\n",
      "Epoch 01691: LearningRateScheduler reducing learning rate to 4.020060215743355e-76.\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1692/5000\n",
      "\n",
      "Epoch 01692: LearningRateScheduler reducing learning rate to 3.6375009059622177e-76.\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1693/5000\n",
      "\n",
      "Epoch 01693: LearningRateScheduler reducing learning rate to 3.2913469278543354e-76.\n",
      "11/11 [==============================] - 0s 16ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1694/5000\n",
      "\n",
      "Epoch 01694: LearningRateScheduler reducing learning rate to 2.9781338560603216e-76.\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1695/5000\n",
      "\n",
      "Epoch 01695: LearningRateScheduler reducing learning rate to 2.6947269488831128e-76.\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1696/5000\n",
      "\n",
      "Epoch 01696: LearningRateScheduler reducing learning rate to 2.4382897747393287e-76.\n",
      "11/11 [==============================] - 0s 18ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1697/5000\n",
      "\n",
      "Epoch 01697: LearningRateScheduler reducing learning rate to 2.2062558241985655e-76.\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1698/5000\n",
      "\n",
      "Epoch 01698: LearningRateScheduler reducing learning rate to 1.9963028234946393e-76.\n",
      "11/11 [==============================] - 0s 15ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1699/5000\n",
      "\n",
      "Epoch 01699: LearningRateScheduler reducing learning rate to 1.8063294924287955e-76.\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1700/5000\n",
      "\n",
      "Epoch 01700: LearningRateScheduler reducing learning rate to 1.6344345140514863e-76.\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1701/5000\n",
      "\n",
      "Epoch 01701: LearningRateScheduler reducing learning rate to 1.4788975056432134e-76.\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1702/5000\n",
      "\n",
      "Epoch 01702: LearningRateScheduler reducing learning rate to 1.3381618005459959e-76.\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1703/5000\n",
      "\n",
      "Epoch 01703: LearningRateScheduler reducing learning rate to 1.2108188685203963e-76.\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1704/5000\n",
      "\n",
      "Epoch 01704: LearningRateScheduler reducing learning rate to 1.0955942187012237e-76.\n",
      "11/11 [==============================] - 0s 16ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1705/5000\n",
      "\n",
      "Epoch 01705: LearningRateScheduler reducing learning rate to 9.913346440647453e-77.\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1706/5000\n",
      "\n",
      "Epoch 01706: LearningRateScheduler reducing learning rate to 8.969966797451464e-77.\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1707/5000\n",
      "\n",
      "Epoch 01707: LearningRateScheduler reducing learning rate to 8.116361596874082e-77.\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1708/5000\n",
      "\n",
      "Epoch 01708: LearningRateScheduler reducing learning rate to 7.343987671161805e-77.\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1709/5000\n",
      "\n",
      "Epoch 01709: LearningRateScheduler reducing learning rate to 6.6451148424620034e-77.\n",
      "11/11 [==============================] - 0s 23ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1710/5000\n",
      "\n",
      "Epoch 01710: LearningRateScheduler reducing learning rate to 6.012748556605787e-77.\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1711/5000\n",
      "\n",
      "Epoch 01711: LearningRateScheduler reducing learning rate to 5.440559879258653e-77.\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1712/5000\n",
      "\n",
      "Epoch 01712: LearningRateScheduler reducing learning rate to 4.922822153818319e-77.\n",
      "11/11 [==============================] - 0s 23ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1713/5000\n",
      "\n",
      "Epoch 01713: LearningRateScheduler reducing learning rate to 4.454353687111215e-77.\n",
      "11/11 [==============================] - 0s 15ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1714/5000\n",
      "\n",
      "Epoch 01714: LearningRateScheduler reducing learning rate to 4.0304658892646916e-77.\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1715/5000\n",
      "\n",
      "Epoch 01715: LearningRateScheduler reducing learning rate to 3.646916348724292e-77.\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1716/5000\n",
      "\n",
      "Epoch 01716: LearningRateScheduler reducing learning rate to 3.299866372772836e-77.\n",
      "11/11 [==============================] - 0s 16ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1717/5000\n",
      "\n",
      "Epoch 01717: LearningRateScheduler reducing learning rate to 2.9858425686033924e-77.\n",
      "11/11 [==============================] - 0s 15ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1718/5000\n",
      "\n",
      "Epoch 01718: LearningRateScheduler reducing learning rate to 2.7017020804369664e-77.\n",
      "11/11 [==============================] - 0s 24ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1719/5000\n",
      "\n",
      "Epoch 01719: LearningRateScheduler reducing learning rate to 2.444601134764979e-77.\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1720/5000\n",
      "\n",
      "Epoch 01720: LearningRateScheduler reducing learning rate to 2.211966578908533e-77.\n",
      "11/11 [==============================] - 0s 21ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1721/5000\n",
      "\n",
      "Epoch 01721: LearningRateScheduler reducing learning rate to 2.001470128041443e-77.\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1722/5000\n",
      "\n",
      "Epoch 01722: LearningRateScheduler reducing learning rate to 1.8110050629330794e-77.\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1723/5000\n",
      "\n",
      "Epoch 01723: LearningRateScheduler reducing learning rate to 1.6386651451944275e-77.\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1724/5000\n",
      "\n",
      "Epoch 01724: LearningRateScheduler reducing learning rate to 1.482725539003255e-77.\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1725/5000\n",
      "\n",
      "Epoch 01725: LearningRateScheduler reducing learning rate to 1.3416255483676894e-77.\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1726/5000\n",
      "\n",
      "Epoch 01726: LearningRateScheduler reducing learning rate to 1.2139529971561053e-77.\n",
      "11/11 [==============================] - 0s 15ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1727/5000\n",
      "\n",
      "Epoch 01727: LearningRateScheduler reducing learning rate to 1.0984300955637199e-77.\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1728/5000\n",
      "\n",
      "Epoch 01728: LearningRateScheduler reducing learning rate to 9.939006515628744e-78.\n",
      "11/11 [==============================] - 0s 16ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1729/5000\n",
      "\n",
      "Epoch 01729: LearningRateScheduler reducing learning rate to 8.993184993444142e-78.\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1730/5000\n",
      "\n",
      "Epoch 01730: LearningRateScheduler reducing learning rate to 8.137370289387783e-78.\n",
      "11/11 [==============================] - 0s 25ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1731/5000\n",
      "\n",
      "Epoch 01731: LearningRateScheduler reducing learning rate to 7.362997122252211e-78.\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1732/5000\n",
      "\n",
      "Epoch 01732: LearningRateScheduler reducing learning rate to 6.6623153051047405e-78.\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1733/5000\n",
      "\n",
      "Epoch 01733: LearningRateScheduler reducing learning rate to 6.028312178812464e-78.\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1734/5000\n",
      "\n",
      "Epoch 01734: LearningRateScheduler reducing learning rate to 5.45464242699143e-78.\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1735/5000\n",
      "\n",
      "Epoch 01735: LearningRateScheduler reducing learning rate to 4.935564569948355e-78.\n",
      "11/11 [==============================] - 0s 16ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1736/5000\n",
      "\n",
      "Epoch 01736: LearningRateScheduler reducing learning rate to 4.465883502021855e-78.\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1737/5000\n",
      "\n",
      "Epoch 01737: LearningRateScheduler reducing learning rate to 4.040898497218753e-78.\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1738/5000\n",
      "\n",
      "Epoch 01738: LearningRateScheduler reducing learning rate to 3.6563561627688263e-78.\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1739/5000\n",
      "\n",
      "Epoch 01739: LearningRateScheduler reducing learning rate to 3.308407869739632e-78.\n",
      "11/11 [==============================] - 0s 20ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1740/5000\n",
      "\n",
      "Epoch 01740: LearningRateScheduler reducing learning rate to 2.993571234665075e-78.\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1741/5000\n",
      "\n",
      "Epoch 01741: LearningRateScheduler reducing learning rate to 2.7086952666810814e-78.\n",
      "11/11 [==============================] - 0s 26ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1742/5000\n",
      "\n",
      "Epoch 01742: LearningRateScheduler reducing learning rate to 2.450928831349879e-78.\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1743/5000\n",
      "\n",
      "Epoch 01743: LearningRateScheduler reducing learning rate to 2.2176921155485294e-78.\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1744/5000\n",
      "\n",
      "Epoch 01744: LearningRateScheduler reducing learning rate to 2.0066508078316474e-78.\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1745/5000\n",
      "\n",
      "Epoch 01745: LearningRateScheduler reducing learning rate to 1.8156927358581706e-78.\n",
      "11/11 [==============================] - 0s 17ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1746/5000\n",
      "\n",
      "Epoch 01746: LearningRateScheduler reducing learning rate to 1.6429067270605638e-78.\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1747/5000\n",
      "\n",
      "Epoch 01747: LearningRateScheduler reducing learning rate to 1.4865634809873558e-78.\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1748/5000\n",
      "\n",
      "Epoch 01748: LearningRateScheduler reducing learning rate to 1.345098261883155e-78.\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1749/5000\n",
      "\n",
      "Epoch 01749: LearningRateScheduler reducing learning rate to 1.2170952382870178e-78.\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1750/5000\n",
      "\n",
      "Epoch 01750: LearningRateScheduler reducing learning rate to 1.1012733129154925e-78.\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1751/5000\n",
      "\n",
      "Epoch 01751: LearningRateScheduler reducing learning rate to 9.964733010103672e-79.\n",
      "11/11 [==============================] - 0s 22ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1752/5000\n",
      "\n",
      "Epoch 01752: LearningRateScheduler reducing learning rate to 9.016463288279698e-79.\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1753/5000\n",
      "\n",
      "Epoch 01753: LearningRateScheduler reducing learning rate to 8.158433361583066e-79.\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1754/5000\n",
      "\n",
      "Epoch 01754: LearningRateScheduler reducing learning rate to 7.382055778113297e-79.\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1755/5000\n",
      "\n",
      "Epoch 01755: LearningRateScheduler reducing learning rate to 6.67956029006551e-79.\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1756/5000\n",
      "\n",
      "Epoch 01756: LearningRateScheduler reducing learning rate to 6.043916086478435e-79.\n",
      "11/11 [==============================] - 0s 24ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1757/5000\n",
      "\n",
      "Epoch 01757: LearningRateScheduler reducing learning rate to 5.468761426515025e-79.\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1758/5000\n",
      "\n",
      "Epoch 01758: LearningRateScheduler reducing learning rate to 4.948339969022534e-79.\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1759/5000\n",
      "\n",
      "Epoch 01759: LearningRateScheduler reducing learning rate to 4.477443161134516e-79.\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1760/5000\n",
      "\n",
      "Epoch 01760: LearningRateScheduler reducing learning rate to 4.051358109323743e-79.\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1761/5000\n",
      "\n",
      "Epoch 01761: LearningRateScheduler reducing learning rate to 3.6658204111795634e-79.\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1762/5000\n",
      "\n",
      "Epoch 01762: LearningRateScheduler reducing learning rate to 3.3169714758351605e-79.\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1763/5000\n",
      "\n",
      "Epoch 01763: LearningRateScheduler reducing learning rate to 3.0013199058936296e-79.\n",
      "11/11 [==============================] - 0s 17ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1764/5000\n",
      "\n",
      "Epoch 01764: LearningRateScheduler reducing learning rate to 2.715706554348736e-79.\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1765/5000\n",
      "\n",
      "Epoch 01765: LearningRateScheduler reducing learning rate to 2.457272906780257e-79.\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1766/5000\n",
      "\n",
      "Epoch 01766: LearningRateScheduler reducing learning rate to 2.2234324723807775e-79.\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1767/5000\n",
      "\n",
      "Epoch 01767: LearningRateScheduler reducing learning rate to 2.0118448974862872e-79.\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1768/5000\n",
      "\n",
      "Epoch 01768: LearningRateScheduler reducing learning rate to 1.820392542530322e-79.\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1769/5000\n",
      "\n",
      "Epoch 01769: LearningRateScheduler reducing learning rate to 1.647159287995062e-79.\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1770/5000\n",
      "\n",
      "Epoch 01770: LearningRateScheduler reducing learning rate to 1.4904113572434098e-79.\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1771/5000\n",
      "\n",
      "Epoch 01771: LearningRateScheduler reducing learning rate to 1.3485799642996047e-79.\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1772/5000\n",
      "\n",
      "Epoch 01772: LearningRateScheduler reducing learning rate to 1.220245612911853e-79.\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1773/5000\n",
      "\n",
      "Epoch 01773: LearningRateScheduler reducing learning rate to 1.1041238897568744e-79.\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1774/5000\n",
      "\n",
      "Epoch 01774: LearningRateScheduler reducing learning rate to 9.990526095994363e-80.\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1775/5000\n",
      "\n",
      "Epoch 01775: LearningRateScheduler reducing learning rate to 9.039801837520466e-80.\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1776/5000\n",
      "\n",
      "Epoch 01776: LearningRateScheduler reducing learning rate to 8.179550954218788e-80.\n",
      "11/11 [==============================] - 0s 15ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1777/5000\n",
      "\n",
      "Epoch 01777: LearningRateScheduler reducing learning rate to 7.40116376610873e-80.\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1778/5000\n",
      "\n",
      "Epoch 01778: LearningRateScheduler reducing learning rate to 6.6968499125871596e-80.\n",
      "11/11 [==============================] - 0s 23ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1779/5000\n",
      "\n",
      "Epoch 01779: LearningRateScheduler reducing learning rate to 6.059560383879741e-80.\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1780/5000\n",
      "\n",
      "Epoch 01780: LearningRateScheduler reducing learning rate to 5.482916972182765e-80.\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1781/5000\n",
      "\n",
      "Epoch 01781: LearningRateScheduler reducing learning rate to 4.961148436415422e-80.\n",
      "11/11 [==============================] - 0s 15ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1782/5000\n",
      "\n",
      "Epoch 01782: LearningRateScheduler reducing learning rate to 4.489032741699167e-80.\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1783/5000\n",
      "\n",
      "Epoch 01783: LearningRateScheduler reducing learning rate to 4.0618447954779814e-80.\n",
      "11/11 [==============================] - 0s 15ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1784/5000\n",
      "\n",
      "Epoch 01784: LearningRateScheduler reducing learning rate to 3.675309157203118e-80.\n",
      "11/11 [==============================] - 0s 20ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1785/5000\n",
      "\n",
      "Epoch 01785: LearningRateScheduler reducing learning rate to 3.3255572482876068e-80.\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1786/5000\n",
      "\n",
      "Epoch 01786: LearningRateScheduler reducing learning rate to 3.0090886340713456e-80.\n",
      "11/11 [==============================] - 0s 21ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1787/5000\n",
      "\n",
      "Epoch 01787: LearningRateScheduler reducing learning rate to 2.722735990294407e-80.\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1788/5000\n",
      "\n",
      "Epoch 01788: LearningRateScheduler reducing learning rate to 2.4636334034515868e-80.\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1789/5000\n",
      "\n",
      "Epoch 01789: LearningRateScheduler reducing learning rate to 2.2291876877662895e-80.\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1790/5000\n",
      "\n",
      "Epoch 01790: LearningRateScheduler reducing learning rate to 2.017052431716012e-80.\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1791/5000\n",
      "\n",
      "Epoch 01791: LearningRateScheduler reducing learning rate to 1.8251045143570803e-80.\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1792/5000\n",
      "\n",
      "Epoch 01792: LearningRateScheduler reducing learning rate to 1.651422856416597e-80.\n",
      "11/11 [==============================] - 0s 18ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1793/5000\n",
      "\n",
      "Epoch 01793: LearningRateScheduler reducing learning rate to 1.4942691934855713e-80.\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1794/5000\n",
      "\n",
      "Epoch 01794: LearningRateScheduler reducing learning rate to 1.3520706788841677e-80.\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1795/5000\n",
      "\n",
      "Epoch 01795: LearningRateScheduler reducing learning rate to 1.2234041420836841e-80.\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1796/5000\n",
      "\n",
      "Epoch 01796: LearningRateScheduler reducing learning rate to 1.1069818451375054e-80.\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1797/5000\n",
      "\n",
      "Epoch 01797: LearningRateScheduler reducing learning rate to 1.00163859456688e-80.\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1798/5000\n",
      "\n",
      "Epoch 01798: LearningRateScheduler reducing learning rate to 9.063200797130682e-81.\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1799/5000\n",
      "\n",
      "Epoch 01799: LearningRateScheduler reducing learning rate to 8.200723208417224e-81.\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1800/5000\n",
      "\n",
      "Epoch 01800: LearningRateScheduler reducing learning rate to 7.420321213931853e-81.\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1801/5000\n",
      "\n",
      "Epoch 01801: LearningRateScheduler reducing learning rate to 6.714184288211594e-81.\n",
      "11/11 [==============================] - 0s 16ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1802/5000\n",
      "\n",
      "Epoch 01802: LearningRateScheduler reducing learning rate to 6.075245175562847e-81.\n",
      "11/11 [==============================] - 0s 16ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1803/5000\n",
      "\n",
      "Epoch 01803: LearningRateScheduler reducing learning rate to 5.497109158591738e-81.\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1804/5000\n",
      "\n",
      "Epoch 01804: LearningRateScheduler reducing learning rate to 4.973990057722002e-81.\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1805/5000\n",
      "\n",
      "Epoch 01805: LearningRateScheduler reducing learning rate to 4.500652321165736e-81.\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1806/5000\n",
      "\n",
      "Epoch 01806: LearningRateScheduler reducing learning rate to 4.0723586257611755e-81.\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1807/5000\n",
      "\n",
      "Epoch 01807: LearningRateScheduler reducing learning rate to 3.684822464250127e-81.\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1808/5000\n",
      "\n",
      "Epoch 01808: LearningRateScheduler reducing learning rate to 3.3341652444730057e-81.\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1809/5000\n",
      "\n",
      "Epoch 01809: LearningRateScheduler reducing learning rate to 3.016877471114206e-81.\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1810/5000\n",
      "\n",
      "Epoch 01810: LearningRateScheduler reducing learning rate to 2.729783621493849e-81.\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1811/5000\n",
      "\n",
      "Epoch 01811: LearningRateScheduler reducing learning rate to 2.470010363869359e-81.\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1812/5000\n",
      "\n",
      "Epoch 01812: LearningRateScheduler reducing learning rate to 2.2349578001655613e-81.\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1813/5000\n",
      "\n",
      "Epoch 01813: LearningRateScheduler reducing learning rate to 2.0222734453211459e-81.\n",
      "11/11 [==============================] - 0s 18ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1814/5000\n",
      "\n",
      "Epoch 01814: LearningRateScheduler reducing learning rate to 1.8298286828270806e-81.\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1815/5000\n",
      "\n",
      "Epoch 01815: LearningRateScheduler reducing learning rate to 1.6556974608174057e-81.\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1816/5000\n",
      "\n",
      "Epoch 01816: LearningRateScheduler reducing learning rate to 1.4981370154947242e-81.\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1817/5000\n",
      "\n",
      "Epoch 01817: LearningRateScheduler reducing learning rate to 1.355570428964314e-81.\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1818/5000\n",
      "\n",
      "Epoch 01818: LearningRateScheduler reducing learning rate to 1.2265708469099748e-81.\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1819/5000\n",
      "\n",
      "Epoch 01819: LearningRateScheduler reducing learning rate to 1.1098471981562082e-81.\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1820/5000\n",
      "\n",
      "Epoch 01820: LearningRateScheduler reducing learning rate to 1.004231273194113e-81.\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1821/5000\n",
      "\n",
      "Epoch 01821: LearningRateScheduler reducing learning rate to 9.086660323479308e-82.\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1822/5000\n",
      "\n",
      "Epoch 01822: LearningRateScheduler reducing learning rate to 8.221950265666627e-82.\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1823/5000\n",
      "\n",
      "Epoch 01823: LearningRateScheduler reducing learning rate to 7.439528249605906e-82.\n",
      "11/11 [==============================] - 0s 22ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1824/5000\n",
      "\n",
      "Epoch 01824: LearningRateScheduler reducing learning rate to 6.731563532779026e-82.\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1825/5000\n",
      "\n",
      "Epoch 01825: LearningRateScheduler reducing learning rate to 6.090970566344832e-82.\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1826/5000\n",
      "\n",
      "Epoch 01826: LearningRateScheduler reducing learning rate to 5.511338080584516e-82.\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1827/5000\n",
      "\n",
      "Epoch 01827: LearningRateScheduler reducing learning rate to 4.986864918759241e-82.\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1828/5000\n",
      "\n",
      "Epoch 01828: LearningRateScheduler reducing learning rate to 4.5123019771842427e-82.\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1829/5000\n",
      "\n",
      "Epoch 01829: LearningRateScheduler reducing learning rate to 4.082899670433969e-82.\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1830/5000\n",
      "\n",
      "Epoch 01830: LearningRateScheduler reducing learning rate to 3.6943603958953636e-82.\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1831/5000\n",
      "\n",
      "Epoch 01831: LearningRateScheduler reducing learning rate to 3.342795521916285e-82.\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1832/5000\n",
      "\n",
      "Epoch 01832: LearningRateScheduler reducing learning rate to 3.0246864690728305e-82.\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1833/5000\n",
      "\n",
      "Epoch 01833: LearningRateScheduler reducing learning rate to 2.736849495044179e-82.\n",
      "11/11 [==============================] - 0s 15ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1834/5000\n",
      "\n",
      "Epoch 01834: LearningRateScheduler reducing learning rate to 2.4764038306488087e-82.\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1835/5000\n",
      "\n",
      "Epoch 01835: LearningRateScheduler reducing learning rate to 2.24074284813864e-82.\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1836/5000\n",
      "\n",
      "Epoch 01836: LearningRateScheduler reducing learning rate to 2.027507973192321e-82.\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1837/5000\n",
      "\n",
      "Epoch 01837: LearningRateScheduler reducing learning rate to 1.8345650795106195e-82.\n",
      "11/11 [==============================] - 0s 16ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1838/5000\n",
      "\n",
      "Epoch 01838: LearningRateScheduler reducing learning rate to 1.6599831297633335e-82.\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1839/5000\n",
      "\n",
      "Epoch 01839: LearningRateScheduler reducing learning rate to 1.5020148491183143e-82.\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1840/5000\n",
      "\n",
      "Epoch 01840: LearningRateScheduler reducing learning rate to 1.3590792379278947e-82.\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1841/5000\n",
      "\n",
      "Epoch 01841: LearningRateScheduler reducing learning rate to 1.2297457485529628e-82.\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1842/5000\n",
      "\n",
      "Epoch 01842: LearningRateScheduler reducing learning rate to 1.112719967961336e-82.\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1843/5000\n",
      "\n",
      "Epoch 01843: LearningRateScheduler reducing learning rate to 1.0068306628071967e-82.\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1844/5000\n",
      "\n",
      "Epoch 01844: LearningRateScheduler reducing learning rate to 9.110180573339028e-83.\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1845/5000\n",
      "\n",
      "Epoch 01845: LearningRateScheduler reducing learning rate to 8.24323226782149e-83.\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1846/5000\n",
      "\n",
      "Epoch 01846: LearningRateScheduler reducing learning rate to 7.458785001486348e-83.\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1847/5000\n",
      "\n",
      "Epoch 01847: LearningRateScheduler reducing learning rate to 6.748987762430095e-83.\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1848/5000\n",
      "\n",
      "Epoch 01848: LearningRateScheduler reducing learning rate to 6.106736661313569e-83.\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1849/5000\n",
      "\n",
      "Epoch 01849: LearningRateScheduler reducing learning rate to 5.5256038332485376e-83.\n",
      "11/11 [==============================] - 0s 20ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1850/5000\n",
      "\n",
      "Epoch 01850: LearningRateScheduler reducing learning rate to 4.999773105566236e-83.\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1851/5000\n",
      "\n",
      "Epoch 01851: LearningRateScheduler reducing learning rate to 4.52398178760621e-83.\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1852/5000\n",
      "\n",
      "Epoch 01852: LearningRateScheduler reducing learning rate to 4.093467999939215e-83.\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1853/5000\n",
      "\n",
      "Epoch 01853: LearningRateScheduler reducing learning rate to 3.7039230158778436e-83.\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1854/5000\n",
      "\n",
      "Epoch 01854: LearningRateScheduler reducing learning rate to 3.351448138290892e-83.\n",
      "11/11 [==============================] - 0s 25ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1855/5000\n",
      "\n",
      "Epoch 01855: LearningRateScheduler reducing learning rate to 3.032515680132571e-83.\n",
      "11/11 [==============================] - 0s 15ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1856/5000\n",
      "\n",
      "Epoch 01856: LearningRateScheduler reducing learning rate to 2.743933658164733e-83.\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1857/5000\n",
      "\n",
      "Epoch 01857: LearningRateScheduler reducing learning rate to 2.4828138465156857e-83.\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1858/5000\n",
      "\n",
      "Epoch 01858: LearningRateScheduler reducing learning rate to 2.246542870345195e-83.\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1859/5000\n",
      "\n",
      "Epoch 01859: LearningRateScheduler reducing learning rate to 2.0327560503102516e-83.\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1860/5000\n",
      "\n",
      "Epoch 01860: LearningRateScheduler reducing learning rate to 1.8393137360597134e-83.\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1861/5000\n",
      "\n",
      "Epoch 01861: LearningRateScheduler reducing learning rate to 1.664279891894355e-83.\n",
      "11/11 [==============================] - 0s 25ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1862/5000\n",
      "\n",
      "Epoch 01862: LearningRateScheduler reducing learning rate to 1.5059027202708199e-83.\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1863/5000\n",
      "\n",
      "Epoch 01863: LearningRateScheduler reducing learning rate to 1.3625971292231845e-83.\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1864/5000\n",
      "\n",
      "Epoch 01864: LearningRateScheduler reducing learning rate to 1.232928868229524e-83.\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1865/5000\n",
      "\n",
      "Epoch 01865: LearningRateScheduler reducing learning rate to 1.1156001737508065e-83.\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1866/5000\n",
      "\n",
      "Epoch 01866: LearningRateScheduler reducing learning rate to 1.0094367807771534e-83.\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1867/5000\n",
      "\n",
      "Epoch 01867: LearningRateScheduler reducing learning rate to 9.133761703889096e-84.\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1868/5000\n",
      "\n",
      "Epoch 01868: LearningRateScheduler reducing learning rate to 8.264569357102784e-84.\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1869/5000\n",
      "\n",
      "Epoch 01869: LearningRateScheduler reducing learning rate to 7.478091598260036e-84.\n",
      "11/11 [==============================] - 0s 26ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1870/5000\n",
      "\n",
      "Epoch 01870: LearningRateScheduler reducing learning rate to 6.766457093606052e-84.\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1871/5000\n",
      "\n",
      "Epoch 01871: LearningRateScheduler reducing learning rate to 6.122543565829638e-84.\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1872/5000\n",
      "\n",
      "Epoch 01872: LearningRateScheduler reducing learning rate to 5.539906511917841e-84.\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1873/5000\n",
      "\n",
      "Epoch 01873: LearningRateScheduler reducing learning rate to 5.0127147044043665e-84.\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1874/5000\n",
      "\n",
      "Epoch 01874: LearningRateScheduler reducing learning rate to 4.535691830484161e-84.\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1875/5000\n",
      "\n",
      "Epoch 01875: LearningRateScheduler reducing learning rate to 4.104063684902107e-84.\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1876/5000\n",
      "\n",
      "Epoch 01876: LearningRateScheduler reducing learning rate to 3.7135103881019893e-84.\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1877/5000\n",
      "\n",
      "Epoch 01877: LearningRateScheduler reducing learning rate to 3.3601231514198418e-84.\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1878/5000\n",
      "\n",
      "Epoch 01878: LearningRateScheduler reducing learning rate to 3.0403651566135986e-84.\n",
      "11/11 [==============================] - 0s 17ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1879/5000\n",
      "\n",
      "Epoch 01879: LearningRateScheduler reducing learning rate to 2.75103615819676e-84.\n",
      "11/11 [==============================] - 0s 16ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1880/5000\n",
      "\n",
      "Epoch 01880: LearningRateScheduler reducing learning rate to 2.4892404543063363e-84.\n",
      "11/11 [==============================] - 0s 17ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1881/5000\n",
      "\n",
      "Epoch 01881: LearningRateScheduler reducing learning rate to 2.252357905545217e-84.\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1882/5000\n",
      "\n",
      "Epoch 01882: LearningRateScheduler reducing learning rate to 2.0380177117463697e-84.\n",
      "11/11 [==============================] - 0s 15ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1883/5000\n",
      "\n",
      "Epoch 01883: LearningRateScheduler reducing learning rate to 1.84407468420815e-84.\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1884/5000\n",
      "\n",
      "Epoch 01884: LearningRateScheduler reducing learning rate to 1.6685877759243893e-84.\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1885/5000\n",
      "\n",
      "Epoch 01885: LearningRateScheduler reducing learning rate to 1.5098006549337974e-84.\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1886/5000\n",
      "\n",
      "Epoch 01886: LearningRateScheduler reducing learning rate to 1.3661241263593058e-84.\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1887/5000\n",
      "\n",
      "Epoch 01887: LearningRateScheduler reducing learning rate to 1.236120227211557e-84.\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1888/5000\n",
      "\n",
      "Epoch 01888: LearningRateScheduler reducing learning rate to 1.1184878347721355e-84.\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1889/5000\n",
      "\n",
      "Epoch 01889: LearningRateScheduler reducing learning rate to 1.0120496445198557e-84.\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1890/5000\n",
      "\n",
      "Epoch 01890: LearningRateScheduler reducing learning rate to 9.157403872715621e-85.\n",
      "11/11 [==============================] - 0s 16ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1891/5000\n",
      "\n",
      "Epoch 01891: LearningRateScheduler reducing learning rate to 8.285961676100546e-85.\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1892/5000\n",
      "\n",
      "Epoch 01892: LearningRateScheduler reducing learning rate to 7.497448168947561e-85.\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1893/5000\n",
      "\n",
      "Epoch 01893: LearningRateScheduler reducing learning rate to 6.783971643048981e-85.\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1894/5000\n",
      "\n",
      "Epoch 01894: LearningRateScheduler reducing learning rate to 6.138391385525642e-85.\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1895/5000\n",
      "\n",
      "Epoch 01895: LearningRateScheduler reducing learning rate to 5.554246212173231e-85.\n",
      "11/11 [==============================] - 0s 22ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1896/5000\n",
      "\n",
      "Epoch 01896: LearningRateScheduler reducing learning rate to 5.0256898017588626e-85.\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1897/5000\n",
      "\n",
      "Epoch 01897: LearningRateScheduler reducing learning rate to 4.547432184073039e-85.\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1898/5000\n",
      "\n",
      "Epoch 01898: LearningRateScheduler reducing learning rate to 4.1146867961302964e-85.\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1899/5000\n",
      "\n",
      "Epoch 01899: LearningRateScheduler reducing learning rate to 3.723122576637214e-85.\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1900/5000\n",
      "\n",
      "Epoch 01900: LearningRateScheduler reducing learning rate to 3.368820619275824e-85.\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1901/5000\n",
      "\n",
      "Epoch 01901: LearningRateScheduler reducing learning rate to 3.048234950971857e-85.\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1902/5000\n",
      "\n",
      "Epoch 01902: LearningRateScheduler reducing learning rate to 2.758157042604282e-85.\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1903/5000\n",
      "\n",
      "Epoch 01903: LearningRateScheduler reducing learning rate to 2.4956836969677707e-85.\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1904/5000\n",
      "\n",
      "Epoch 01904: LearningRateScheduler reducing learning rate to 2.2581879925987688e-85.\n",
      "11/11 [==============================] - 0s 15ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1905/5000\n",
      "\n",
      "Epoch 01905: LearningRateScheduler reducing learning rate to 2.043292992662888e-85.\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1906/5000\n",
      "\n",
      "Epoch 01906: LearningRateScheduler reducing learning rate to 1.8488479557720674e-85.\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1907/5000\n",
      "\n",
      "Epoch 01907: LearningRateScheduler reducing learning rate to 1.672906810641821e-85.\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1908/5000\n",
      "\n",
      "Epoch 01908: LearningRateScheduler reducing learning rate to 1.513708679155926e-85.\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1909/5000\n",
      "\n",
      "Epoch 01909: LearningRateScheduler reducing learning rate to 1.3696602529060786e-85.\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1910/5000\n",
      "\n",
      "Epoch 01910: LearningRateScheduler reducing learning rate to 1.2393198468260227e-85.\n",
      "11/11 [==============================] - 0s 18ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1911/5000\n",
      "\n",
      "Epoch 01911: LearningRateScheduler reducing learning rate to 1.1213829703227857e-85.\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1912/5000\n",
      "\n",
      "Epoch 01912: LearningRateScheduler reducing learning rate to 1.0146692714963413e-85.\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1913/5000\n",
      "\n",
      "Epoch 01913: LearningRateScheduler reducing learning rate to 9.181107237811827e-86.\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1914/5000\n",
      "\n",
      "Epoch 01914: LearningRateScheduler reducing learning rate to 8.307409367772962e-86.\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1915/5000\n",
      "\n",
      "Epoch 01915: LearningRateScheduler reducing learning rate to 7.516854842903473e-86.\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1916/5000\n",
      "\n",
      "Epoch 01916: LearningRateScheduler reducing learning rate to 6.801531527803915e-86.\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1917/5000\n",
      "\n",
      "Epoch 01917: LearningRateScheduler reducing learning rate to 6.15428022630813e-86.\n",
      "11/11 [==============================] - 0s 32ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1918/5000\n",
      "\n",
      "Epoch 01918: LearningRateScheduler reducing learning rate to 5.568623029842441e-86.\n",
      "11/11 [==============================] - 0s 17ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1919/5000\n",
      "\n",
      "Epoch 01919: LearningRateScheduler reducing learning rate to 5.038698484338246e-86.\n",
      "11/11 [==============================] - 0s 15ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1920/5000\n",
      "\n",
      "Epoch 01920: LearningRateScheduler reducing learning rate to 4.559202926830347e-86.\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1921/5000\n",
      "\n",
      "Epoch 01921: LearningRateScheduler reducing learning rate to 4.125337404615185e-86.\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1922/5000\n",
      "\n",
      "Epoch 01922: LearningRateScheduler reducing learning rate to 3.7327596457190854e-86.\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1923/5000\n",
      "\n",
      "Epoch 01923: LearningRateScheduler reducing learning rate to 3.3775405999813e-86.\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1924/5000\n",
      "\n",
      "Epoch 01924: LearningRateScheduler reducing learning rate to 3.056125115798722e-86.\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1925/5000\n",
      "\n",
      "Epoch 01925: LearningRateScheduler reducing learning rate to 2.7652963589741797e-86.\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1926/5000\n",
      "\n",
      "Epoch 01926: LearningRateScheduler reducing learning rate to 2.502143617558451e-86.\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1927/5000\n",
      "\n",
      "Epoch 01927: LearningRateScheduler reducing learning rate to 2.2640331704666927e-86.\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1928/5000\n",
      "\n",
      "Epoch 01928: LearningRateScheduler reducing learning rate to 2.0485819283128612e-86.\n",
      "11/11 [==============================] - 0s 23ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1929/5000\n",
      "\n",
      "Epoch 01929: LearningRateScheduler reducing learning rate to 1.8536335826497472e-86.\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1930/5000\n",
      "\n",
      "Epoch 01930: LearningRateScheduler reducing learning rate to 1.677237024909552e-86.\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1931/5000\n",
      "\n",
      "Epoch 01931: LearningRateScheduler reducing learning rate to 1.5176268190534823e-86.\n",
      "11/11 [==============================] - 0s 27ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1932/5000\n",
      "\n",
      "Epoch 01932: LearningRateScheduler reducing learning rate to 1.3732055324944481e-86.\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1933/5000\n",
      "\n",
      "Epoch 01933: LearningRateScheduler reducing learning rate to 1.2425277484549784e-86.\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1934/5000\n",
      "\n",
      "Epoch 01934: LearningRateScheduler reducing learning rate to 1.1242855997500433e-86.\n",
      "11/11 [==============================] - 0s 17ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1935/5000\n",
      "\n",
      "Epoch 01935: LearningRateScheduler reducing learning rate to 1.0172956792128453e-86.\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1936/5000\n",
      "\n",
      "Epoch 01936: LearningRateScheduler reducing learning rate to 9.20487195758094e-87.\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1937/5000\n",
      "\n",
      "Epoch 01937: LearningRateScheduler reducing learning rate to 8.328912575448957e-87.\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1938/5000\n",
      "\n",
      "Epoch 01938: LearningRateScheduler reducing learning rate to 7.536311749816511e-87.\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1939/5000\n",
      "\n",
      "Epoch 01939: LearningRateScheduler reducing learning rate to 6.819136865218075e-87.\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1940/5000\n",
      "\n",
      "Epoch 01940: LearningRateScheduler reducing learning rate to 6.170210194357786e-87.\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1941/5000\n",
      "\n",
      "Epoch 01941: LearningRateScheduler reducing learning rate to 5.5830370610018865e-87.\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1942/5000\n",
      "\n",
      "Epoch 01942: LearningRateScheduler reducing learning rate to 5.0517408390759045e-87.\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1943/5000\n",
      "\n",
      "Epoch 01943: LearningRateScheduler reducing learning rate to 4.57100413741628e-87.\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1944/5000\n",
      "\n",
      "Epoch 01944: LearningRateScheduler reducing learning rate to 4.136015581531458e-87.\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1945/5000\n",
      "\n",
      "Epoch 01945: LearningRateScheduler reducing learning rate to 3.742421659749444e-87.\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1946/5000\n",
      "\n",
      "Epoch 01946: LearningRateScheduler reducing learning rate to 3.3862831518095563e-87.\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1947/5000\n",
      "\n",
      "Epoch 01947: LearningRateScheduler reducing learning rate to 3.064035703821961e-87.\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1948/5000\n",
      "\n",
      "Epoch 01948: LearningRateScheduler reducing learning rate to 2.7724541550162724e-87.\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1949/5000\n",
      "\n",
      "Epoch 01949: LearningRateScheduler reducing learning rate to 2.5086202592480066e-87.\n",
      "11/11 [==============================] - 0s 17ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1950/5000\n",
      "\n",
      "Epoch 01950: LearningRateScheduler reducing learning rate to 2.269893478210679e-87.\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1951/5000\n",
      "\n",
      "Epoch 01951: LearningRateScheduler reducing learning rate to 2.053884554040826e-87.\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1952/5000\n",
      "\n",
      "Epoch 01952: LearningRateScheduler reducing learning rate to 1.8584315968221967e-87.\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1953/5000\n",
      "\n",
      "Epoch 01953: LearningRateScheduler reducing learning rate to 1.6815784476650517e-87.\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1954/5000\n",
      "\n",
      "Epoch 01954: LearningRateScheduler reducing learning rate to 1.5215551008101708e-87.\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1955/5000\n",
      "\n",
      "Epoch 01955: LearningRateScheduler reducing learning rate to 1.376759988816527e-87.\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1956/5000\n",
      "\n",
      "Epoch 01956: LearningRateScheduler reducing learning rate to 1.24574395353597e-87.\n",
      "11/11 [==============================] - 0s 17ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1957/5000\n",
      "\n",
      "Epoch 01957: LearningRateScheduler reducing learning rate to 1.1271957424513699e-87.\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1958/5000\n",
      "\n",
      "Epoch 01958: LearningRateScheduler reducing learning rate to 1.01992888522083e-87.\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1959/5000\n",
      "\n",
      "Epoch 01959: LearningRateScheduler reducing learning rate to 9.228698190835155e-88.\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1960/5000\n",
      "\n",
      "Epoch 01960: LearningRateScheduler reducing learning rate to 8.35047144282846e-88.\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1961/5000\n",
      "\n",
      "Epoch 01961: LearningRateScheduler reducing learning rate to 7.555819019711961e-88.\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1962/5000\n",
      "\n",
      "Epoch 01962: LearningRateScheduler reducing learning rate to 6.83678777294301e-88.\n",
      "11/11 [==============================] - 0s 23ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1963/5000\n",
      "\n",
      "Epoch 01963: LearningRateScheduler reducing learning rate to 6.186181396129607e-88.\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1964/5000\n",
      "\n",
      "Epoch 01964: LearningRateScheduler reducing learning rate to 5.597488401976032e-88.\n",
      "11/11 [==============================] - 0s 23ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1965/5000\n",
      "\n",
      "Epoch 01965: LearningRateScheduler reducing learning rate to 5.064816953130252e-88.\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1966/5000\n",
      "\n",
      "Epoch 01966: LearningRateScheduler reducing learning rate to 4.582835894695159e-88.\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1967/5000\n",
      "\n",
      "Epoch 01967: LearningRateScheduler reducing learning rate to 4.14672139823839e-88.\n",
      "11/11 [==============================] - 0s 17ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1968/5000\n",
      "\n",
      "Epoch 01968: LearningRateScheduler reducing learning rate to 3.7521086832965096e-88.\n",
      "11/11 [==============================] - 0s 26ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1969/5000\n",
      "\n",
      "Epoch 01969: LearningRateScheduler reducing learning rate to 3.3950483331843373e-88.\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1970/5000\n",
      "\n",
      "Epoch 01970: LearningRateScheduler reducing learning rate to 3.0719667679058213e-88.\n",
      "11/11 [==============================] - 0s 18ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1971/5000\n",
      "\n",
      "Epoch 01971: LearningRateScheduler reducing learning rate to 2.7796304785641913e-88.\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1972/5000\n",
      "\n",
      "Epoch 01972: LearningRateScheduler reducing learning rate to 2.515113665318024e-88.\n",
      "11/11 [==============================] - 0s 24ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1973/5000\n",
      "\n",
      "Epoch 01973: LearningRateScheduler reducing learning rate to 2.2757689549933323e-88.\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1974/5000\n",
      "\n",
      "Epoch 01974: LearningRateScheduler reducing learning rate to 2.0592009052825727e-88.\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1975/5000\n",
      "\n",
      "Epoch 01975: LearningRateScheduler reducing learning rate to 1.863242030353204e-88.\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1976/5000\n",
      "\n",
      "Epoch 01976: LearningRateScheduler reducing learning rate to 1.6859311079208817e-88.\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1977/5000\n",
      "\n",
      "Epoch 01977: LearningRateScheduler reducing learning rate to 1.525493550677601e-88.\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1978/5000\n",
      "\n",
      "Epoch 01978: LearningRateScheduler reducing learning rate to 1.3803236456256363e-88.\n",
      "11/11 [==============================] - 0s 16ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1979/5000\n",
      "\n",
      "Epoch 01979: LearningRateScheduler reducing learning rate to 1.2489684835618907e-88.\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1980/5000\n",
      "\n",
      "Epoch 01980: LearningRateScheduler reducing learning rate to 1.1301134178744354e-88.\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1981/5000\n",
      "\n",
      "Epoch 01981: LearningRateScheduler reducing learning rate to 1.0225689071173034e-88.\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1982/5000\n",
      "\n",
      "Epoch 01982: LearningRateScheduler reducing learning rate to 9.252586096798527e-89.\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1983/5000\n",
      "\n",
      "Epoch 01983: LearningRateScheduler reducing learning rate to 8.372086113982644e-89.\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1984/5000\n",
      "\n",
      "Epoch 01984: LearningRateScheduler reducing learning rate to 7.57537678295081e-89.\n",
      "11/11 [==============================] - 0s 22ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1985/5000\n",
      "\n",
      "Epoch 01985: LearningRateScheduler reducing learning rate to 6.854484368934804e-89.\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1986/5000\n",
      "\n",
      "Epoch 01986: LearningRateScheduler reducing learning rate to 6.202193938354846e-89.\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1987/5000\n",
      "\n",
      "Epoch 01987: LearningRateScheduler reducing learning rate to 5.61197714933915e-89.\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1988/5000\n",
      "\n",
      "Epoch 01988: LearningRateScheduler reducing learning rate to 5.07792691388487e-89.\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1989/5000\n",
      "\n",
      "Epoch 01989: LearningRateScheduler reducing learning rate to 4.594698277734921e-89.\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1990/5000\n",
      "\n",
      "Epoch 01990: LearningRateScheduler reducing learning rate to 4.1574549262799595e-89.\n",
      "11/11 [==============================] - 0s 17ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1991/5000\n",
      "\n",
      "Epoch 01991: LearningRateScheduler reducing learning rate to 3.761820781096061e-89.\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1992/5000\n",
      "\n",
      "Epoch 01992: LearningRateScheduler reducing learning rate to 3.403836202680899e-89.\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1993/5000\n",
      "\n",
      "Epoch 01993: LearningRateScheduler reducing learning rate to 3.0799183610511275e-89.\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1994/5000\n",
      "\n",
      "Epoch 01994: LearningRateScheduler reducing learning rate to 2.7868253775750625e-89.\n",
      "11/11 [==============================] - 0s 15ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1995/5000\n",
      "\n",
      "Epoch 01995: LearningRateScheduler reducing learning rate to 2.5216238791621218e-89.\n",
      "11/11 [==============================] - 0s 18ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1996/5000\n",
      "\n",
      "Epoch 01996: LearningRateScheduler reducing learning rate to 2.2816596400788877e-89.\n",
      "11/11 [==============================] - 0s 22ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1997/5000\n",
      "\n",
      "Epoch 01997: LearningRateScheduler reducing learning rate to 2.0645310175657908e-89.\n",
      "11/11 [==============================] - 0s 25ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1998/5000\n",
      "\n",
      "Epoch 01998: LearningRateScheduler reducing learning rate to 1.868064915389393e-89.\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 1999/5000\n",
      "\n",
      "Epoch 01999: LearningRateScheduler reducing learning rate to 1.6902950347645114e-89.\n",
      "11/11 [==============================] - 0s 39ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2000/5000\n",
      "\n",
      "Epoch 02000: LearningRateScheduler reducing learning rate to 1.5294421949753315e-89.\n",
      "11/11 [==============================] - 0s 15ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2001/5000\n",
      "\n",
      "Epoch 02001: LearningRateScheduler reducing learning rate to 1.3838965267367377e-89.\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2002/5000\n",
      "\n",
      "Epoch 02002: LearningRateScheduler reducing learning rate to 1.2522013600813735e-89.\n",
      "11/11 [==============================] - 0s 21ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2003/5000\n",
      "\n",
      "Epoch 02003: LearningRateScheduler reducing learning rate to 1.1330386455171533e-89.\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2004/5000\n",
      "\n",
      "Epoch 02004: LearningRateScheduler reducing learning rate to 1.0252157625447077e-89.\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2005/5000\n",
      "\n",
      "Epoch 02005: LearningRateScheduler reducing learning rate to 9.276535835107259e-90.\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2006/5000\n",
      "\n",
      "Epoch 02006: LearningRateScheduler reducing learning rate to 8.393756733356556e-90.\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2007/5000\n",
      "\n",
      "Epoch 02007: LearningRateScheduler reducing learning rate to 7.594985170232123e-90.\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2008/5000\n",
      "\n",
      "Epoch 02008: LearningRateScheduler reducing learning rate to 6.872226771454275e-90.\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2009/5000\n",
      "\n",
      "Epoch 02009: LearningRateScheduler reducing learning rate to 6.21824792804032e-90.\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2010/5000\n",
      "\n",
      "Epoch 02010: LearningRateScheduler reducing learning rate to 5.626503399915491e-90.\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2011/5000\n",
      "\n",
      "Epoch 02011: LearningRateScheduler reducing learning rate to 5.09107080895011e-90.\n",
      "11/11 [==============================] - 0s 15ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2012/5000\n",
      "\n",
      "Epoch 02012: LearningRateScheduler reducing learning rate to 4.606591365808557e-90.\n",
      "11/11 [==============================] - 1s 93ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2013/5000\n",
      "\n",
      "Epoch 02013: LearningRateScheduler reducing learning rate to 4.168216237384983e-90.\n",
      "11/11 [==============================] - 0s 24ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2014/5000\n",
      "\n",
      "Epoch 02014: LearningRateScheduler reducing learning rate to 3.7715580180510115e-90.\n",
      "11/11 [==============================] - 0s 17ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2015/5000\n",
      "\n",
      "Epoch 02015: LearningRateScheduler reducing learning rate to 3.4126468190261173e-90.\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2016/5000\n",
      "\n",
      "Epoch 02016: LearningRateScheduler reducing learning rate to 3.0878905363962404e-90.\n",
      "11/11 [==============================] - 0s 18ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2017/5000\n",
      "\n",
      "Epoch 02017: LearningRateScheduler reducing learning rate to 2.794038900130385e-90.\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2018/5000\n",
      "\n",
      "Epoch 02018: LearningRateScheduler reducing learning rate to 2.5281509442860242e-90.\n",
      "11/11 [==============================] - 1s 66ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2019/5000\n",
      "\n",
      "Epoch 02019: LearningRateScheduler reducing learning rate to 2.287565572832952e-90.\n",
      "11/11 [==============================] - 0s 24ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2020/5000\n",
      "\n",
      "Epoch 02020: LearningRateScheduler reducing learning rate to 2.0698749265101312e-90.\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2021/5000\n",
      "\n",
      "Epoch 02021: LearningRateScheduler reducing learning rate to 1.8729002841608094e-90.\n",
      "11/11 [==============================] - 0s 15ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2022/5000\n",
      "\n",
      "Epoch 02022: LearningRateScheduler reducing learning rate to 1.6946702573588431e-90.\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2023/5000\n",
      "\n",
      "Epoch 02023: LearningRateScheduler reducing learning rate to 1.5334010600909196e-90.\n",
      "11/11 [==============================] - 0s 15ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2024/5000\n",
      "\n",
      "Epoch 02024: LearningRateScheduler reducing learning rate to 1.3874786560262786e-90.\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2025/5000\n",
      "\n",
      "Epoch 02025: LearningRateScheduler reducing learning rate to 1.2554426046988285e-90.\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2026/5000\n",
      "\n",
      "Epoch 02026: LearningRateScheduler reducing learning rate to 1.1359714449280343e-90.\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2027/5000\n",
      "\n",
      "Epoch 02027: LearningRateScheduler reducing learning rate to 1.0278694691912374e-90.\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2028/5000\n",
      "\n",
      "Epoch 02028: LearningRateScheduler reducing learning rate to 9.300547565809968e-91.\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2029/5000\n",
      "\n",
      "Epoch 02029: LearningRateScheduler reducing learning rate to 8.415483445768169e-91.\n",
      "11/11 [==============================] - 0s 17ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2030/5000\n",
      "\n",
      "Epoch 02030: LearningRateScheduler reducing learning rate to 7.614644312593273e-91.\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2031/5000\n",
      "\n",
      "Epoch 02031: LearningRateScheduler reducing learning rate to 6.890015099069141e-91.\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2032/5000\n",
      "\n",
      "Epoch 02032: LearningRateScheduler reducing learning rate to 6.234343472470356e-91.\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2033/5000\n",
      "\n",
      "Epoch 02033: LearningRateScheduler reducing learning rate to 5.641067250779447e-91.\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2034/5000\n",
      "\n",
      "Epoch 02034: LearningRateScheduler reducing learning rate to 5.104248726162512e-91.\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2035/5000\n",
      "\n",
      "Epoch 02035: LearningRateScheduler reducing learning rate to 4.61851523839425e-91.\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2036/5000\n",
      "\n",
      "Epoch 02036: LearningRateScheduler reducing learning rate to 4.1790054034684106e-91.\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2037/5000\n",
      "\n",
      "Epoch 02037: LearningRateScheduler reducing learning rate to 3.7813204592325947e-91.\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2038/5000\n",
      "\n",
      "Epoch 02038: LearningRateScheduler reducing learning rate to 3.421480241098589e-91.\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2039/5000\n",
      "\n",
      "Epoch 02039: LearningRateScheduler reducing learning rate to 3.0958833472167173e-91.\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2040/5000\n",
      "\n",
      "Epoch 02040: LearningRateScheduler reducing learning rate to 2.801271094436115e-91.\n",
      "11/11 [==============================] - 0s 20ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2041/5000\n",
      "\n",
      "Epoch 02041: LearningRateScheduler reducing learning rate to 2.534694904308355e-91.\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2042/5000\n",
      "\n",
      "Epoch 02042: LearningRateScheduler reducing learning rate to 2.2934867927232233e-91.\n",
      "11/11 [==============================] - 0s 15ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2043/5000\n",
      "\n",
      "Epoch 02043: LearningRateScheduler reducing learning rate to 2.0752326678272675e-91.\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2044/5000\n",
      "\n",
      "Epoch 02044: LearningRateScheduler reducing learning rate to 1.8777481689807117e-91.\n",
      "11/11 [==============================] - 0s 15ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2045/5000\n",
      "\n",
      "Epoch 02045: LearningRateScheduler reducing learning rate to 1.6990568049422674e-91.\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2046/5000\n",
      "\n",
      "Epoch 02046: LearningRateScheduler reducing learning rate to 1.5373701724803968e-91.\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2047/5000\n",
      "\n",
      "Epoch 02047: LearningRateScheduler reducing learning rate to 1.3910700574326287e-91.\n",
      "11/11 [==============================] - 0s 17ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2048/5000\n",
      "\n",
      "Epoch 02048: LearningRateScheduler reducing learning rate to 1.2586922390744807e-91.\n",
      "11/11 [==============================] - 0s 18ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2049/5000\n",
      "\n",
      "Epoch 02049: LearningRateScheduler reducing learning rate to 1.1389118357060604e-91.\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2050/5000\n",
      "\n",
      "Epoch 02050: LearningRateScheduler reducing learning rate to 1.0305300447908725e-91.\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2051/5000\n",
      "\n",
      "Epoch 02051: LearningRateScheduler reducing learning rate to 9.324621449370602e-92.\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2052/5000\n",
      "\n",
      "Epoch 02052: LearningRateScheduler reducing learning rate to 8.437266396411031e-92.\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2053/5000\n",
      "\n",
      "Epoch 02053: LearningRateScheduler reducing learning rate to 7.634354341410166e-92.\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2054/5000\n",
      "\n",
      "Epoch 02054: LearningRateScheduler reducing learning rate to 6.907849470653231e-92.\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2055/5000\n",
      "\n",
      "Epoch 02055: LearningRateScheduler reducing learning rate to 6.250480679206976e-92.\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2056/5000\n",
      "\n",
      "Epoch 02056: LearningRateScheduler reducing learning rate to 5.655668799257323e-92.\n",
      "11/11 [==============================] - 0s 17ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2057/5000\n",
      "\n",
      "Epoch 02057: LearningRateScheduler reducing learning rate to 5.117460753586415e-92.\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2058/5000\n",
      "\n",
      "Epoch 02058: LearningRateScheduler reducing learning rate to 4.630469975175515e-92.\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2059/5000\n",
      "\n",
      "Epoch 02059: LearningRateScheduler reducing learning rate to 4.18982249663087e-92.\n",
      "11/11 [==============================] - 0s 32ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2060/5000\n",
      "\n",
      "Epoch 02060: LearningRateScheduler reducing learning rate to 3.791108169880476e-92.\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2061/5000\n",
      "\n",
      "Epoch 02061: LearningRateScheduler reducing learning rate to 3.4303365279297016e-92.\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2062/5000\n",
      "\n",
      "Epoch 02062: LearningRateScheduler reducing learning rate to 3.103896846926279e-92.\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2063/5000\n",
      "\n",
      "Epoch 02063: LearningRateScheduler reducing learning rate to 2.8085220088227464e-92.\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2064/5000\n",
      "\n",
      "Epoch 02064: LearningRateScheduler reducing learning rate to 2.5412558029603545e-92.\n",
      "11/11 [==============================] - 0s 15ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2065/5000\n",
      "\n",
      "Epoch 02065: LearningRateScheduler reducing learning rate to 2.2994233393195594e-92.\n",
      "11/11 [==============================] - 0s 15ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2066/5000\n",
      "\n",
      "Epoch 02066: LearningRateScheduler reducing learning rate to 2.080604277321546e-92.\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2067/5000\n",
      "\n",
      "Epoch 02067: LearningRateScheduler reducing learning rate to 1.8826086022461587e-92.\n",
      "11/11 [==============================] - 0s 15ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2068/5000\n",
      "\n",
      "Epoch 02068: LearningRateScheduler reducing learning rate to 1.7034547068287107e-92.\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2069/5000\n",
      "\n",
      "Epoch 02069: LearningRateScheduler reducing learning rate to 1.541349558668102e-92.\n",
      "11/11 [==============================] - 0s 16ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2070/5000\n",
      "\n",
      "Epoch 02070: LearningRateScheduler reducing learning rate to 1.394670754956119e-92.\n",
      "11/11 [==============================] - 0s 21ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2071/5000\n",
      "\n",
      "Epoch 02071: LearningRateScheduler reducing learning rate to 1.2619502849247643e-92.\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2072/5000\n",
      "\n",
      "Epoch 02072: LearningRateScheduler reducing learning rate to 1.1418598375010412e-92.\n",
      "11/11 [==============================] - 0s 15ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2073/5000\n",
      "\n",
      "Epoch 02073: LearningRateScheduler reducing learning rate to 1.0331975071234086e-92.\n",
      "11/11 [==============================] - 0s 20ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2074/5000\n",
      "\n",
      "Epoch 02074: LearningRateScheduler reducing learning rate to 9.3487576466674e-93.\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2075/5000\n",
      "\n",
      "Epoch 02075: LearningRateScheduler reducing learning rate to 8.459105730854515e-93.\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2076/5000\n",
      "\n",
      "Epoch 02076: LearningRateScheduler reducing learning rate to 7.6541153883996306e-93.\n",
      "11/11 [==============================] - 0s 21ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2077/5000\n",
      "\n",
      "Epoch 02077: LearningRateScheduler reducing learning rate to 6.9257300053886704e-93.\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2078/5000\n",
      "\n",
      "Epoch 02078: LearningRateScheduler reducing learning rate to 6.266659656090092e-93.\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2079/5000\n",
      "\n",
      "Epoch 02079: LearningRateScheduler reducing learning rate to 5.670308142926706e-93.\n",
      "11/11 [==============================] - 0s 30ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2080/5000\n",
      "\n",
      "Epoch 02080: LearningRateScheduler reducing learning rate to 5.130706979514106e-93.\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2081/5000\n",
      "\n",
      "Epoch 02081: LearningRateScheduler reducing learning rate to 4.642455656042647e-93.\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2082/5000\n",
      "\n",
      "Epoch 02082: LearningRateScheduler reducing learning rate to 4.20066758915997e-93.\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2083/5000\n",
      "\n",
      "Epoch 02083: LearningRateScheduler reducing learning rate to 3.8009212154028676e-93.\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2084/5000\n",
      "\n",
      "Epoch 02084: LearningRateScheduler reducing learning rate to 3.439215738703252e-93.\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2085/5000\n",
      "\n",
      "Epoch 02085: LearningRateScheduler reducing learning rate to 3.1119310890769032e-93.\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2086/5000\n",
      "\n",
      "Epoch 02086: LearningRateScheduler reducing learning rate to 2.8157916917461933e-93.\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2087/5000\n",
      "\n",
      "Epoch 02087: LearningRateScheduler reducing learning rate to 2.5478336840866744e-93.\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2088/5000\n",
      "\n",
      "Epoch 02088: LearningRateScheduler reducing learning rate to 2.305375252294046e-93.\n",
      "11/11 [==============================] - 0s 18ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2089/5000\n",
      "\n",
      "Epoch 02089: LearningRateScheduler reducing learning rate to 2.0859897908897552e-93.\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2090/5000\n",
      "\n",
      "Epoch 02090: LearningRateScheduler reducing learning rate to 1.8874816164380683e-93.\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2091/5000\n",
      "\n",
      "Epoch 02091: LearningRateScheduler reducing learning rate to 1.7078639924081707e-93.\n",
      "11/11 [==============================] - 0s 15ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2092/5000\n",
      "\n",
      "Epoch 02092: LearningRateScheduler reducing learning rate to 1.5453392452471597e-93.\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2093/5000\n",
      "\n",
      "Epoch 02093: LearningRateScheduler reducing learning rate to 1.3982807726590864e-93.\n",
      "11/11 [==============================] - 0s 21ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2094/5000\n",
      "\n",
      "Epoch 02094: LearningRateScheduler reducing learning rate to 1.2652167640221814e-93.\n",
      "11/11 [==============================] - 0s 26ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2095/5000\n",
      "\n",
      "Epoch 02095: LearningRateScheduler reducing learning rate to 1.1448154700136492e-93.\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2096/5000\n",
      "\n",
      "Epoch 02096: LearningRateScheduler reducing learning rate to 1.0358718740147797e-93.\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2097/5000\n",
      "\n",
      "Epoch 02097: LearningRateScheduler reducing learning rate to 9.372956318995828e-94.\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2098/5000\n",
      "\n",
      "Epoch 02098: LearningRateScheduler reducing learning rate to 8.481001595044064e-94.\n",
      "11/11 [==============================] - 0s 17ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2099/5000\n",
      "\n",
      "Epoch 02099: LearningRateScheduler reducing learning rate to 7.67392758561857e-94.\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2100/5000\n",
      "\n",
      "Epoch 02100: LearningRateScheduler reducing learning rate to 6.943656822766072e-94.\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2101/5000\n",
      "\n",
      "Epoch 02101: LearningRateScheduler reducing learning rate to 6.282880511239463e-94.\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2102/5000\n",
      "\n",
      "Epoch 02102: LearningRateScheduler reducing learning rate to 5.684985379618236e-94.\n",
      "11/11 [==============================] - 0s 16ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2103/5000\n",
      "\n",
      "Epoch 02103: LearningRateScheduler reducing learning rate to 5.143987492465973e-94.\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2104/5000\n",
      "\n",
      "Epoch 02104: LearningRateScheduler reducing learning rate to 4.6544723610922076e-94.\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2105/5000\n",
      "\n",
      "Epoch 02105: LearningRateScheduler reducing learning rate to 4.2115407535304335e-94.\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2106/5000\n",
      "\n",
      "Epoch 02106: LearningRateScheduler reducing learning rate to 3.8107596613777184e-94.\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2107/5000\n",
      "\n",
      "Epoch 02107: LearningRateScheduler reducing learning rate to 3.448117932756524e-94.\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2108/5000\n",
      "\n",
      "Epoch 02108: LearningRateScheduler reducing learning rate to 3.1199861273589215e-94.\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2109/5000\n",
      "\n",
      "Epoch 02109: LearningRateScheduler reducing learning rate to 2.8230801917874755e-94.\n",
      "11/11 [==============================] - 0s 24ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2110/5000\n",
      "\n",
      "Epoch 02110: LearningRateScheduler reducing learning rate to 2.554428591645455e-94.\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2111/5000\n",
      "\n",
      "Epoch 02111: LearningRateScheduler reducing learning rate to 2.3113425714217192e-94.\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2112/5000\n",
      "\n",
      "Epoch 02112: LearningRateScheduler reducing learning rate to 2.0913892445217765e-94.\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2113/5000\n",
      "\n",
      "Epoch 02113: LearningRateScheduler reducing learning rate to 1.892367244121271e-94.\n",
      "11/11 [==============================] - 0s 16ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2114/5000\n",
      "\n",
      "Epoch 02114: LearningRateScheduler reducing learning rate to 1.712284691146525e-94.\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2115/5000\n",
      "\n",
      "Epoch 02115: LearningRateScheduler reducing learning rate to 1.549339258879531e-94.\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2116/5000\n",
      "\n",
      "Epoch 02116: LearningRateScheduler reducing learning rate to 1.40190013466631e-94.\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2117/5000\n",
      "\n",
      "Epoch 02117: LearningRateScheduler reducing learning rate to 1.2684916981956992e-94.\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2118/5000\n",
      "\n",
      "Epoch 02118: LearningRateScheduler reducing learning rate to 1.1477787529954525e-94.\n",
      "11/11 [==============================] - 0s 15ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2119/5000\n",
      "\n",
      "Epoch 02119: LearningRateScheduler reducing learning rate to 1.0385531633369446e-94.\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2120/5000\n",
      "\n",
      "Epoch 02120: LearningRateScheduler reducing learning rate to 9.397217628068845e-95.\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2121/5000\n",
      "\n",
      "Epoch 02121: LearningRateScheduler reducing learning rate to 8.502954135303866e-95.\n",
      "11/11 [==============================] - 0s 16ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2122/5000\n",
      "\n",
      "Epoch 02122: LearningRateScheduler reducing learning rate to 7.693791065466361e-95.\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2123/5000\n",
      "\n",
      "Epoch 02123: LearningRateScheduler reducing learning rate to 6.961630042584756e-95.\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2124/5000\n",
      "\n",
      "Epoch 02124: LearningRateScheduler reducing learning rate to 6.299143353053994e-95.\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2125/5000\n",
      "\n",
      "Epoch 02125: LearningRateScheduler reducing learning rate to 5.699700607415784e-95.\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2126/5000\n",
      "\n",
      "Epoch 02126: LearningRateScheduler reducing learning rate to 5.1573023811921185e-95.\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2127/5000\n",
      "\n",
      "Epoch 02127: LearningRateScheduler reducing learning rate to 4.666520170628476e-95.\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2128/5000\n",
      "\n",
      "Epoch 02128: LearningRateScheduler reducing learning rate to 4.22244206240422e-95.\n",
      "11/11 [==============================] - 0s 25ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2129/5000\n",
      "\n",
      "Epoch 02129: LearningRateScheduler reducing learning rate to 3.820623573552288e-95.\n",
      "11/11 [==============================] - 0s 15ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2130/5000\n",
      "\n",
      "Epoch 02130: LearningRateScheduler reducing learning rate to 3.457043169580393e-95.\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2131/5000\n",
      "\n",
      "Epoch 02131: LearningRateScheduler reducing learning rate to 3.128062015601991e-95.\n",
      "11/11 [==============================] - 0s 16ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2132/5000\n",
      "\n",
      "Epoch 02132: LearningRateScheduler reducing learning rate to 2.8303875576536008e-95.\n",
      "11/11 [==============================] - 0s 17ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2133/5000\n",
      "\n",
      "Epoch 02133: LearningRateScheduler reducing learning rate to 2.5610405697084044e-95.\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2134/5000\n",
      "\n",
      "Epoch 02134: LearningRateScheduler reducing learning rate to 2.3173253365803085e-95.\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2135/5000\n",
      "\n",
      "Epoch 02135: LearningRateScheduler reducing learning rate to 2.0968026743006492e-95.\n",
      "11/11 [==============================] - 0s 15ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2136/5000\n",
      "\n",
      "Epoch 02136: LearningRateScheduler reducing learning rate to 1.8972655179451053e-95.\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2137/5000\n",
      "\n",
      "Epoch 02137: LearningRateScheduler reducing learning rate to 1.7167168325860677e-95.\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2138/5000\n",
      "\n",
      "Epoch 02138: LearningRateScheduler reducing learning rate to 1.553349626296057e-95.\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2139/5000\n",
      "\n",
      "Epoch 02139: LearningRateScheduler reducing learning rate to 1.4055288651648548e-95.\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2140/5000\n",
      "\n",
      "Epoch 02140: LearningRateScheduler reducing learning rate to 1.2717751093307867e-95.\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2141/5000\n",
      "\n",
      "Epoch 02141: LearningRateScheduler reducing learning rate to 1.1507497062492757e-95.\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2142/5000\n",
      "\n",
      "Epoch 02142: LearningRateScheduler reducing learning rate to 1.04124139300821e-95.\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2143/5000\n",
      "\n",
      "Epoch 02143: LearningRateScheduler reducing learning rate to 9.421541736017199e-96.\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2144/5000\n",
      "\n",
      "Epoch 02144: LearningRateScheduler reducing learning rate to 8.524963498335883e-96.\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2145/5000\n",
      "\n",
      "Epoch 02145: LearningRateScheduler reducing learning rate to 7.713705960685087e-96.\n",
      "11/11 [==============================] - 0s 18ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2146/5000\n",
      "\n",
      "Epoch 02146: LearningRateScheduler reducing learning rate to 6.979649784954924e-96.\n",
      "11/11 [==============================] - 0s 16ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2147/5000\n",
      "\n",
      "Epoch 02147: LearningRateScheduler reducing learning rate to 6.315448290213711e-96.\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2148/5000\n",
      "\n",
      "Epoch 02148: LearningRateScheduler reducing learning rate to 5.714453924656622e-96.\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2149/5000\n",
      "\n",
      "Epoch 02149: LearningRateScheduler reducing learning rate to 5.170651734671783e-96.\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2150/5000\n",
      "\n",
      "Epoch 02150: LearningRateScheduler reducing learning rate to 4.678599165163598e-96.\n",
      "11/11 [==============================] - 0s 24ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2151/5000\n",
      "\n",
      "Epoch 02151: LearningRateScheduler reducing learning rate to 4.23337158863185e-96.\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2152/5000\n",
      "\n",
      "Epoch 02152: LearningRateScheduler reducing learning rate to 3.8305130178443445e-96.\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2153/5000\n",
      "\n",
      "Epoch 02153: LearningRateScheduler reducing learning rate to 3.465991508819428e-96.\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2154/5000\n",
      "\n",
      "Epoch 02154: LearningRateScheduler reducing learning rate to 3.1361588077747487e-96.\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2155/5000\n",
      "\n",
      "Epoch 02155: LearningRateScheduler reducing learning rate to 2.837713838177653e-96.\n",
      "11/11 [==============================] - 0s 16ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2156/5000\n",
      "\n",
      "Epoch 02156: LearningRateScheduler reducing learning rate to 2.567669662461595e-96.\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2157/5000\n",
      "\n",
      "Epoch 02157: LearningRateScheduler reducing learning rate to 2.3233235877509607e-96.\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2158/5000\n",
      "\n",
      "Epoch 02158: LearningRateScheduler reducing learning rate to 2.1022301164026334e-96.\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2159/5000\n",
      "\n",
      "Epoch 02159: LearningRateScheduler reducing learning rate to 1.9021764706432044e-96.\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2160/5000\n",
      "\n",
      "Epoch 02160: LearningRateScheduler reducing learning rate to 1.721160446345561e-96.\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2161/5000\n",
      "\n",
      "Epoch 02161: LearningRateScheduler reducing learning rate to 1.5573703742969461e-96.\n",
      "11/11 [==============================] - 0s 22ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2162/5000\n",
      "\n",
      "Epoch 02162: LearningRateScheduler reducing learning rate to 1.4091669884045125e-96.\n",
      "11/11 [==============================] - 0s 15ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2163/5000\n",
      "\n",
      "Epoch 02163: LearningRateScheduler reducing learning rate to 1.2750670193694553e-96.\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2164/5000\n",
      "\n",
      "Epoch 02164: LearningRateScheduler reducing learning rate to 1.1537283496290714e-96.\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2165/5000\n",
      "\n",
      "Epoch 02165: LearningRateScheduler reducing learning rate to 1.0439365809932637e-96.\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2166/5000\n",
      "\n",
      "Epoch 02166: LearningRateScheduler reducing learning rate to 9.445928805392374e-97.\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2167/5000\n",
      "\n",
      "Epoch 02167: LearningRateScheduler reducing learning rate to 8.547029831222539e-97.\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2168/5000\n",
      "\n",
      "Epoch 02168: LearningRateScheduler reducing learning rate to 7.73367240435977e-97.\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2169/5000\n",
      "\n",
      "Epoch 02169: LearningRateScheduler reducing learning rate to 6.997716170296885e-97.\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2170/5000\n",
      "\n",
      "Epoch 02170: LearningRateScheduler reducing learning rate to 6.331795431679952e-97.\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2171/5000\n",
      "\n",
      "Epoch 02171: LearningRateScheduler reducing learning rate to 5.729245429933205e-97.\n",
      "11/11 [==============================] - 0s 23ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2172/5000\n",
      "\n",
      "Epoch 02172: LearningRateScheduler reducing learning rate to 5.184035642114964e-97.\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2173/5000\n",
      "\n",
      "Epoch 02173: LearningRateScheduler reducing learning rate to 4.690709425417719e-97.\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2174/5000\n",
      "\n",
      "Epoch 02174: LearningRateScheduler reducing learning rate to 4.2443294052519324e-97.\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2175/5000\n",
      "\n",
      "Epoch 02175: LearningRateScheduler reducing learning rate to 3.84042806034228e-97.\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2176/5000\n",
      "\n",
      "Epoch 02176: LearningRateScheduler reducing learning rate to 3.474963010272977e-97.\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2177/5000\n",
      "\n",
      "Epoch 02177: LearningRateScheduler reducing learning rate to 3.1442765579857946e-97.\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2178/5000\n",
      "\n",
      "Epoch 02178: LearningRateScheduler reducing learning rate to 2.8450590823188767e-97.\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2179/5000\n",
      "\n",
      "Epoch 02179: LearningRateScheduler reducing learning rate to 2.5743159142051837e-97.\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2180/5000\n",
      "\n",
      "Epoch 02180: LearningRateScheduler reducing learning rate to 2.3293373650183123e-97.\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2181/5000\n",
      "\n",
      "Epoch 02181: LearningRateScheduler reducing learning rate to 2.1076716070978672e-97.\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2182/5000\n",
      "\n",
      "Epoch 02182: LearningRateScheduler reducing learning rate to 1.9071001350340923e-97.\n",
      "11/11 [==============================] - ETA: 0s - loss: 86.5012 - mae: 6.5823 - mse: 86.501 - 0s 14ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2183/5000\n",
      "\n",
      "Epoch 02183: LearningRateScheduler reducing learning rate to 1.7256155621202874e-97.\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2184/5000\n",
      "\n",
      "Epoch 02184: LearningRateScheduler reducing learning rate to 1.5614015297516009e-97.\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2185/5000\n",
      "\n",
      "Epoch 02185: LearningRateScheduler reducing learning rate to 1.4128145286978441e-97.\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2186/5000\n",
      "\n",
      "Epoch 02186: LearningRateScheduler reducing learning rate to 1.2783674503106556e-97.\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2187/5000\n",
      "\n",
      "Epoch 02187: LearningRateScheduler reducing learning rate to 1.1567147030402802e-97.\n",
      "11/11 [==============================] - 0s 17ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2188/5000\n",
      "\n",
      "Epoch 02188: LearningRateScheduler reducing learning rate to 1.0466387453032047e-97.\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2189/5000\n",
      "\n",
      "Epoch 02189: LearningRateScheduler reducing learning rate to 9.470378999165535e-98.\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2190/5000\n",
      "\n",
      "Epoch 02190: LearningRateScheduler reducing learning rate to 8.569153281426967e-98.\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2191/5000\n",
      "\n",
      "Epoch 02191: LearningRateScheduler reducing learning rate to 7.753690529920791e-98.\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2192/5000\n",
      "\n",
      "Epoch 02192: LearningRateScheduler reducing learning rate to 7.01582931934324e-98.\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2193/5000\n",
      "\n",
      "Epoch 02193: LearningRateScheduler reducing learning rate to 6.348184886695557e-98.\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2194/5000\n",
      "\n",
      "Epoch 02194: LearningRateScheduler reducing learning rate to 5.744075222092541e-98.\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2195/5000\n",
      "\n",
      "Epoch 02195: LearningRateScheduler reducing learning rate to 5.197454192962575e-98.\n",
      "11/11 [==============================] - 0s 15ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2196/5000\n",
      "\n",
      "Epoch 02196: LearningRateScheduler reducing learning rate to 4.7028510323204556e-98.\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2197/5000\n",
      "\n",
      "Epoch 02197: LearningRateScheduler reducing learning rate to 4.2553155854924914e-98.\n",
      "11/11 [==============================] - 0s 16ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2198/5000\n",
      "\n",
      "Epoch 02198: LearningRateScheduler reducing learning rate to 3.850368767305226e-98.\n",
      "11/11 [==============================] - 0s 18ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2199/5000\n",
      "\n",
      "Epoch 02199: LearningRateScheduler reducing learning rate to 3.4839577338947805e-98.\n",
      "11/11 [==============================] - 0s 34ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2200/5000\n",
      "\n",
      "Epoch 02200: LearningRateScheduler reducing learning rate to 3.1524153204837835e-98.\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2201/5000\n",
      "\n",
      "Epoch 02201: LearningRateScheduler reducing learning rate to 2.852423339163565e-98.\n",
      "11/11 [==============================] - ETA: 0s - loss: 122.2086 - mae: 8.2034 - mse: 122.208 - 0s 21ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2202/5000\n",
      "\n",
      "Epoch 02202: LearningRateScheduler reducing learning rate to 2.580979369354212e-98.\n",
      "11/11 [==============================] - 0s 27ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2203/5000\n",
      "\n",
      "Epoch 02203: LearningRateScheduler reducing learning rate to 2.3353667085705576e-98.\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2204/5000\n",
      "\n",
      "Epoch 02204: LearningRateScheduler reducing learning rate to 2.1131271827501325e-98.\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2205/5000\n",
      "\n",
      "Epoch 02205: LearningRateScheduler reducing learning rate to 1.912036544021242e-98.\n",
      "11/11 [==============================] - 0s 22ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2206/5000\n",
      "\n",
      "Epoch 02206: LearningRateScheduler reducing learning rate to 1.73008220968259e-98.\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2207/5000\n",
      "\n",
      "Epoch 02207: LearningRateScheduler reducing learning rate to 1.5654431195991065e-98.\n",
      "11/11 [==============================] - 0s 18ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2208/5000\n",
      "\n",
      "Epoch 02208: LearningRateScheduler reducing learning rate to 1.4164715104202216e-98.\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2209/5000\n",
      "\n",
      "Epoch 02209: LearningRateScheduler reducing learning rate to 1.2816764242101364e-98.\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2210/5000\n",
      "\n",
      "Epoch 02210: LearningRateScheduler reducing learning rate to 1.1597087864398677e-98.\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2211/5000\n",
      "\n",
      "Epoch 02211: LearningRateScheduler reducing learning rate to 1.0493479039958718e-98.\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2212/5000\n",
      "\n",
      "Epoch 02212: LearningRateScheduler reducing learning rate to 9.49489248073049e-99.\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2213/5000\n",
      "\n",
      "Epoch 02213: LearningRateScheduler reducing learning rate to 8.591333996793272e-99.\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2214/5000\n",
      "\n",
      "Epoch 02214: LearningRateScheduler reducing learning rate to 7.773760471143031e-99.\n",
      "11/11 [==============================] - 0s 22ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2215/5000\n",
      "\n",
      "Epoch 02215: LearningRateScheduler reducing learning rate to 7.033989353139104e-99.\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2216/5000\n",
      "\n",
      "Epoch 02216: LearningRateScheduler reducing learning rate to 6.364616764786853e-99.\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2217/5000\n",
      "\n",
      "Epoch 02217: LearningRateScheduler reducing learning rate to 5.758943400237987e-99.\n",
      "11/11 [==============================] - ETA: 0s - loss: 151.2338 - mae: 8.8802 - mse: 151.233 - 0s 17ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2218/5000\n",
      "\n",
      "Epoch 02218: LearningRateScheduler reducing learning rate to 5.210907476886599e-99.\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2219/5000\n",
      "\n",
      "Epoch 02219: LearningRateScheduler reducing learning rate to 4.715024067010374e-99.\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2220/5000\n",
      "\n",
      "Epoch 02220: LearningRateScheduler reducing learning rate to 4.2663302027711e-99.\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2221/5000\n",
      "\n",
      "Epoch 02221: LearningRateScheduler reducing learning rate to 3.860335205164257e-99.\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2222/5000\n",
      "\n",
      "Epoch 02222: LearningRateScheduler reducing learning rate to 3.4929757397940627e-99.\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2223/5000\n",
      "\n",
      "Epoch 02223: LearningRateScheduler reducing learning rate to 3.1605751496575234e-99.\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2224/5000\n",
      "\n",
      "Epoch 02224: LearningRateScheduler reducing learning rate to 2.8598066579247463e-99.\n",
      "11/11 [==============================] - 0s 16ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2225/5000\n",
      "\n",
      "Epoch 02225: LearningRateScheduler reducing learning rate to 2.587660072438689e-99.\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2226/5000\n",
      "\n",
      "Epoch 02226: LearningRateScheduler reducing learning rate to 2.341411658700181e-99.\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2227/5000\n",
      "\n",
      "Epoch 02227: LearningRateScheduler reducing learning rate to 2.1185968798175164e-99.\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2228/5000\n",
      "\n",
      "Epoch 02228: LearningRateScheduler reducing learning rate to 1.9169857305931328e-99.\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2229/5000\n",
      "\n",
      "Epoch 02229: LearningRateScheduler reducing learning rate to 1.7345604188816778e-99.\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2230/5000\n",
      "\n",
      "Epoch 02230: LearningRateScheduler reducing learning rate to 1.5694951708482787e-99.\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2231/5000\n",
      "\n",
      "Epoch 02231: LearningRateScheduler reducing learning rate to 1.4201379580102719e-99.\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2232/5000\n",
      "\n",
      "Epoch 02232: LearningRateScheduler reducing learning rate to 1.284993963180845e-99.\n",
      "11/11 [==============================] - 0s 17ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2233/5000\n",
      "\n",
      "Epoch 02233: LearningRateScheduler reducing learning rate to 1.1627106198363573e-99.\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2234/5000\n",
      "\n",
      "Epoch 02234: LearningRateScheduler reducing learning rate to 1.0520640751757256e-99.\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2235/5000\n",
      "\n",
      "Epoch 02235: LearningRateScheduler reducing learning rate to 9.519469413903988e-100.\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2236/5000\n",
      "\n",
      "Epoch 02236: LearningRateScheduler reducing learning rate to 8.613572125549222e-100.\n",
      "11/11 [==============================] - 0s 26ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2237/5000\n",
      "\n",
      "Epoch 02237: LearningRateScheduler reducing learning rate to 7.793882362148294e-100.\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2238/5000\n",
      "\n",
      "Epoch 02238: LearningRateScheduler reducing learning rate to 7.052196393042308e-100.\n",
      "11/11 [==============================] - 0s 16ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2239/5000\n",
      "\n",
      "Epoch 02239: LearningRateScheduler reducing learning rate to 6.381091175762945e-100.\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2240/5000\n",
      "\n",
      "Epoch 02240: LearningRateScheduler reducing learning rate to 5.773850063729421e-100.\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2241/5000\n",
      "\n",
      "Epoch 02241: LearningRateScheduler reducing learning rate to 5.22439558379172e-100.\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2242/5000\n",
      "\n",
      "Epoch 02242: LearningRateScheduler reducing learning rate to 4.727228610836462e-100.\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2243/5000\n",
      "\n",
      "Epoch 02243: LearningRateScheduler reducing learning rate to 4.2773733306950045e-100.\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2244/5000\n",
      "\n",
      "Epoch 02244: LearningRateScheduler reducing learning rate to 3.870327440521962e-100.\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2245/5000\n",
      "\n",
      "Epoch 02245: LearningRateScheduler reducing learning rate to 3.5020170882356365e-100.\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2246/5000\n",
      "\n",
      "Epoch 02246: LearningRateScheduler reducing learning rate to 3.1687561000369605e-100.\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2247/5000\n",
      "\n",
      "Epoch 02247: LearningRateScheduler reducing learning rate to 2.867209087943075e-100.\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2248/5000\n",
      "\n",
      "Epoch 02248: LearningRateScheduler reducing learning rate to 2.5943580681036655e-100.\n",
      "11/11 [==============================] - 0s 18ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2249/5000\n",
      "\n",
      "Epoch 02249: LearningRateScheduler reducing learning rate to 2.347472255803694e-100.\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2250/5000\n",
      "\n",
      "Epoch 02250: LearningRateScheduler reducing learning rate to 2.1240807348524763e-100.\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2251/5000\n",
      "\n",
      "Epoch 02251: LearningRateScheduler reducing learning rate to 1.9219477278238493e-100.\n",
      "11/11 [==============================] - 0s 23ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2252/5000\n",
      "\n",
      "Epoch 02252: LearningRateScheduler reducing learning rate to 1.739050219644171e-100.\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2253/5000\n",
      "\n",
      "Epoch 02253: LearningRateScheduler reducing learning rate to 1.5735577105777092e-100.\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2254/5000\n",
      "\n",
      "Epoch 02254: LearningRateScheduler reducing learning rate to 1.4238138959697184e-100.\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2255/5000\n",
      "\n",
      "Epoch 02255: LearningRateScheduler reducing learning rate to 1.2883200893929676e-100.\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2256/5000\n",
      "\n",
      "Epoch 02256: LearningRateScheduler reducing learning rate to 1.165720223290196e-100.\n",
      "11/11 [==============================] - 0s 24ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2257/5000\n",
      "\n",
      "Epoch 02257: LearningRateScheduler reducing learning rate to 1.0547872769941792e-100.\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2258/5000\n",
      "\n",
      "Epoch 02258: LearningRateScheduler reducing learning rate to 9.544109962925991e-101.\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2259/5000\n",
      "\n",
      "Epoch 02259: LearningRateScheduler reducing learning rate to 8.635867816305281e-101.\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2260/5000\n",
      "\n",
      "Epoch 02260: LearningRateScheduler reducing learning rate to 7.814056337405554e-101.\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2261/5000\n",
      "\n",
      "Epoch 02261: LearningRateScheduler reducing learning rate to 7.070450560725609e-101.\n",
      "11/11 [==============================] - 0s 18ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2262/5000\n",
      "\n",
      "Epoch 02262: LearningRateScheduler reducing learning rate to 6.397608229717718e-101.\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2263/5000\n",
      "\n",
      "Epoch 02263: LearningRateScheduler reducing learning rate to 5.788795312183418e-101.\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2264/5000\n",
      "\n",
      "Epoch 02264: LearningRateScheduler reducing learning rate to 5.237918603814741e-101.\n",
      "11/11 [==============================] - 0s 15ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2265/5000\n",
      "\n",
      "Epoch 02265: LearningRateScheduler reducing learning rate to 4.7394647453582754e-101.\n",
      "11/11 [==============================] - 0s 15ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2266/5000\n",
      "\n",
      "Epoch 02266: LearningRateScheduler reducing learning rate to 4.288445043062463e-101.\n",
      "11/11 [==============================] - 0s 25ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2267/5000\n",
      "\n",
      "Epoch 02267: LearningRateScheduler reducing learning rate to 3.88034554015366e-101.\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2268/5000\n",
      "\n",
      "Epoch 02268: LearningRateScheduler reducing learning rate to 3.511081839640009e-101.\n",
      "11/11 [==============================] - 0s 18ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2269/5000\n",
      "\n",
      "Epoch 02269: LearningRateScheduler reducing learning rate to 3.1769582262928306e-101.\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2270/5000\n",
      "\n",
      "Epoch 02270: LearningRateScheduler reducing learning rate to 2.874630678686923e-101.\n",
      "11/11 [==============================] - 0s 15ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2271/5000\n",
      "\n",
      "Epoch 02271: LearningRateScheduler reducing learning rate to 2.601073401110048e-101.\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2272/5000\n",
      "\n",
      "Epoch 02272: LearningRateScheduler reducing learning rate to 2.353548540382374e-101.\n",
      "11/11 [==============================] - 0s 15ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2273/5000\n",
      "\n",
      "Epoch 02273: LearningRateScheduler reducing learning rate to 2.129578784501901e-101.\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2274/5000\n",
      "\n",
      "Epoch 02274: LearningRateScheduler reducing learning rate to 1.9269225688728683e-101.\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2275/5000\n",
      "\n",
      "Epoch 02275: LearningRateScheduler reducing learning rate to 1.7435516419741544e-101.\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2276/5000\n",
      "\n",
      "Epoch 02276: LearningRateScheduler reducing learning rate to 1.5776307659362607e-101.\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2277/5000\n",
      "\n",
      "Epoch 02277: LearningRateScheduler reducing learning rate to 1.427499348863827e-101.\n",
      "11/11 [==============================] - 0s 18ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2278/5000\n",
      "\n",
      "Epoch 02278: LearningRateScheduler reducing learning rate to 1.2916548250739659e-101.\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2279/5000\n",
      "\n",
      "Epoch 02279: LearningRateScheduler reducing learning rate to 1.1687376169136232e-101.\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2280/5000\n",
      "\n",
      "Epoch 02280: LearningRateScheduler reducing learning rate to 1.0575175276496291e-101.\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2281/5000\n",
      "\n",
      "Epoch 02281: LearningRateScheduler reducing learning rate to 9.568814292462673e-102.\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2282/5000\n",
      "\n",
      "Epoch 02282: LearningRateScheduler reducing learning rate to 8.658221218057315e-102.\n",
      "11/11 [==============================] - 0s 20ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2283/5000\n",
      "\n",
      "Epoch 02283: LearningRateScheduler reducing learning rate to 7.834282531731188e-102.\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2284/5000\n",
      "\n",
      "Epoch 02284: LearningRateScheduler reducing learning rate to 7.088751978175909e-102.\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2285/5000\n",
      "\n",
      "Epoch 02285: LearningRateScheduler reducing learning rate to 6.414168037030027e-102.\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2286/5000\n",
      "\n",
      "Epoch 02286: LearningRateScheduler reducing learning rate to 5.803779245475061e-102.\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2287/5000\n",
      "\n",
      "Epoch 02287: LearningRateScheduler reducing learning rate to 5.251476627326225e-102.\n",
      "11/11 [==============================] - 0s 15ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2288/5000\n",
      "\n",
      "Epoch 02288: LearningRateScheduler reducing learning rate to 4.751732552346077e-102.\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2289/5000\n",
      "\n",
      "Epoch 02289: LearningRateScheduler reducing learning rate to 4.299545413862269e-102.\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2290/5000\n",
      "\n",
      "Epoch 02290: LearningRateScheduler reducing learning rate to 3.8903895710075086e-102.\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2291/5000\n",
      "\n",
      "Epoch 02291: LearningRateScheduler reducing learning rate to 3.5201700545844787e-102.\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2292/5000\n",
      "\n",
      "Epoch 02292: LearningRateScheduler reducing learning rate to 3.18518158323765e-102.\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2293/5000\n",
      "\n",
      "Epoch 02293: LearningRateScheduler reducing learning rate to 2.8820714797524616e-102.\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2294/5000\n",
      "\n",
      "Epoch 02294: LearningRateScheduler reducing learning rate to 2.6078061163343092e-102.\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2295/5000\n",
      "\n",
      "Epoch 02295: LearningRateScheduler reducing learning rate to 2.359640553042333e-102.\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2296/5000\n",
      "\n",
      "Epoch 02296: LearningRateScheduler reducing learning rate to 2.1350910655077806e-102.\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2297/5000\n",
      "\n",
      "Epoch 02297: LearningRateScheduler reducing learning rate to 1.931910286985662e-102.\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2298/5000\n",
      "\n",
      "Epoch 02298: LearningRateScheduler reducing learning rate to 1.7480647159532261e-102.\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2299/5000\n",
      "\n",
      "Epoch 02299: LearningRateScheduler reducing learning rate to 1.5817143641428892e-102.\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2300/5000\n",
      "\n",
      "Epoch 02300: LearningRateScheduler reducing learning rate to 1.4311943413214495e-102.\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2301/5000\n",
      "\n",
      "Epoch 02301: LearningRateScheduler reducing learning rate to 1.2949981925089835e-102.\n",
      "11/11 [==============================] - 1s 53ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2302/5000\n",
      "\n",
      "Epoch 02302: LearningRateScheduler reducing learning rate to 1.1717628208710366e-102.\n",
      "11/11 [==============================] - 0s 15ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2303/5000\n",
      "\n",
      "Epoch 02303: LearningRateScheduler reducing learning rate to 1.0602548453874875e-102.\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2304/5000\n",
      "\n",
      "Epoch 02304: LearningRateScheduler reducing learning rate to 9.593582567605352e-103.\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2305/5000\n",
      "\n",
      "Epoch 02305: LearningRateScheduler reducing learning rate to 8.680632480186867e-103.\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2306/5000\n",
      "\n",
      "Epoch 02306: LearningRateScheduler reducing learning rate to 7.854561080291416e-103.\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2307/5000\n",
      "\n",
      "Epoch 02307: LearningRateScheduler reducing learning rate to 7.107100767696462e-103.\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2308/5000\n",
      "\n",
      "Epoch 02308: LearningRateScheduler reducing learning rate to 6.430770708363889e-103.\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2309/5000\n",
      "\n",
      "Epoch 02309: LearningRateScheduler reducing learning rate to 5.818801963737294e-103.\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2310/5000\n",
      "\n",
      "Epoch 02310: LearningRateScheduler reducing learning rate to 5.265069744930654e-103.\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2311/5000\n",
      "\n",
      "Epoch 02311: LearningRateScheduler reducing learning rate to 4.764032113782328e-103.\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2312/5000\n",
      "\n",
      "Epoch 02312: LearningRateScheduler reducing learning rate to 4.3106745172750983e-103.\n",
      "11/11 [==============================] - 0s 24ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2313/5000\n",
      "\n",
      "Epoch 02313: LearningRateScheduler reducing learning rate to 3.9004596002046284e-103.\n",
      "11/11 [==============================] - 0s 20ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2314/5000\n",
      "\n",
      "Epoch 02314: LearningRateScheduler reducing learning rate to 3.5292817938027477e-103.\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2315/5000\n",
      "\n",
      "Epoch 02315: LearningRateScheduler reducing learning rate to 3.1934262258258156e-103.\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2316/5000\n",
      "\n",
      "Epoch 02316: LearningRateScheduler reducing learning rate to 2.889531540864567e-103.\n",
      "11/11 [==============================] - 0s 18ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2317/5000\n",
      "\n",
      "Epoch 02317: LearningRateScheduler reducing learning rate to 2.614556258769303e-103.\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2318/5000\n",
      "\n",
      "Epoch 02318: LearningRateScheduler reducing learning rate to 2.3657483344945877e-103.\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2319/5000\n",
      "\n",
      "Epoch 02319: LearningRateScheduler reducing learning rate to 2.1406176147069666e-103.\n",
      "11/11 [==============================] - 0s 16ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2320/5000\n",
      "\n",
      "Epoch 02320: LearningRateScheduler reducing learning rate to 1.9369109154937574e-103.\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2321/5000\n",
      "\n",
      "Epoch 02321: LearningRateScheduler reducing learning rate to 1.7525894717410476e-103.\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2322/5000\n",
      "\n",
      "Epoch 02322: LearningRateScheduler reducing learning rate to 1.58580853248714e-103.\n",
      "11/11 [==============================] - 0s 16ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2323/5000\n",
      "\n",
      "Epoch 02323: LearningRateScheduler reducing learning rate to 1.434898898035066e-103.\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2324/5000\n",
      "\n",
      "Epoch 02324: LearningRateScheduler reducing learning rate to 1.2983502140407002e-103.\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2325/5000\n",
      "\n",
      "Epoch 02325: LearningRateScheduler reducing learning rate to 1.1747958553790293e-103.\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2326/5000\n",
      "\n",
      "Epoch 02326: LearningRateScheduler reducing learning rate to 1.0629992485005135e-103.\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2327/5000\n",
      "\n",
      "Epoch 02327: LearningRateScheduler reducing learning rate to 9.618414953873481e-104.\n",
      "11/11 [==============================] - 0s 17ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2328/5000\n",
      "\n",
      "Epoch 02328: LearningRateScheduler reducing learning rate to 8.703101752461393e-104.\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2329/5000\n",
      "\n",
      "Epoch 02329: LearningRateScheduler reducing learning rate to 7.874892118601447e-104.\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2330/5000\n",
      "\n",
      "Epoch 02330: LearningRateScheduler reducing learning rate to 7.125497051907101e-104.\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2331/5000\n",
      "\n",
      "Epoch 02331: LearningRateScheduler reducing learning rate to 6.447416354670499e-104.\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2332/5000\n",
      "\n",
      "Epoch 02332: LearningRateScheduler reducing learning rate to 5.833863567362741e-104.\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2333/5000\n",
      "\n",
      "Epoch 02333: LearningRateScheduler reducing learning rate to 5.278698047466585e-104.\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2334/5000\n",
      "\n",
      "Epoch 02334: LearningRateScheduler reducing learning rate to 4.7763635118611536e-104.\n",
      "11/11 [==============================] - 0s 18ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2335/5000\n",
      "\n",
      "Epoch 02335: LearningRateScheduler reducing learning rate to 4.321832427673639e-104.\n",
      "11/11 [==============================] - 0s 21ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2336/5000\n",
      "\n",
      "Epoch 02336: LearningRateScheduler reducing learning rate to 3.910555695040321e-104.\n",
      "11/11 [==============================] - 0s 31ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2337/5000\n",
      "\n",
      "Epoch 02337: LearningRateScheduler reducing learning rate to 3.5384171181860204e-104.\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2338/5000\n",
      "\n",
      "Epoch 02338: LearningRateScheduler reducing learning rate to 3.2016922091536977e-104.\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2339/5000\n",
      "\n",
      "Epoch 02339: LearningRateScheduler reducing learning rate to 2.8970109118764957e-104.\n",
      "11/11 [==============================] - 0s 18ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2340/5000\n",
      "\n",
      "Epoch 02340: LearningRateScheduler reducing learning rate to 2.6213238735243442e-104.\n",
      "11/11 [==============================] - 0s 15ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2341/5000\n",
      "\n",
      "Epoch 02341: LearningRateScheduler reducing learning rate to 2.3718719255558012e-104.\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2342/5000\n",
      "\n",
      "Epoch 02342: LearningRateScheduler reducing learning rate to 2.146158469031842e-104.\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2343/5000\n",
      "\n",
      "Epoch 02343: LearningRateScheduler reducing learning rate to 1.941924487814791e-104.\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2344/5000\n",
      "\n",
      "Epoch 02344: LearningRateScheduler reducing learning rate to 1.7571259395751485e-104.\n",
      "11/11 [==============================] - 0s 8ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2345/5000\n",
      "\n",
      "Epoch 02345: LearningRateScheduler reducing learning rate to 1.5899132983291962e-104.\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2346/5000\n",
      "\n",
      "Epoch 02346: LearningRateScheduler reducing learning rate to 1.4386130437612344e-104.\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2347/5000\n",
      "\n",
      "Epoch 02347: LearningRateScheduler reducing learning rate to 1.3017109120697386e-104.\n",
      "11/11 [==============================] - 0s 32ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2348/5000\n",
      "\n",
      "Epoch 02348: LearningRateScheduler reducing learning rate to 1.177836740706423e-104.\n",
      "11/11 [==============================] - 0s 41ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2349/5000\n",
      "\n",
      "Epoch 02349: LearningRateScheduler reducing learning rate to 1.0657507553286958e-104.\n",
      "11/11 [==============================] - 0s 17ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2350/5000\n",
      "\n",
      "Epoch 02350: LearningRateScheduler reducing learning rate to 9.643311617214962e-105.\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2351/5000\n",
      "\n",
      "Epoch 02351: LearningRateScheduler reducing learning rate to 8.72562918503701e-105.\n",
      "11/11 [==============================] - 0s 45ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2352/5000\n",
      "\n",
      "Epoch 02352: LearningRateScheduler reducing learning rate to 7.895275782527923e-105.\n",
      "11/11 [==============================] - 0s 24ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2353/5000\n",
      "\n",
      "Epoch 02353: LearningRateScheduler reducing learning rate to 7.143940953744447e-105.\n",
      "11/11 [==============================] - 0s 27ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2354/5000\n",
      "\n",
      "Epoch 02354: LearningRateScheduler reducing learning rate to 6.464105087187512e-105.\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2355/5000\n",
      "\n",
      "Epoch 02355: LearningRateScheduler reducing learning rate to 5.848964157003893e-105.\n",
      "11/11 [==============================] - 0s 17ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2356/5000\n",
      "\n",
      "Epoch 02356: LearningRateScheduler reducing learning rate to 5.2923616260083054e-105.\n",
      "11/11 [==============================] - 0s 15ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2357/5000\n",
      "\n",
      "Epoch 02357: LearningRateScheduler reducing learning rate to 4.788726828989839e-105.\n",
      "11/11 [==============================] - 0s 21ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2358/5000\n",
      "\n",
      "Epoch 02358: LearningRateScheduler reducing learning rate to 4.333019219622719e-105.\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2359/5000\n",
      "\n",
      "Epoch 02359: LearningRateScheduler reducing learning rate to 3.920677922983631e-105.\n",
      "11/11 [==============================] - 1s 29ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2360/5000\n",
      "\n",
      "Epoch 02360: LearningRateScheduler reducing learning rate to 3.5475760887831186e-105.\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2361/5000\n",
      "\n",
      "Epoch 02361: LearningRateScheduler reducing learning rate to 3.209979588460643e-105.\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2362/5000\n",
      "\n",
      "Epoch 02362: LearningRateScheduler reducing learning rate to 2.9045096427707943e-105.\n",
      "11/11 [==============================] - 0s 16ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2363/5000\n",
      "\n",
      "Epoch 02363: LearningRateScheduler reducing learning rate to 2.628109005825288e-105.\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2364/5000\n",
      "\n",
      "Epoch 02364: LearningRateScheduler reducing learning rate to 2.3780113671480194e-105.\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2365/5000\n",
      "\n",
      "Epoch 02365: LearningRateScheduler reducing learning rate to 2.1517136655103885e-105.\n",
      "11/11 [==============================] - 0s 15ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2366/5000\n",
      "\n",
      "Epoch 02366: LearningRateScheduler reducing learning rate to 1.9469510374531214e-105.\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2367/5000\n",
      "\n",
      "Epoch 02367: LearningRateScheduler reducing learning rate to 1.761674149771475e-105.\n",
      "11/11 [==============================] - 0s 22ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2368/5000\n",
      "\n",
      "Epoch 02368: LearningRateScheduler reducing learning rate to 1.594028689099925e-105.\n",
      "11/11 [==============================] - 0s 25ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2369/5000\n",
      "\n",
      "Epoch 02369: LearningRateScheduler reducing learning rate to 1.4423368033204297e-105.\n",
      "11/11 [==============================] - 0s 17ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2370/5000\n",
      "\n",
      "Epoch 02370: LearningRateScheduler reducing learning rate to 1.3050803090547046e-105.\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2371/5000\n",
      "\n",
      "Epoch 02371: LearningRateScheduler reducing learning rate to 1.1808854971746378e-105.\n",
      "11/11 [==============================] - 0s 32ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2372/5000\n",
      "\n",
      "Epoch 02372: LearningRateScheduler reducing learning rate to 1.0685093842595853e-105.\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2373/5000\n",
      "\n",
      "Epoch 02373: LearningRateScheduler reducing learning rate to 9.668272724006417e-106.\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2374/5000\n",
      "\n",
      "Epoch 02374: LearningRateScheduler reducing learning rate to 8.74821492845751e-106.\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2375/5000\n",
      "\n",
      "Epoch 02375: LearningRateScheduler reducing learning rate to 7.915712208289176e-106.\n",
      "11/11 [==============================] - 1s 50ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2376/5000\n",
      "\n",
      "Epoch 02376: LearningRateScheduler reducing learning rate to 7.162432596464142e-106.\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2377/5000\n",
      "\n",
      "Epoch 02377: LearningRateScheduler reducing learning rate to 6.480837017441059e-106.\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2378/5000\n",
      "\n",
      "Epoch 02378: LearningRateScheduler reducing learning rate to 5.864103833573271e-106.\n",
      "11/11 [==============================] - 0s 24ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2379/5000\n",
      "\n",
      "Epoch 02379: LearningRateScheduler reducing learning rate to 5.306060571865242e-106.\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2380/5000\n",
      "\n",
      "Epoch 02380: LearningRateScheduler reducing learning rate to 4.80112214778898e-106.\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2381/5000\n",
      "\n",
      "Epoch 02381: LearningRateScheduler reducing learning rate to 4.344234967880666e-106.\n",
      "11/11 [==============================] - 1s 60ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2382/5000\n",
      "\n",
      "Epoch 02382: LearningRateScheduler reducing learning rate to 3.930826351678582e-106.\n",
      "11/11 [==============================] - 0s 28ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2383/5000\n",
      "\n",
      "Epoch 02383: LearningRateScheduler reducing learning rate to 3.5567587668005796e-106.\n",
      "11/11 [==============================] - 1s 71ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2384/5000\n",
      "\n",
      "Epoch 02384: LearningRateScheduler reducing learning rate to 3.218288419128618e-106.\n",
      "11/11 [==============================] - 0s 37ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2385/5000\n",
      "\n",
      "Epoch 02385: LearningRateScheduler reducing learning rate to 2.912027783659385e-106.\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2386/5000\n",
      "\n",
      "Epoch 02386: LearningRateScheduler reducing learning rate to 2.634911701015351e-106.\n",
      "11/11 [==============================] - 0s 11ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2387/5000\n",
      "\n",
      "Epoch 02387: LearningRateScheduler reducing learning rate to 2.3841667002994144e-106.\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2388/5000\n",
      "\n",
      "Epoch 02388: LearningRateScheduler reducing learning rate to 2.1572832412662478e-106.\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2389/5000\n",
      "\n",
      "Epoch 02389: LearningRateScheduler reducing learning rate to 1.951990597999609e-106.\n",
      "11/11 [==============================] - 0s 39ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2390/5000\n",
      "\n",
      "Epoch 02390: LearningRateScheduler reducing learning rate to 1.7662341327244446e-106.\n",
      "11/11 [==============================] - 0s 17ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2391/5000\n",
      "\n",
      "Epoch 02391: LearningRateScheduler reducing learning rate to 1.598154732301378e-106.\n",
      "11/11 [==============================] - 0s 37ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2392/5000\n",
      "\n",
      "Epoch 02392: LearningRateScheduler reducing learning rate to 1.446070201597496e-106.\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2393/5000\n",
      "\n",
      "Epoch 02393: LearningRateScheduler reducing learning rate to 1.3084584275122253e-106.\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2394/5000\n",
      "\n",
      "Epoch 02394: LearningRateScheduler reducing learning rate to 1.1839421451575605e-106.\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2395/5000\n",
      "\n",
      "Epoch 02395: LearningRateScheduler reducing learning rate to 1.0712751537283283e-106.\n",
      "11/11 [==============================] - 0s 13ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2396/5000\n",
      "\n",
      "Epoch 02396: LearningRateScheduler reducing learning rate to 9.693298441056218e-107.\n",
      "11/11 [==============================] - 0s 9ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2397/5000\n",
      "\n",
      "Epoch 02397: LearningRateScheduler reducing learning rate to 8.770859133657102e-107.\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2398/5000\n",
      "\n",
      "Epoch 02398: LearningRateScheduler reducing learning rate to 7.936201532455451e-107.\n",
      "11/11 [==============================] - 0s 23ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2399/5000\n",
      "\n",
      "Epoch 02399: LearningRateScheduler reducing learning rate to 7.180972103640056e-107.\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2400/5000\n",
      "\n",
      "Epoch 02400: LearningRateScheduler reducing learning rate to 6.497612257245958e-107.\n",
      "11/11 [==============================] - 0s 38ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2401/5000\n",
      "\n",
      "Epoch 02401: LearningRateScheduler reducing learning rate to 5.87928269824527e-107.\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2402/5000\n",
      "\n",
      "Epoch 02402: LearningRateScheduler reducing learning rate to 5.319794976583618e-107.\n",
      "11/11 [==============================] - 0s 18ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2403/5000\n",
      "\n",
      "Epoch 02403: LearningRateScheduler reducing learning rate to 4.813549551092616e-107.\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2404/5000\n",
      "\n",
      "Epoch 02404: LearningRateScheduler reducing learning rate to 4.3554797473988197e-107.\n",
      "11/11 [==============================] - 0s 17ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2405/5000\n",
      "\n",
      "Epoch 02405: LearningRateScheduler reducing learning rate to 3.941001048944284e-107.\n",
      "11/11 [==============================] - 0s 19ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2406/5000\n",
      "\n",
      "Epoch 02406: LearningRateScheduler reducing learning rate to 3.565965213603775e-107.\n",
      "11/11 [==============================] - 0s 25ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2407/5000\n",
      "\n",
      "Epoch 02407: LearningRateScheduler reducing learning rate to 3.2266187566832154e-107.\n",
      "11/11 [==============================] - 0s 21ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2408/5000\n",
      "\n",
      "Epoch 02408: LearningRateScheduler reducing learning rate to 2.919565384783655e-107.\n",
      "11/11 [==============================] - 0s 30ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2409/5000\n",
      "\n",
      "Epoch 02409: LearningRateScheduler reducing learning rate to 2.6417320045548204e-107.\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2410/5000\n",
      "\n",
      "Epoch 02410: LearningRateScheduler reducing learning rate to 2.390337966144357e-107.\n",
      "11/11 [==============================] - 0s 15ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2411/5000\n",
      "\n",
      "Epoch 02411: LearningRateScheduler reducing learning rate to 2.1628672335193994e-107.\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2412/5000\n",
      "\n",
      "Epoch 02412: LearningRateScheduler reducing learning rate to 1.9570432031322276e-107.\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2413/5000\n",
      "\n",
      "Epoch 02413: LearningRateScheduler reducing learning rate to 1.770805918906999e-107.\n",
      "11/11 [==============================] - 0s 29ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2414/5000\n",
      "\n",
      "Epoch 02414: LearningRateScheduler reducing learning rate to 1.6022914555066128e-107.\n",
      "11/11 [==============================] - 0s 16ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2415/5000\n",
      "\n",
      "Epoch 02415: LearningRateScheduler reducing learning rate to 1.449813263541691e-107.\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2416/5000\n",
      "\n",
      "Epoch 02416: LearningRateScheduler reducing learning rate to 1.3118452900173596e-107.\n",
      "11/11 [==============================] - 0s 30ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2417/5000\n",
      "\n",
      "Epoch 02417: LearningRateScheduler reducing learning rate to 1.1870067050819153e-107.\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2418/5000\n",
      "\n",
      "Epoch 02418: LearningRateScheduler reducing learning rate to 1.074048082217698e-107.\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2419/5000\n",
      "\n",
      "Epoch 02419: LearningRateScheduler reducing learning rate to 9.718388935603413e-108.\n",
      "11/11 [==============================] - 0s 23ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2420/5000\n",
      "\n",
      "Epoch 02420: LearningRateScheduler reducing learning rate to 8.79356195196068e-108.\n",
      "11/11 [==============================] - 0s 18ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2421/5000\n",
      "\n",
      "Epoch 02421: LearningRateScheduler reducing learning rate to 7.9567438919514e-108.\n",
      "11/11 [==============================] - 0s 17ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2422/5000\n",
      "\n",
      "Epoch 02422: LearningRateScheduler reducing learning rate to 7.199559599166532e-108.\n",
      "11/11 [==============================] - ETA: 0s - loss: 100.4673 - mae: 8.5072 - mse: 100.467 - 0s 14ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2423/5000\n",
      "\n",
      "Epoch 02423: LearningRateScheduler reducing learning rate to 6.514430918705891e-108.\n",
      "11/11 [==============================] - 0s 14ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2424/5000\n",
      "\n",
      "Epoch 02424: LearningRateScheduler reducing learning rate to 5.894500852455496e-108.\n",
      "11/11 [==============================] - 0s 12ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2425/5000\n",
      "\n",
      "Epoch 02425: LearningRateScheduler reducing learning rate to 5.333564931946624e-108.\n",
      "11/11 [==============================] - 0s 22ms/step - loss: 115.6317 - mae: 7.8841 - mse: 115.6317 - val_loss: 120.0149 - val_mae: 8.3874 - val_mse: 120.0149\n",
      "Epoch 2426/5000\n",
      "\n",
      "Epoch 02426: LearningRateScheduler reducing learning rate to 4.8260091219497485e-108.\n",
      " 1/11 [=>............................] - ETA: 0s - loss: 157.7248 - mae: 9.3503 - mse: 157.7248"
     ]
    }
   ],
   "source": [
    "\n",
    "# snippet of using the LearningRateScheduler callback\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "import math\n",
    "# YOUR CODE HERE\n",
    "initial_lr = 0.001\n",
    "initial_learning_rate = 0.01\n",
    "def lr_exp_decay(epoch, lr):\n",
    "    k = 0.1\n",
    "    return initial_learning_rate * math.exp(-k*epoch)\n",
    "# Fit the model to the training data\n",
    "history_exp_decay = model.fit(\n",
    "    X_train, \n",
    "    y_train,\n",
    "    epochs=5000, \n",
    "    validation_split=0.2,validation_data=(X_test, y_test),\n",
    "    callbacks=[LearningRateScheduler(lr_exp_decay, verbose=1)],)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plot_loss_mae' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-8c5c9608d7e9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplot_loss_mae\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory_exp_decay\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'plot_loss_mae' is not defined"
     ]
    }
   ],
   "source": [
    "plot_loss_mae(history_exp_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exp_decay(epoch):\n",
    "   initial_lrate = 0.1\n",
    "   k = 0.1\n",
    "   lrate = initial_lrate * exp(-k*t)\n",
    "   return lrate\n",
    "lrate = LearningRateScheduler(exp_decay)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
